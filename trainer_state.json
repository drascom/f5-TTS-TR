{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.455622380171314,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 9.112447603426281e-05,
      "grad_norm": 4.810983180999756,
      "learning_rate": 4.999999897557577e-05,
      "loss": 3.2064,
      "step": 1
    },
    {
      "epoch": 0.00018224895206852561,
      "grad_norm": 1.9742110967636108,
      "learning_rate": 4.999999590230316e-05,
      "loss": 2.9028,
      "step": 2
    },
    {
      "epoch": 0.0002733734281027884,
      "grad_norm": 3.905468463897705,
      "learning_rate": 4.99999907801824e-05,
      "loss": 3.1475,
      "step": 3
    },
    {
      "epoch": 0.00036449790413705123,
      "grad_norm": 3.2750675678253174,
      "learning_rate": 4.9999983609213935e-05,
      "loss": 3.1676,
      "step": 4
    },
    {
      "epoch": 0.000455622380171314,
      "grad_norm": 2.567809581756592,
      "learning_rate": 4.999997438939835e-05,
      "loss": 2.3521,
      "step": 5
    },
    {
      "epoch": 0.0005467468562055768,
      "grad_norm": 2.246769905090332,
      "learning_rate": 4.9999963120736396e-05,
      "loss": 3.0339,
      "step": 6
    },
    {
      "epoch": 0.0006378713322398396,
      "grad_norm": 3.4692740440368652,
      "learning_rate": 4.999994980322899e-05,
      "loss": 2.579,
      "step": 7
    },
    {
      "epoch": 0.0007289958082741025,
      "grad_norm": 3.0380406379699707,
      "learning_rate": 4.999993443687723e-05,
      "loss": 3.035,
      "step": 8
    },
    {
      "epoch": 0.0008201202843083652,
      "grad_norm": 4.001114368438721,
      "learning_rate": 4.999991702168238e-05,
      "loss": 3.0391,
      "step": 9
    },
    {
      "epoch": 0.000911244760342628,
      "grad_norm": 2.6958892345428467,
      "learning_rate": 4.9999897557645856e-05,
      "loss": 3.2347,
      "step": 10
    },
    {
      "epoch": 0.0010023692363768909,
      "grad_norm": 3.562798500061035,
      "learning_rate": 4.9999876044769266e-05,
      "loss": 3.0267,
      "step": 11
    },
    {
      "epoch": 0.0010934937124111536,
      "grad_norm": 3.4214489459991455,
      "learning_rate": 4.999985248305436e-05,
      "loss": 3.0557,
      "step": 12
    },
    {
      "epoch": 0.0011846181884454165,
      "grad_norm": 5.271027565002441,
      "learning_rate": 4.9999826872503085e-05,
      "loss": 3.191,
      "step": 13
    },
    {
      "epoch": 0.0012757426644796792,
      "grad_norm": 4.908616542816162,
      "learning_rate": 4.999979921311753e-05,
      "loss": 3.1134,
      "step": 14
    },
    {
      "epoch": 0.001366867140513942,
      "grad_norm": 3.8159241676330566,
      "learning_rate": 4.999976950489995e-05,
      "loss": 3.3288,
      "step": 15
    },
    {
      "epoch": 0.001457991616548205,
      "grad_norm": 4.082758903503418,
      "learning_rate": 4.99997377478528e-05,
      "loss": 3.1537,
      "step": 16
    },
    {
      "epoch": 0.0015491160925824676,
      "grad_norm": 3.051913022994995,
      "learning_rate": 4.999970394197867e-05,
      "loss": 3.2477,
      "step": 17
    },
    {
      "epoch": 0.0016402405686167304,
      "grad_norm": 4.618947505950928,
      "learning_rate": 4.9999668087280336e-05,
      "loss": 3.1326,
      "step": 18
    },
    {
      "epoch": 0.0017313650446509933,
      "grad_norm": 3.8966140747070312,
      "learning_rate": 4.9999630183760745e-05,
      "loss": 3.0284,
      "step": 19
    },
    {
      "epoch": 0.001822489520685256,
      "grad_norm": 4.619442462921143,
      "learning_rate": 4.999959023142298e-05,
      "loss": 3.3404,
      "step": 20
    },
    {
      "epoch": 0.0019136139967195188,
      "grad_norm": 4.453152656555176,
      "learning_rate": 4.999954823027034e-05,
      "loss": 3.1907,
      "step": 21
    },
    {
      "epoch": 0.0020047384727537817,
      "grad_norm": 2.996080160140991,
      "learning_rate": 4.999950418030625e-05,
      "loss": 3.32,
      "step": 22
    },
    {
      "epoch": 0.0020958629487880446,
      "grad_norm": 3.222342014312744,
      "learning_rate": 4.999945808153433e-05,
      "loss": 3.3387,
      "step": 23
    },
    {
      "epoch": 0.002186987424822307,
      "grad_norm": 2.4748733043670654,
      "learning_rate": 4.9999409933958354e-05,
      "loss": 3.2016,
      "step": 24
    },
    {
      "epoch": 0.00227811190085657,
      "grad_norm": 2.91965913772583,
      "learning_rate": 4.9999359737582266e-05,
      "loss": 3.0563,
      "step": 25
    },
    {
      "epoch": 0.002369236376890833,
      "grad_norm": 3.890470027923584,
      "learning_rate": 4.9999307492410176e-05,
      "loss": 2.9124,
      "step": 26
    },
    {
      "epoch": 0.0024603608529250955,
      "grad_norm": 2.584951877593994,
      "learning_rate": 4.999925319844638e-05,
      "loss": 3.1967,
      "step": 27
    },
    {
      "epoch": 0.0025514853289593585,
      "grad_norm": 3.9780001640319824,
      "learning_rate": 4.999919685569532e-05,
      "loss": 3.279,
      "step": 28
    },
    {
      "epoch": 0.0026426098049936214,
      "grad_norm": 2.990048885345459,
      "learning_rate": 4.999913846416161e-05,
      "loss": 3.1481,
      "step": 29
    },
    {
      "epoch": 0.002733734281027884,
      "grad_norm": 3.9399380683898926,
      "learning_rate": 4.999907802385003e-05,
      "loss": 2.9727,
      "step": 30
    },
    {
      "epoch": 0.002824858757062147,
      "grad_norm": 3.745975971221924,
      "learning_rate": 4.999901553476555e-05,
      "loss": 2.8796,
      "step": 31
    },
    {
      "epoch": 0.00291598323309641,
      "grad_norm": 3.4274938106536865,
      "learning_rate": 4.999895099691328e-05,
      "loss": 3.0988,
      "step": 32
    },
    {
      "epoch": 0.0030071077091306723,
      "grad_norm": 2.9808709621429443,
      "learning_rate": 4.999888441029852e-05,
      "loss": 3.0458,
      "step": 33
    },
    {
      "epoch": 0.0030982321851649353,
      "grad_norm": 2.1409685611724854,
      "learning_rate": 4.9998815774926714e-05,
      "loss": 3.0208,
      "step": 34
    },
    {
      "epoch": 0.0031893566611991982,
      "grad_norm": 2.318962335586548,
      "learning_rate": 4.9998745090803486e-05,
      "loss": 3.0808,
      "step": 35
    },
    {
      "epoch": 0.0032804811372334607,
      "grad_norm": 3.4923746585845947,
      "learning_rate": 4.999867235793464e-05,
      "loss": 3.2533,
      "step": 36
    },
    {
      "epoch": 0.0033716056132677237,
      "grad_norm": 2.893838882446289,
      "learning_rate": 4.9998597576326135e-05,
      "loss": 3.1789,
      "step": 37
    },
    {
      "epoch": 0.0034627300893019866,
      "grad_norm": 2.9843909740448,
      "learning_rate": 4.999852074598409e-05,
      "loss": 3.088,
      "step": 38
    },
    {
      "epoch": 0.003553854565336249,
      "grad_norm": 4.7274041175842285,
      "learning_rate": 4.99984418669148e-05,
      "loss": 3.0058,
      "step": 39
    },
    {
      "epoch": 0.003644979041370512,
      "grad_norm": 4.613773822784424,
      "learning_rate": 4.999836093912475e-05,
      "loss": 3.0773,
      "step": 40
    },
    {
      "epoch": 0.003736103517404775,
      "grad_norm": 5.928118705749512,
      "learning_rate": 4.9998277962620556e-05,
      "loss": 3.8138,
      "step": 41
    },
    {
      "epoch": 0.0038272279934390375,
      "grad_norm": 3.0459301471710205,
      "learning_rate": 4.9998192937409015e-05,
      "loss": 3.1967,
      "step": 42
    },
    {
      "epoch": 0.0039183524694733005,
      "grad_norm": 3.857424736022949,
      "learning_rate": 4.999810586349711e-05,
      "loss": 3.2375,
      "step": 43
    },
    {
      "epoch": 0.004009476945507563,
      "grad_norm": 2.038663387298584,
      "learning_rate": 4.999801674089197e-05,
      "loss": 3.0122,
      "step": 44
    },
    {
      "epoch": 0.004100601421541826,
      "grad_norm": 2.0778284072875977,
      "learning_rate": 4.999792556960089e-05,
      "loss": 3.0134,
      "step": 45
    },
    {
      "epoch": 0.004191725897576089,
      "grad_norm": 3.187390089035034,
      "learning_rate": 4.999783234963136e-05,
      "loss": 2.9639,
      "step": 46
    },
    {
      "epoch": 0.004282850373610351,
      "grad_norm": 4.144068717956543,
      "learning_rate": 4.9997737080991005e-05,
      "loss": 3.0554,
      "step": 47
    },
    {
      "epoch": 0.004373974849644614,
      "grad_norm": 3.5001306533813477,
      "learning_rate": 4.999763976368763e-05,
      "loss": 3.0371,
      "step": 48
    },
    {
      "epoch": 0.004465099325678877,
      "grad_norm": 3.6111905574798584,
      "learning_rate": 4.9997540397729226e-05,
      "loss": 2.9927,
      "step": 49
    },
    {
      "epoch": 0.00455622380171314,
      "grad_norm": 2.989147901535034,
      "learning_rate": 4.999743898312392e-05,
      "loss": 3.1585,
      "step": 50
    },
    {
      "epoch": 0.004647348277747403,
      "grad_norm": 3.8651845455169678,
      "learning_rate": 4.9997335519880036e-05,
      "loss": 2.4887,
      "step": 51
    },
    {
      "epoch": 0.004738472753781666,
      "grad_norm": 3.4701950550079346,
      "learning_rate": 4.9997230008006045e-05,
      "loss": 3.0551,
      "step": 52
    },
    {
      "epoch": 0.004829597229815928,
      "grad_norm": 2.621978998184204,
      "learning_rate": 4.99971224475106e-05,
      "loss": 3.1943,
      "step": 53
    },
    {
      "epoch": 0.004920721705850191,
      "grad_norm": 2.872795343399048,
      "learning_rate": 4.999701283840252e-05,
      "loss": 2.8501,
      "step": 54
    },
    {
      "epoch": 0.005011846181884454,
      "grad_norm": 2.817953586578369,
      "learning_rate": 4.9996901180690774e-05,
      "loss": 2.9754,
      "step": 55
    },
    {
      "epoch": 0.005102970657918717,
      "grad_norm": 1.9297401905059814,
      "learning_rate": 4.999678747438452e-05,
      "loss": 3.034,
      "step": 56
    },
    {
      "epoch": 0.00519409513395298,
      "grad_norm": 3.435847520828247,
      "learning_rate": 4.9996671719493085e-05,
      "loss": 3.1619,
      "step": 57
    },
    {
      "epoch": 0.005285219609987243,
      "grad_norm": 1.9299631118774414,
      "learning_rate": 4.999655391602594e-05,
      "loss": 3.0437,
      "step": 58
    },
    {
      "epoch": 0.005376344086021506,
      "grad_norm": 3.256587266921997,
      "learning_rate": 4.999643406399275e-05,
      "loss": 3.0342,
      "step": 59
    },
    {
      "epoch": 0.005467468562055768,
      "grad_norm": 2.301542282104492,
      "learning_rate": 4.999631216340333e-05,
      "loss": 3.0987,
      "step": 60
    },
    {
      "epoch": 0.005558593038090031,
      "grad_norm": 4.333310127258301,
      "learning_rate": 4.999618821426768e-05,
      "loss": 2.9449,
      "step": 61
    },
    {
      "epoch": 0.005649717514124294,
      "grad_norm": 1.8521463871002197,
      "learning_rate": 4.999606221659595e-05,
      "loss": 2.991,
      "step": 62
    },
    {
      "epoch": 0.005740841990158557,
      "grad_norm": 3.421117067337036,
      "learning_rate": 4.999593417039847e-05,
      "loss": 3.1391,
      "step": 63
    },
    {
      "epoch": 0.00583196646619282,
      "grad_norm": 3.233793020248413,
      "learning_rate": 4.999580407568573e-05,
      "loss": 3.0962,
      "step": 64
    },
    {
      "epoch": 0.005923090942227083,
      "grad_norm": 3.7364771366119385,
      "learning_rate": 4.99956719324684e-05,
      "loss": 3.1224,
      "step": 65
    },
    {
      "epoch": 0.006014215418261345,
      "grad_norm": 3.039381980895996,
      "learning_rate": 4.9995537740757296e-05,
      "loss": 3.0738,
      "step": 66
    },
    {
      "epoch": 0.006105339894295608,
      "grad_norm": 2.6636710166931152,
      "learning_rate": 4.999540150056343e-05,
      "loss": 2.7519,
      "step": 67
    },
    {
      "epoch": 0.0061964643703298706,
      "grad_norm": 4.360434532165527,
      "learning_rate": 4.999526321189796e-05,
      "loss": 3.3508,
      "step": 68
    },
    {
      "epoch": 0.0062875888463641335,
      "grad_norm": 3.0914595127105713,
      "learning_rate": 4.999512287477222e-05,
      "loss": 3.1953,
      "step": 69
    },
    {
      "epoch": 0.0063787133223983964,
      "grad_norm": 2.340858221054077,
      "learning_rate": 4.999498048919771e-05,
      "loss": 3.0567,
      "step": 70
    },
    {
      "epoch": 0.006469837798432659,
      "grad_norm": 2.7261271476745605,
      "learning_rate": 4.9994836055186104e-05,
      "loss": 2.9738,
      "step": 71
    },
    {
      "epoch": 0.0065609622744669215,
      "grad_norm": 4.343580722808838,
      "learning_rate": 4.999468957274923e-05,
      "loss": 2.1352,
      "step": 72
    },
    {
      "epoch": 0.006652086750501184,
      "grad_norm": 4.178849220275879,
      "learning_rate": 4.9994541041899104e-05,
      "loss": 2.9654,
      "step": 73
    },
    {
      "epoch": 0.006743211226535447,
      "grad_norm": 2.4153099060058594,
      "learning_rate": 4.999439046264789e-05,
      "loss": 3.0347,
      "step": 74
    },
    {
      "epoch": 0.00683433570256971,
      "grad_norm": 3.150313377380371,
      "learning_rate": 4.999423783500793e-05,
      "loss": 3.0065,
      "step": 75
    },
    {
      "epoch": 0.006925460178603973,
      "grad_norm": 3.502631902694702,
      "learning_rate": 4.999408315899173e-05,
      "loss": 3.0649,
      "step": 76
    },
    {
      "epoch": 0.007016584654638236,
      "grad_norm": 3.1519196033477783,
      "learning_rate": 4.999392643461198e-05,
      "loss": 2.5478,
      "step": 77
    },
    {
      "epoch": 0.007107709130672498,
      "grad_norm": 3.289674758911133,
      "learning_rate": 4.9993767661881505e-05,
      "loss": 2.9503,
      "step": 78
    },
    {
      "epoch": 0.007198833606706761,
      "grad_norm": 2.8465538024902344,
      "learning_rate": 4.999360684081333e-05,
      "loss": 2.7604,
      "step": 79
    },
    {
      "epoch": 0.007289958082741024,
      "grad_norm": 1.927045464515686,
      "learning_rate": 4.999344397142064e-05,
      "loss": 3.2912,
      "step": 80
    },
    {
      "epoch": 0.007381082558775287,
      "grad_norm": 3.03855037689209,
      "learning_rate": 4.9993279053716767e-05,
      "loss": 2.8311,
      "step": 81
    },
    {
      "epoch": 0.00747220703480955,
      "grad_norm": 3.128101348876953,
      "learning_rate": 4.9993112087715236e-05,
      "loss": 3.2222,
      "step": 82
    },
    {
      "epoch": 0.007563331510843813,
      "grad_norm": 2.3521552085876465,
      "learning_rate": 4.999294307342972e-05,
      "loss": 3.0606,
      "step": 83
    },
    {
      "epoch": 0.007654455986878075,
      "grad_norm": 4.49812650680542,
      "learning_rate": 4.999277201087409e-05,
      "loss": 2.5668,
      "step": 84
    },
    {
      "epoch": 0.007745580462912338,
      "grad_norm": 1.8955475091934204,
      "learning_rate": 4.999259890006235e-05,
      "loss": 2.9524,
      "step": 85
    },
    {
      "epoch": 0.007836704938946601,
      "grad_norm": 2.4218337535858154,
      "learning_rate": 4.999242374100869e-05,
      "loss": 2.7087,
      "step": 86
    },
    {
      "epoch": 0.007927829414980863,
      "grad_norm": 2.254610538482666,
      "learning_rate": 4.999224653372747e-05,
      "loss": 3.0396,
      "step": 87
    },
    {
      "epoch": 0.008018953891015127,
      "grad_norm": 3.1680707931518555,
      "learning_rate": 4.99920672782332e-05,
      "loss": 3.0425,
      "step": 88
    },
    {
      "epoch": 0.008110078367049389,
      "grad_norm": 3.0137205123901367,
      "learning_rate": 4.999188597454059e-05,
      "loss": 2.9103,
      "step": 89
    },
    {
      "epoch": 0.008201202843083653,
      "grad_norm": 3.345235824584961,
      "learning_rate": 4.999170262266448e-05,
      "loss": 3.1356,
      "step": 90
    },
    {
      "epoch": 0.008292327319117915,
      "grad_norm": 3.2153403759002686,
      "learning_rate": 4.999151722261991e-05,
      "loss": 3.3007,
      "step": 91
    },
    {
      "epoch": 0.008383451795152179,
      "grad_norm": 2.3512165546417236,
      "learning_rate": 4.999132977442207e-05,
      "loss": 3.0219,
      "step": 92
    },
    {
      "epoch": 0.00847457627118644,
      "grad_norm": 2.2550835609436035,
      "learning_rate": 4.9991140278086316e-05,
      "loss": 3.1043,
      "step": 93
    },
    {
      "epoch": 0.008565700747220703,
      "grad_norm": 4.217181205749512,
      "learning_rate": 4.9990948733628186e-05,
      "loss": 3.1672,
      "step": 94
    },
    {
      "epoch": 0.008656825223254967,
      "grad_norm": 3.7786593437194824,
      "learning_rate": 4.9990755141063376e-05,
      "loss": 3.051,
      "step": 95
    },
    {
      "epoch": 0.008747949699289229,
      "grad_norm": 2.935464859008789,
      "learning_rate": 4.999055950040775e-05,
      "loss": 2.7657,
      "step": 96
    },
    {
      "epoch": 0.008839074175323492,
      "grad_norm": 2.1169307231903076,
      "learning_rate": 4.999036181167733e-05,
      "loss": 2.951,
      "step": 97
    },
    {
      "epoch": 0.008930198651357754,
      "grad_norm": 3.458928108215332,
      "learning_rate": 4.999016207488835e-05,
      "loss": 2.7861,
      "step": 98
    },
    {
      "epoch": 0.009021323127392018,
      "grad_norm": 3.1938233375549316,
      "learning_rate": 4.998996029005715e-05,
      "loss": 2.782,
      "step": 99
    },
    {
      "epoch": 0.00911244760342628,
      "grad_norm": 1.885495901107788,
      "learning_rate": 4.998975645720027e-05,
      "loss": 2.9136,
      "step": 100
    },
    {
      "epoch": 0.009203572079460542,
      "grad_norm": 3.273291826248169,
      "learning_rate": 4.998955057633442e-05,
      "loss": 3.2322,
      "step": 101
    },
    {
      "epoch": 0.009294696555494806,
      "grad_norm": 1.6428008079528809,
      "learning_rate": 4.998934264747648e-05,
      "loss": 2.7924,
      "step": 102
    },
    {
      "epoch": 0.009385821031529068,
      "grad_norm": 3.5032806396484375,
      "learning_rate": 4.9989132670643486e-05,
      "loss": 2.8194,
      "step": 103
    },
    {
      "epoch": 0.009476945507563332,
      "grad_norm": 2.517979145050049,
      "learning_rate": 4.998892064585264e-05,
      "loss": 2.5998,
      "step": 104
    },
    {
      "epoch": 0.009568069983597594,
      "grad_norm": 2.546837329864502,
      "learning_rate": 4.9988706573121324e-05,
      "loss": 3.0157,
      "step": 105
    },
    {
      "epoch": 0.009659194459631856,
      "grad_norm": 3.4117698669433594,
      "learning_rate": 4.9988490452467074e-05,
      "loss": 3.1974,
      "step": 106
    },
    {
      "epoch": 0.00975031893566612,
      "grad_norm": 3.3978612422943115,
      "learning_rate": 4.9988272283907616e-05,
      "loss": 2.4654,
      "step": 107
    },
    {
      "epoch": 0.009841443411700382,
      "grad_norm": 2.8470406532287598,
      "learning_rate": 4.998805206746082e-05,
      "loss": 2.7727,
      "step": 108
    },
    {
      "epoch": 0.009932567887734646,
      "grad_norm": 2.559325695037842,
      "learning_rate": 4.998782980314474e-05,
      "loss": 2.9934,
      "step": 109
    },
    {
      "epoch": 0.010023692363768908,
      "grad_norm": 2.0657799243927,
      "learning_rate": 4.998760549097758e-05,
      "loss": 3.084,
      "step": 110
    },
    {
      "epoch": 0.010114816839803172,
      "grad_norm": 4.179584980010986,
      "learning_rate": 4.998737913097773e-05,
      "loss": 2.7185,
      "step": 111
    },
    {
      "epoch": 0.010205941315837434,
      "grad_norm": 3.370600700378418,
      "learning_rate": 4.998715072316375e-05,
      "loss": 3.3553,
      "step": 112
    },
    {
      "epoch": 0.010297065791871696,
      "grad_norm": 3.263859987258911,
      "learning_rate": 4.998692026755435e-05,
      "loss": 3.2411,
      "step": 113
    },
    {
      "epoch": 0.01038819026790596,
      "grad_norm": 3.004282236099243,
      "learning_rate": 4.998668776416842e-05,
      "loss": 3.2068,
      "step": 114
    },
    {
      "epoch": 0.010479314743940222,
      "grad_norm": 4.776451110839844,
      "learning_rate": 4.9986453213024996e-05,
      "loss": 2.931,
      "step": 115
    },
    {
      "epoch": 0.010570439219974486,
      "grad_norm": 3.8841757774353027,
      "learning_rate": 4.9986216614143335e-05,
      "loss": 2.6147,
      "step": 116
    },
    {
      "epoch": 0.010661563696008748,
      "grad_norm": 3.1611170768737793,
      "learning_rate": 4.9985977967542794e-05,
      "loss": 3.2392,
      "step": 117
    },
    {
      "epoch": 0.010752688172043012,
      "grad_norm": 3.0968759059906006,
      "learning_rate": 4.998573727324295e-05,
      "loss": 3.0568,
      "step": 118
    },
    {
      "epoch": 0.010843812648077274,
      "grad_norm": 2.2086093425750732,
      "learning_rate": 4.998549453126353e-05,
      "loss": 3.0838,
      "step": 119
    },
    {
      "epoch": 0.010934937124111536,
      "grad_norm": 3.466169834136963,
      "learning_rate": 4.998524974162442e-05,
      "loss": 2.9381,
      "step": 120
    },
    {
      "epoch": 0.0110260616001458,
      "grad_norm": 3.442246437072754,
      "learning_rate": 4.998500290434568e-05,
      "loss": 2.8684,
      "step": 121
    },
    {
      "epoch": 0.011117186076180062,
      "grad_norm": 3.4763424396514893,
      "learning_rate": 4.998475401944754e-05,
      "loss": 3.1428,
      "step": 122
    },
    {
      "epoch": 0.011208310552214325,
      "grad_norm": 3.302222728729248,
      "learning_rate": 4.9984503086950416e-05,
      "loss": 2.9401,
      "step": 123
    },
    {
      "epoch": 0.011299435028248588,
      "grad_norm": 2.623926877975464,
      "learning_rate": 4.998425010687484e-05,
      "loss": 3.0758,
      "step": 124
    },
    {
      "epoch": 0.01139055950428285,
      "grad_norm": 3.527374744415283,
      "learning_rate": 4.998399507924157e-05,
      "loss": 2.7145,
      "step": 125
    },
    {
      "epoch": 0.011481683980317113,
      "grad_norm": 2.3806726932525635,
      "learning_rate": 4.9983738004071495e-05,
      "loss": 2.7357,
      "step": 126
    },
    {
      "epoch": 0.011572808456351375,
      "grad_norm": 3.3078513145446777,
      "learning_rate": 4.998347888138569e-05,
      "loss": 3.0489,
      "step": 127
    },
    {
      "epoch": 0.01166393293238564,
      "grad_norm": 2.8850951194763184,
      "learning_rate": 4.9983217711205386e-05,
      "loss": 3.0792,
      "step": 128
    },
    {
      "epoch": 0.011755057408419901,
      "grad_norm": 1.8585487604141235,
      "learning_rate": 4.998295449355199e-05,
      "loss": 2.9322,
      "step": 129
    },
    {
      "epoch": 0.011846181884454165,
      "grad_norm": 4.037771701812744,
      "learning_rate": 4.9982689228447064e-05,
      "loss": 2.9084,
      "step": 130
    },
    {
      "epoch": 0.011937306360488427,
      "grad_norm": 2.485532283782959,
      "learning_rate": 4.998242191591237e-05,
      "loss": 2.8812,
      "step": 131
    },
    {
      "epoch": 0.01202843083652269,
      "grad_norm": 2.1966612339019775,
      "learning_rate": 4.9982152555969786e-05,
      "loss": 3.1189,
      "step": 132
    },
    {
      "epoch": 0.012119555312556953,
      "grad_norm": 1.8410605192184448,
      "learning_rate": 4.9981881148641405e-05,
      "loss": 3.0395,
      "step": 133
    },
    {
      "epoch": 0.012210679788591215,
      "grad_norm": 2.812716007232666,
      "learning_rate": 4.998160769394947e-05,
      "loss": 3.0484,
      "step": 134
    },
    {
      "epoch": 0.012301804264625479,
      "grad_norm": 3.6690826416015625,
      "learning_rate": 4.99813321919164e-05,
      "loss": 2.9871,
      "step": 135
    },
    {
      "epoch": 0.012392928740659741,
      "grad_norm": 4.960578918457031,
      "learning_rate": 4.998105464256475e-05,
      "loss": 2.1866,
      "step": 136
    },
    {
      "epoch": 0.012484053216694003,
      "grad_norm": 3.6404716968536377,
      "learning_rate": 4.998077504591728e-05,
      "loss": 3.0983,
      "step": 137
    },
    {
      "epoch": 0.012575177692728267,
      "grad_norm": 3.0544800758361816,
      "learning_rate": 4.9980493401996905e-05,
      "loss": 3.0374,
      "step": 138
    },
    {
      "epoch": 0.012666302168762529,
      "grad_norm": 2.765801191329956,
      "learning_rate": 4.99802097108267e-05,
      "loss": 2.8467,
      "step": 139
    },
    {
      "epoch": 0.012757426644796793,
      "grad_norm": 2.434861898422241,
      "learning_rate": 4.997992397242992e-05,
      "loss": 2.9516,
      "step": 140
    },
    {
      "epoch": 0.012848551120831055,
      "grad_norm": 3.1218740940093994,
      "learning_rate": 4.997963618682998e-05,
      "loss": 3.0314,
      "step": 141
    },
    {
      "epoch": 0.012939675596865319,
      "grad_norm": 1.805385947227478,
      "learning_rate": 4.997934635405047e-05,
      "loss": 2.9169,
      "step": 142
    },
    {
      "epoch": 0.01303080007289958,
      "grad_norm": 3.3232510089874268,
      "learning_rate": 4.9979054474115144e-05,
      "loss": 3.2397,
      "step": 143
    },
    {
      "epoch": 0.013121924548933843,
      "grad_norm": 2.7170846462249756,
      "learning_rate": 4.9978760547047915e-05,
      "loss": 3.1618,
      "step": 144
    },
    {
      "epoch": 0.013213049024968107,
      "grad_norm": 5.612462520599365,
      "learning_rate": 4.9978464572872876e-05,
      "loss": 3.7115,
      "step": 145
    },
    {
      "epoch": 0.013304173501002369,
      "grad_norm": 3.0545284748077393,
      "learning_rate": 4.997816655161428e-05,
      "loss": 2.8899,
      "step": 146
    },
    {
      "epoch": 0.013395297977036633,
      "grad_norm": 2.2899675369262695,
      "learning_rate": 4.9977866483296544e-05,
      "loss": 3.01,
      "step": 147
    },
    {
      "epoch": 0.013486422453070895,
      "grad_norm": 1.921761393547058,
      "learning_rate": 4.997756436794428e-05,
      "loss": 2.9777,
      "step": 148
    },
    {
      "epoch": 0.013577546929105158,
      "grad_norm": 3.515000104904175,
      "learning_rate": 4.997726020558223e-05,
      "loss": 2.9246,
      "step": 149
    },
    {
      "epoch": 0.01366867140513942,
      "grad_norm": 1.8955378532409668,
      "learning_rate": 4.997695399623533e-05,
      "loss": 2.9843,
      "step": 150
    },
    {
      "epoch": 0.013759795881173683,
      "grad_norm": 2.8406405448913574,
      "learning_rate": 4.9976645739928675e-05,
      "loss": 2.819,
      "step": 151
    },
    {
      "epoch": 0.013850920357207946,
      "grad_norm": 2.192831039428711,
      "learning_rate": 4.9976335436687525e-05,
      "loss": 2.9843,
      "step": 152
    },
    {
      "epoch": 0.013942044833242209,
      "grad_norm": 2.079318046569824,
      "learning_rate": 4.997602308653731e-05,
      "loss": 2.9742,
      "step": 153
    },
    {
      "epoch": 0.014033169309276472,
      "grad_norm": 2.436514139175415,
      "learning_rate": 4.997570868950363e-05,
      "loss": 2.8987,
      "step": 154
    },
    {
      "epoch": 0.014124293785310734,
      "grad_norm": 2.0050177574157715,
      "learning_rate": 4.9975392245612254e-05,
      "loss": 2.9146,
      "step": 155
    },
    {
      "epoch": 0.014215418261344996,
      "grad_norm": 2.7386975288391113,
      "learning_rate": 4.99750737548891e-05,
      "loss": 3.106,
      "step": 156
    },
    {
      "epoch": 0.01430654273737926,
      "grad_norm": 3.8090169429779053,
      "learning_rate": 4.9974753217360295e-05,
      "loss": 2.5344,
      "step": 157
    },
    {
      "epoch": 0.014397667213413522,
      "grad_norm": 1.7630343437194824,
      "learning_rate": 4.9974430633052085e-05,
      "loss": 2.6978,
      "step": 158
    },
    {
      "epoch": 0.014488791689447786,
      "grad_norm": 2.020094156265259,
      "learning_rate": 4.9974106001990923e-05,
      "loss": 3.0851,
      "step": 159
    },
    {
      "epoch": 0.014579916165482048,
      "grad_norm": 2.589174270629883,
      "learning_rate": 4.997377932420341e-05,
      "loss": 2.8161,
      "step": 160
    },
    {
      "epoch": 0.014671040641516312,
      "grad_norm": 3.2339718341827393,
      "learning_rate": 4.997345059971631e-05,
      "loss": 2.6908,
      "step": 161
    },
    {
      "epoch": 0.014762165117550574,
      "grad_norm": 2.0948266983032227,
      "learning_rate": 4.997311982855657e-05,
      "loss": 2.9472,
      "step": 162
    },
    {
      "epoch": 0.014853289593584836,
      "grad_norm": 2.4752578735351562,
      "learning_rate": 4.99727870107513e-05,
      "loss": 3.1334,
      "step": 163
    },
    {
      "epoch": 0.0149444140696191,
      "grad_norm": 3.5128135681152344,
      "learning_rate": 4.997245214632778e-05,
      "loss": 2.8477,
      "step": 164
    },
    {
      "epoch": 0.015035538545653362,
      "grad_norm": 2.700103282928467,
      "learning_rate": 4.997211523531344e-05,
      "loss": 3.0588,
      "step": 165
    },
    {
      "epoch": 0.015126663021687626,
      "grad_norm": 4.285271644592285,
      "learning_rate": 4.9971776277735906e-05,
      "loss": 2.7581,
      "step": 166
    },
    {
      "epoch": 0.015217787497721888,
      "grad_norm": 3.8157570362091064,
      "learning_rate": 4.997143527362294e-05,
      "loss": 3.0998,
      "step": 167
    },
    {
      "epoch": 0.01530891197375615,
      "grad_norm": 3.245560646057129,
      "learning_rate": 4.99710922230025e-05,
      "loss": 3.0689,
      "step": 168
    },
    {
      "epoch": 0.015400036449790414,
      "grad_norm": 3.2004127502441406,
      "learning_rate": 4.9970747125902694e-05,
      "loss": 2.8422,
      "step": 169
    },
    {
      "epoch": 0.015491160925824676,
      "grad_norm": 3.4759035110473633,
      "learning_rate": 4.997039998235181e-05,
      "loss": 3.1256,
      "step": 170
    },
    {
      "epoch": 0.01558228540185894,
      "grad_norm": 2.497690200805664,
      "learning_rate": 4.99700507923783e-05,
      "loss": 2.9096,
      "step": 171
    },
    {
      "epoch": 0.015673409877893202,
      "grad_norm": 2.1334710121154785,
      "learning_rate": 4.996969955601078e-05,
      "loss": 2.9044,
      "step": 172
    },
    {
      "epoch": 0.015764534353927464,
      "grad_norm": 2.300891160964966,
      "learning_rate": 4.9969346273278025e-05,
      "loss": 2.8908,
      "step": 173
    },
    {
      "epoch": 0.015855658829961726,
      "grad_norm": 2.0513505935668945,
      "learning_rate": 4.996899094420901e-05,
      "loss": 2.8817,
      "step": 174
    },
    {
      "epoch": 0.01594678330599599,
      "grad_norm": 2.8278379440307617,
      "learning_rate": 4.996863356883282e-05,
      "loss": 3.1366,
      "step": 175
    },
    {
      "epoch": 0.016037907782030254,
      "grad_norm": 3.43684720993042,
      "learning_rate": 4.996827414717878e-05,
      "loss": 3.0606,
      "step": 176
    },
    {
      "epoch": 0.016129032258064516,
      "grad_norm": 3.0732672214508057,
      "learning_rate": 4.9967912679276316e-05,
      "loss": 2.843,
      "step": 177
    },
    {
      "epoch": 0.016220156734098778,
      "grad_norm": 3.4920480251312256,
      "learning_rate": 4.996754916515508e-05,
      "loss": 2.8969,
      "step": 178
    },
    {
      "epoch": 0.016311281210133043,
      "grad_norm": 3.4001033306121826,
      "learning_rate": 4.996718360484485e-05,
      "loss": 2.8035,
      "step": 179
    },
    {
      "epoch": 0.016402405686167305,
      "grad_norm": 1.7016775608062744,
      "learning_rate": 4.9966815998375575e-05,
      "loss": 2.7236,
      "step": 180
    },
    {
      "epoch": 0.016493530162201567,
      "grad_norm": 3.0281150341033936,
      "learning_rate": 4.99664463457774e-05,
      "loss": 3.2244,
      "step": 181
    },
    {
      "epoch": 0.01658465463823583,
      "grad_norm": 3.8529109954833984,
      "learning_rate": 4.9966074647080606e-05,
      "loss": 2.7765,
      "step": 182
    },
    {
      "epoch": 0.01667577911427009,
      "grad_norm": 4.410470008850098,
      "learning_rate": 4.996570090231566e-05,
      "loss": 3.2551,
      "step": 183
    },
    {
      "epoch": 0.016766903590304357,
      "grad_norm": 2.621544122695923,
      "learning_rate": 4.99653251115132e-05,
      "loss": 2.8792,
      "step": 184
    },
    {
      "epoch": 0.01685802806633862,
      "grad_norm": 3.7127602100372314,
      "learning_rate": 4.9964947274704e-05,
      "loss": 2.4034,
      "step": 185
    },
    {
      "epoch": 0.01694915254237288,
      "grad_norm": 3.494060754776001,
      "learning_rate": 4.996456739191905e-05,
      "loss": 2.697,
      "step": 186
    },
    {
      "epoch": 0.017040277018407143,
      "grad_norm": 2.872035264968872,
      "learning_rate": 4.9964185463189475e-05,
      "loss": 3.0493,
      "step": 187
    },
    {
      "epoch": 0.017131401494441405,
      "grad_norm": 3.452061414718628,
      "learning_rate": 4.996380148854657e-05,
      "loss": 3.1065,
      "step": 188
    },
    {
      "epoch": 0.01722252597047567,
      "grad_norm": 2.105959415435791,
      "learning_rate": 4.996341546802181e-05,
      "loss": 2.9128,
      "step": 189
    },
    {
      "epoch": 0.017313650446509933,
      "grad_norm": 3.6331939697265625,
      "learning_rate": 4.996302740164683e-05,
      "loss": 2.7058,
      "step": 190
    },
    {
      "epoch": 0.017404774922544195,
      "grad_norm": 2.7457187175750732,
      "learning_rate": 4.996263728945343e-05,
      "loss": 2.7081,
      "step": 191
    },
    {
      "epoch": 0.017495899398578457,
      "grad_norm": 2.6010935306549072,
      "learning_rate": 4.9962245131473585e-05,
      "loss": 2.6168,
      "step": 192
    },
    {
      "epoch": 0.01758702387461272,
      "grad_norm": 3.7223236560821533,
      "learning_rate": 4.996185092773943e-05,
      "loss": 2.9499,
      "step": 193
    },
    {
      "epoch": 0.017678148350646985,
      "grad_norm": 3.707608938217163,
      "learning_rate": 4.996145467828327e-05,
      "loss": 2.4925,
      "step": 194
    },
    {
      "epoch": 0.017769272826681247,
      "grad_norm": 2.3394830226898193,
      "learning_rate": 4.996105638313759e-05,
      "loss": 2.965,
      "step": 195
    },
    {
      "epoch": 0.01786039730271551,
      "grad_norm": 3.4291019439697266,
      "learning_rate": 4.9960656042335016e-05,
      "loss": 2.9025,
      "step": 196
    },
    {
      "epoch": 0.01795152177874977,
      "grad_norm": 3.2778756618499756,
      "learning_rate": 4.9960253655908374e-05,
      "loss": 2.7247,
      "step": 197
    },
    {
      "epoch": 0.018042646254784037,
      "grad_norm": 2.798750877380371,
      "learning_rate": 4.995984922389063e-05,
      "loss": 2.6033,
      "step": 198
    },
    {
      "epoch": 0.0181337707308183,
      "grad_norm": 3.6283669471740723,
      "learning_rate": 4.995944274631493e-05,
      "loss": 2.6426,
      "step": 199
    },
    {
      "epoch": 0.01822489520685256,
      "grad_norm": 3.634934902191162,
      "learning_rate": 4.9959034223214595e-05,
      "loss": 2.8299,
      "step": 200
    },
    {
      "epoch": 0.018316019682886823,
      "grad_norm": 4.604448318481445,
      "learning_rate": 4.99586236546231e-05,
      "loss": 2.9753,
      "step": 201
    },
    {
      "epoch": 0.018407144158921085,
      "grad_norm": 3.651458501815796,
      "learning_rate": 4.995821104057409e-05,
      "loss": 2.5728,
      "step": 202
    },
    {
      "epoch": 0.01849826863495535,
      "grad_norm": 2.339756727218628,
      "learning_rate": 4.9957796381101374e-05,
      "loss": 3.1176,
      "step": 203
    },
    {
      "epoch": 0.018589393110989613,
      "grad_norm": 3.3692636489868164,
      "learning_rate": 4.9957379676238945e-05,
      "loss": 3.2663,
      "step": 204
    },
    {
      "epoch": 0.018680517587023875,
      "grad_norm": 4.322436809539795,
      "learning_rate": 4.995696092602096e-05,
      "loss": 3.0394,
      "step": 205
    },
    {
      "epoch": 0.018771642063058137,
      "grad_norm": 3.796675205230713,
      "learning_rate": 4.995654013048172e-05,
      "loss": 2.8621,
      "step": 206
    },
    {
      "epoch": 0.0188627665390924,
      "grad_norm": 2.7833352088928223,
      "learning_rate": 4.995611728965571e-05,
      "loss": 2.8098,
      "step": 207
    },
    {
      "epoch": 0.018953891015126664,
      "grad_norm": 3.21239972114563,
      "learning_rate": 4.995569240357761e-05,
      "loss": 2.7568,
      "step": 208
    },
    {
      "epoch": 0.019045015491160926,
      "grad_norm": 1.92779541015625,
      "learning_rate": 4.995526547228222e-05,
      "loss": 2.9617,
      "step": 209
    },
    {
      "epoch": 0.01913613996719519,
      "grad_norm": 1.4563132524490356,
      "learning_rate": 4.9954836495804525e-05,
      "loss": 2.7798,
      "step": 210
    },
    {
      "epoch": 0.01922726444322945,
      "grad_norm": 2.3598124980926514,
      "learning_rate": 4.99544054741797e-05,
      "loss": 2.7242,
      "step": 211
    },
    {
      "epoch": 0.019318388919263713,
      "grad_norm": 3.2183914184570312,
      "learning_rate": 4.995397240744305e-05,
      "loss": 2.725,
      "step": 212
    },
    {
      "epoch": 0.019409513395297978,
      "grad_norm": 3.6406822204589844,
      "learning_rate": 4.9953537295630074e-05,
      "loss": 2.9159,
      "step": 213
    },
    {
      "epoch": 0.01950063787133224,
      "grad_norm": 3.356208086013794,
      "learning_rate": 4.995310013877643e-05,
      "loss": 2.3788,
      "step": 214
    },
    {
      "epoch": 0.019591762347366502,
      "grad_norm": 3.275792360305786,
      "learning_rate": 4.9952660936917953e-05,
      "loss": 3.2876,
      "step": 215
    },
    {
      "epoch": 0.019682886823400764,
      "grad_norm": 3.7224249839782715,
      "learning_rate": 4.995221969009063e-05,
      "loss": 2.4127,
      "step": 216
    },
    {
      "epoch": 0.01977401129943503,
      "grad_norm": 2.694688081741333,
      "learning_rate": 4.995177639833062e-05,
      "loss": 2.685,
      "step": 217
    },
    {
      "epoch": 0.019865135775469292,
      "grad_norm": 2.4117233753204346,
      "learning_rate": 4.995133106167425e-05,
      "loss": 2.8893,
      "step": 218
    },
    {
      "epoch": 0.019956260251503554,
      "grad_norm": 2.3335092067718506,
      "learning_rate": 4.995088368015804e-05,
      "loss": 2.9949,
      "step": 219
    },
    {
      "epoch": 0.020047384727537816,
      "grad_norm": 2.5341763496398926,
      "learning_rate": 4.995043425381862e-05,
      "loss": 2.7349,
      "step": 220
    },
    {
      "epoch": 0.020138509203572078,
      "grad_norm": 2.1053128242492676,
      "learning_rate": 4.994998278269286e-05,
      "loss": 2.872,
      "step": 221
    },
    {
      "epoch": 0.020229633679606344,
      "grad_norm": 4.247159957885742,
      "learning_rate": 4.9949529266817716e-05,
      "loss": 2.8256,
      "step": 222
    },
    {
      "epoch": 0.020320758155640606,
      "grad_norm": 2.9789533615112305,
      "learning_rate": 4.9949073706230395e-05,
      "loss": 2.9463,
      "step": 223
    },
    {
      "epoch": 0.020411882631674868,
      "grad_norm": 4.879419326782227,
      "learning_rate": 4.994861610096821e-05,
      "loss": 3.0509,
      "step": 224
    },
    {
      "epoch": 0.02050300710770913,
      "grad_norm": 2.939770221710205,
      "learning_rate": 4.994815645106867e-05,
      "loss": 2.6338,
      "step": 225
    },
    {
      "epoch": 0.020594131583743392,
      "grad_norm": 4.3412861824035645,
      "learning_rate": 4.994769475656945e-05,
      "loss": 3.0108,
      "step": 226
    },
    {
      "epoch": 0.020685256059777658,
      "grad_norm": 2.51426100730896,
      "learning_rate": 4.994723101750838e-05,
      "loss": 3.0526,
      "step": 227
    },
    {
      "epoch": 0.02077638053581192,
      "grad_norm": 3.505676031112671,
      "learning_rate": 4.994676523392347e-05,
      "loss": 2.7056,
      "step": 228
    },
    {
      "epoch": 0.020867505011846182,
      "grad_norm": 4.260951519012451,
      "learning_rate": 4.994629740585289e-05,
      "loss": 3.1183,
      "step": 229
    },
    {
      "epoch": 0.020958629487880444,
      "grad_norm": 2.5711376667022705,
      "learning_rate": 4.994582753333498e-05,
      "loss": 3.0857,
      "step": 230
    },
    {
      "epoch": 0.021049753963914706,
      "grad_norm": 2.166073799133301,
      "learning_rate": 4.994535561640824e-05,
      "loss": 2.8542,
      "step": 231
    },
    {
      "epoch": 0.02114087843994897,
      "grad_norm": 1.8539170026779175,
      "learning_rate": 4.9944881655111366e-05,
      "loss": 3.0293,
      "step": 232
    },
    {
      "epoch": 0.021232002915983234,
      "grad_norm": 1.9353551864624023,
      "learning_rate": 4.994440564948318e-05,
      "loss": 2.8262,
      "step": 233
    },
    {
      "epoch": 0.021323127392017496,
      "grad_norm": 3.0932233333587646,
      "learning_rate": 4.994392759956271e-05,
      "loss": 2.8178,
      "step": 234
    },
    {
      "epoch": 0.021414251868051758,
      "grad_norm": 4.661931991577148,
      "learning_rate": 4.994344750538913e-05,
      "loss": 3.0574,
      "step": 235
    },
    {
      "epoch": 0.021505376344086023,
      "grad_norm": 3.5828258991241455,
      "learning_rate": 4.994296536700177e-05,
      "loss": 2.9585,
      "step": 236
    },
    {
      "epoch": 0.021596500820120285,
      "grad_norm": 3.237452983856201,
      "learning_rate": 4.994248118444016e-05,
      "loss": 3.2907,
      "step": 237
    },
    {
      "epoch": 0.021687625296154547,
      "grad_norm": 1.7712563276290894,
      "learning_rate": 4.9941994957743976e-05,
      "loss": 2.81,
      "step": 238
    },
    {
      "epoch": 0.02177874977218881,
      "grad_norm": 4.197923183441162,
      "learning_rate": 4.9941506686953064e-05,
      "loss": 2.8537,
      "step": 239
    },
    {
      "epoch": 0.02186987424822307,
      "grad_norm": 4.0791239738464355,
      "learning_rate": 4.994101637210744e-05,
      "loss": 3.1615,
      "step": 240
    },
    {
      "epoch": 0.021960998724257337,
      "grad_norm": 1.8465322256088257,
      "learning_rate": 4.994052401324729e-05,
      "loss": 2.8619,
      "step": 241
    },
    {
      "epoch": 0.0220521232002916,
      "grad_norm": 3.064363956451416,
      "learning_rate": 4.9940029610412964e-05,
      "loss": 2.9496,
      "step": 242
    },
    {
      "epoch": 0.02214324767632586,
      "grad_norm": 5.682558536529541,
      "learning_rate": 4.993953316364498e-05,
      "loss": 2.9704,
      "step": 243
    },
    {
      "epoch": 0.022234372152360123,
      "grad_norm": 3.240743637084961,
      "learning_rate": 4.993903467298402e-05,
      "loss": 2.9054,
      "step": 244
    },
    {
      "epoch": 0.022325496628394385,
      "grad_norm": 3.0782744884490967,
      "learning_rate": 4.993853413847094e-05,
      "loss": 2.328,
      "step": 245
    },
    {
      "epoch": 0.02241662110442865,
      "grad_norm": 3.2922260761260986,
      "learning_rate": 4.993803156014677e-05,
      "loss": 2.9641,
      "step": 246
    },
    {
      "epoch": 0.022507745580462913,
      "grad_norm": 3.023947238922119,
      "learning_rate": 4.993752693805268e-05,
      "loss": 2.8576,
      "step": 247
    },
    {
      "epoch": 0.022598870056497175,
      "grad_norm": 3.0554006099700928,
      "learning_rate": 4.993702027223004e-05,
      "loss": 2.7858,
      "step": 248
    },
    {
      "epoch": 0.022689994532531437,
      "grad_norm": 3.693634271621704,
      "learning_rate": 4.9936511562720364e-05,
      "loss": 2.7147,
      "step": 249
    },
    {
      "epoch": 0.0227811190085657,
      "grad_norm": 3.201390027999878,
      "learning_rate": 4.993600080956535e-05,
      "loss": 1.9882,
      "step": 250
    },
    {
      "epoch": 0.022872243484599965,
      "grad_norm": 1.9936951398849487,
      "learning_rate": 4.993548801280686e-05,
      "loss": 3.1181,
      "step": 251
    },
    {
      "epoch": 0.022963367960634227,
      "grad_norm": 3.260298490524292,
      "learning_rate": 4.993497317248691e-05,
      "loss": 3.1898,
      "step": 252
    },
    {
      "epoch": 0.02305449243666849,
      "grad_norm": 4.4110517501831055,
      "learning_rate": 4.9934456288647694e-05,
      "loss": 2.8244,
      "step": 253
    },
    {
      "epoch": 0.02314561691270275,
      "grad_norm": 2.4315178394317627,
      "learning_rate": 4.993393736133157e-05,
      "loss": 2.4183,
      "step": 254
    },
    {
      "epoch": 0.023236741388737013,
      "grad_norm": 3.644021511077881,
      "learning_rate": 4.993341639058108e-05,
      "loss": 2.654,
      "step": 255
    },
    {
      "epoch": 0.02332786586477128,
      "grad_norm": 5.052486896514893,
      "learning_rate": 4.99328933764389e-05,
      "loss": 3.4613,
      "step": 256
    },
    {
      "epoch": 0.02341899034080554,
      "grad_norm": 2.375401020050049,
      "learning_rate": 4.993236831894792e-05,
      "loss": 2.9367,
      "step": 257
    },
    {
      "epoch": 0.023510114816839803,
      "grad_norm": 2.3803887367248535,
      "learning_rate": 4.993184121815114e-05,
      "loss": 2.9791,
      "step": 258
    },
    {
      "epoch": 0.023601239292874065,
      "grad_norm": 3.324200391769409,
      "learning_rate": 4.993131207409179e-05,
      "loss": 2.8913,
      "step": 259
    },
    {
      "epoch": 0.02369236376890833,
      "grad_norm": 2.2855703830718994,
      "learning_rate": 4.99307808868132e-05,
      "loss": 3.1401,
      "step": 260
    },
    {
      "epoch": 0.023783488244942592,
      "grad_norm": 3.8498096466064453,
      "learning_rate": 4.9930247656358926e-05,
      "loss": 2.7348,
      "step": 261
    },
    {
      "epoch": 0.023874612720976855,
      "grad_norm": 2.989870071411133,
      "learning_rate": 4.992971238277266e-05,
      "loss": 2.7794,
      "step": 262
    },
    {
      "epoch": 0.023965737197011117,
      "grad_norm": 1.9267282485961914,
      "learning_rate": 4.9929175066098285e-05,
      "loss": 2.7822,
      "step": 263
    },
    {
      "epoch": 0.02405686167304538,
      "grad_norm": 3.4673049449920654,
      "learning_rate": 4.992863570637981e-05,
      "loss": 2.7716,
      "step": 264
    },
    {
      "epoch": 0.024147986149079644,
      "grad_norm": 4.030117034912109,
      "learning_rate": 4.9928094303661465e-05,
      "loss": 2.3569,
      "step": 265
    },
    {
      "epoch": 0.024239110625113906,
      "grad_norm": 2.8260750770568848,
      "learning_rate": 4.99275508579876e-05,
      "loss": 2.9893,
      "step": 266
    },
    {
      "epoch": 0.02433023510114817,
      "grad_norm": 3.653055429458618,
      "learning_rate": 4.9927005369402756e-05,
      "loss": 3.1263,
      "step": 267
    },
    {
      "epoch": 0.02442135957718243,
      "grad_norm": 2.9457454681396484,
      "learning_rate": 4.992645783795165e-05,
      "loss": 3.0073,
      "step": 268
    },
    {
      "epoch": 0.024512484053216693,
      "grad_norm": 1.6144747734069824,
      "learning_rate": 4.992590826367913e-05,
      "loss": 2.7446,
      "step": 269
    },
    {
      "epoch": 0.024603608529250958,
      "grad_norm": 3.5470383167266846,
      "learning_rate": 4.992535664663027e-05,
      "loss": 2.7096,
      "step": 270
    },
    {
      "epoch": 0.02469473300528522,
      "grad_norm": 2.77581787109375,
      "learning_rate": 4.992480298685025e-05,
      "loss": 2.7322,
      "step": 271
    },
    {
      "epoch": 0.024785857481319482,
      "grad_norm": 3.7434020042419434,
      "learning_rate": 4.992424728438445e-05,
      "loss": 2.9467,
      "step": 272
    },
    {
      "epoch": 0.024876981957353744,
      "grad_norm": 2.4266579151153564,
      "learning_rate": 4.992368953927842e-05,
      "loss": 2.5198,
      "step": 273
    },
    {
      "epoch": 0.024968106433388006,
      "grad_norm": 3.2962489128112793,
      "learning_rate": 4.9923129751577866e-05,
      "loss": 3.1596,
      "step": 274
    },
    {
      "epoch": 0.025059230909422272,
      "grad_norm": 7.043239116668701,
      "learning_rate": 4.9922567921328665e-05,
      "loss": 3.488,
      "step": 275
    },
    {
      "epoch": 0.025150355385456534,
      "grad_norm": 2.4639480113983154,
      "learning_rate": 4.992200404857686e-05,
      "loss": 2.4812,
      "step": 276
    },
    {
      "epoch": 0.025241479861490796,
      "grad_norm": 1.857424020767212,
      "learning_rate": 4.9921438133368655e-05,
      "loss": 2.8176,
      "step": 277
    },
    {
      "epoch": 0.025332604337525058,
      "grad_norm": 4.494309425354004,
      "learning_rate": 4.992087017575044e-05,
      "loss": 3.1054,
      "step": 278
    },
    {
      "epoch": 0.025423728813559324,
      "grad_norm": 4.276947975158691,
      "learning_rate": 4.992030017576875e-05,
      "loss": 2.1909,
      "step": 279
    },
    {
      "epoch": 0.025514853289593586,
      "grad_norm": 4.083865642547607,
      "learning_rate": 4.991972813347032e-05,
      "loss": 2.9479,
      "step": 280
    },
    {
      "epoch": 0.025605977765627848,
      "grad_norm": 2.375821828842163,
      "learning_rate": 4.991915404890201e-05,
      "loss": 2.7618,
      "step": 281
    },
    {
      "epoch": 0.02569710224166211,
      "grad_norm": 3.873976707458496,
      "learning_rate": 4.9918577922110875e-05,
      "loss": 3.0425,
      "step": 282
    },
    {
      "epoch": 0.025788226717696372,
      "grad_norm": 2.8449337482452393,
      "learning_rate": 4.991799975314414e-05,
      "loss": 2.8696,
      "step": 283
    },
    {
      "epoch": 0.025879351193730638,
      "grad_norm": 4.654402732849121,
      "learning_rate": 4.991741954204917e-05,
      "loss": 2.7261,
      "step": 284
    },
    {
      "epoch": 0.0259704756697649,
      "grad_norm": 2.0715343952178955,
      "learning_rate": 4.991683728887353e-05,
      "loss": 2.9878,
      "step": 285
    },
    {
      "epoch": 0.02606160014579916,
      "grad_norm": 1.749382734298706,
      "learning_rate": 4.991625299366494e-05,
      "loss": 2.8315,
      "step": 286
    },
    {
      "epoch": 0.026152724621833424,
      "grad_norm": 2.439875841140747,
      "learning_rate": 4.991566665647127e-05,
      "loss": 2.7062,
      "step": 287
    },
    {
      "epoch": 0.026243849097867686,
      "grad_norm": 3.324129581451416,
      "learning_rate": 4.991507827734058e-05,
      "loss": 3.0118,
      "step": 288
    },
    {
      "epoch": 0.02633497357390195,
      "grad_norm": 3.6669745445251465,
      "learning_rate": 4.99144878563211e-05,
      "loss": 2.8931,
      "step": 289
    },
    {
      "epoch": 0.026426098049936213,
      "grad_norm": 3.859177350997925,
      "learning_rate": 4.99138953934612e-05,
      "loss": 1.8732,
      "step": 290
    },
    {
      "epoch": 0.026517222525970476,
      "grad_norm": 3.1230263710021973,
      "learning_rate": 4.991330088880945e-05,
      "loss": 3.0979,
      "step": 291
    },
    {
      "epoch": 0.026608347002004738,
      "grad_norm": 6.740344524383545,
      "learning_rate": 4.9912704342414565e-05,
      "loss": 3.7346,
      "step": 292
    },
    {
      "epoch": 0.026699471478039,
      "grad_norm": 2.1976635456085205,
      "learning_rate": 4.9912105754325435e-05,
      "loss": 2.9069,
      "step": 293
    },
    {
      "epoch": 0.026790595954073265,
      "grad_norm": 3.5679171085357666,
      "learning_rate": 4.991150512459111e-05,
      "loss": 3.3382,
      "step": 294
    },
    {
      "epoch": 0.026881720430107527,
      "grad_norm": 2.9355552196502686,
      "learning_rate": 4.9910902453260824e-05,
      "loss": 2.7731,
      "step": 295
    },
    {
      "epoch": 0.02697284490614179,
      "grad_norm": 3.473723888397217,
      "learning_rate": 4.991029774038397e-05,
      "loss": 2.995,
      "step": 296
    },
    {
      "epoch": 0.02706396938217605,
      "grad_norm": 3.0785019397735596,
      "learning_rate": 4.9909690986010095e-05,
      "loss": 2.7569,
      "step": 297
    },
    {
      "epoch": 0.027155093858210317,
      "grad_norm": 2.493504762649536,
      "learning_rate": 4.990908219018894e-05,
      "loss": 2.9982,
      "step": 298
    },
    {
      "epoch": 0.02724621833424458,
      "grad_norm": 2.2580089569091797,
      "learning_rate": 4.990847135297038e-05,
      "loss": 2.5264,
      "step": 299
    },
    {
      "epoch": 0.02733734281027884,
      "grad_norm": 2.80513858795166,
      "learning_rate": 4.990785847440449e-05,
      "loss": 2.6634,
      "step": 300
    },
    {
      "epoch": 0.027428467286313103,
      "grad_norm": 2.0804553031921387,
      "learning_rate": 4.9907243554541486e-05,
      "loss": 2.9459,
      "step": 301
    },
    {
      "epoch": 0.027519591762347365,
      "grad_norm": 3.045847177505493,
      "learning_rate": 4.9906626593431776e-05,
      "loss": 2.611,
      "step": 302
    },
    {
      "epoch": 0.02761071623838163,
      "grad_norm": 2.9146780967712402,
      "learning_rate": 4.9906007591125915e-05,
      "loss": 2.7193,
      "step": 303
    },
    {
      "epoch": 0.027701840714415893,
      "grad_norm": 2.089606761932373,
      "learning_rate": 4.990538654767464e-05,
      "loss": 2.9351,
      "step": 304
    },
    {
      "epoch": 0.027792965190450155,
      "grad_norm": 4.3602471351623535,
      "learning_rate": 4.9904763463128826e-05,
      "loss": 2.4889,
      "step": 305
    },
    {
      "epoch": 0.027884089666484417,
      "grad_norm": 2.540752410888672,
      "learning_rate": 4.9904138337539566e-05,
      "loss": 3.1232,
      "step": 306
    },
    {
      "epoch": 0.02797521414251868,
      "grad_norm": 3.7896921634674072,
      "learning_rate": 4.9903511170958074e-05,
      "loss": 3.2954,
      "step": 307
    },
    {
      "epoch": 0.028066338618552945,
      "grad_norm": 2.222221612930298,
      "learning_rate": 4.990288196343575e-05,
      "loss": 2.8993,
      "step": 308
    },
    {
      "epoch": 0.028157463094587207,
      "grad_norm": 3.169107437133789,
      "learning_rate": 4.990225071502418e-05,
      "loss": 2.6121,
      "step": 309
    },
    {
      "epoch": 0.02824858757062147,
      "grad_norm": 2.2904651165008545,
      "learning_rate": 4.9901617425775067e-05,
      "loss": 3.0291,
      "step": 310
    },
    {
      "epoch": 0.02833971204665573,
      "grad_norm": 2.019195556640625,
      "learning_rate": 4.990098209574033e-05,
      "loss": 2.8676,
      "step": 311
    },
    {
      "epoch": 0.028430836522689993,
      "grad_norm": 3.177980661392212,
      "learning_rate": 4.9900344724972024e-05,
      "loss": 2.7502,
      "step": 312
    },
    {
      "epoch": 0.02852196099872426,
      "grad_norm": 4.665421485900879,
      "learning_rate": 4.989970531352241e-05,
      "loss": 2.7847,
      "step": 313
    },
    {
      "epoch": 0.02861308547475852,
      "grad_norm": 3.270128011703491,
      "learning_rate": 4.9899063861443854e-05,
      "loss": 2.2844,
      "step": 314
    },
    {
      "epoch": 0.028704209950792783,
      "grad_norm": 4.186242580413818,
      "learning_rate": 4.989842036878895e-05,
      "loss": 3.1222,
      "step": 315
    },
    {
      "epoch": 0.028795334426827045,
      "grad_norm": 3.346442937850952,
      "learning_rate": 4.989777483561043e-05,
      "loss": 3.1899,
      "step": 316
    },
    {
      "epoch": 0.028886458902861307,
      "grad_norm": 4.0838541984558105,
      "learning_rate": 4.9897127261961196e-05,
      "loss": 2.7816,
      "step": 317
    },
    {
      "epoch": 0.028977583378895572,
      "grad_norm": 2.0978620052337646,
      "learning_rate": 4.989647764789432e-05,
      "loss": 2.9921,
      "step": 318
    },
    {
      "epoch": 0.029068707854929834,
      "grad_norm": 2.830665349960327,
      "learning_rate": 4.989582599346304e-05,
      "loss": 1.9849,
      "step": 319
    },
    {
      "epoch": 0.029159832330964097,
      "grad_norm": 2.3662290573120117,
      "learning_rate": 4.989517229872076e-05,
      "loss": 3.0048,
      "step": 320
    },
    {
      "epoch": 0.02925095680699836,
      "grad_norm": 3.844538927078247,
      "learning_rate": 4.989451656372106e-05,
      "loss": 2.8494,
      "step": 321
    },
    {
      "epoch": 0.029342081283032624,
      "grad_norm": 6.312561511993408,
      "learning_rate": 4.989385878851767e-05,
      "loss": 2.9627,
      "step": 322
    },
    {
      "epoch": 0.029433205759066886,
      "grad_norm": 3.0908846855163574,
      "learning_rate": 4.98931989731645e-05,
      "loss": 2.974,
      "step": 323
    },
    {
      "epoch": 0.02952433023510115,
      "grad_norm": 2.5474183559417725,
      "learning_rate": 4.989253711771563e-05,
      "loss": 2.8485,
      "step": 324
    },
    {
      "epoch": 0.02961545471113541,
      "grad_norm": 2.3893277645111084,
      "learning_rate": 4.98918732222253e-05,
      "loss": 2.8714,
      "step": 325
    },
    {
      "epoch": 0.029706579187169672,
      "grad_norm": 2.8498189449310303,
      "learning_rate": 4.989120728674792e-05,
      "loss": 2.567,
      "step": 326
    },
    {
      "epoch": 0.029797703663203938,
      "grad_norm": 3.217600107192993,
      "learning_rate": 4.989053931133806e-05,
      "loss": 2.7916,
      "step": 327
    },
    {
      "epoch": 0.0298888281392382,
      "grad_norm": 3.3806464672088623,
      "learning_rate": 4.988986929605047e-05,
      "loss": 2.6764,
      "step": 328
    },
    {
      "epoch": 0.029979952615272462,
      "grad_norm": 3.6862692832946777,
      "learning_rate": 4.988919724094005e-05,
      "loss": 2.8749,
      "step": 329
    },
    {
      "epoch": 0.030071077091306724,
      "grad_norm": 2.995811939239502,
      "learning_rate": 4.9888523146061885e-05,
      "loss": 3.0403,
      "step": 330
    },
    {
      "epoch": 0.030162201567340986,
      "grad_norm": 2.922734498977661,
      "learning_rate": 4.988784701147122e-05,
      "loss": 2.792,
      "step": 331
    },
    {
      "epoch": 0.030253326043375252,
      "grad_norm": 2.79219651222229,
      "learning_rate": 4.988716883722348e-05,
      "loss": 2.9411,
      "step": 332
    },
    {
      "epoch": 0.030344450519409514,
      "grad_norm": 4.6752495765686035,
      "learning_rate": 4.9886488623374214e-05,
      "loss": 3.1196,
      "step": 333
    },
    {
      "epoch": 0.030435574995443776,
      "grad_norm": 2.8725225925445557,
      "learning_rate": 4.988580636997918e-05,
      "loss": 2.7962,
      "step": 334
    },
    {
      "epoch": 0.030526699471478038,
      "grad_norm": 3.8501832485198975,
      "learning_rate": 4.9885122077094304e-05,
      "loss": 2.6243,
      "step": 335
    },
    {
      "epoch": 0.0306178239475123,
      "grad_norm": 2.141561985015869,
      "learning_rate": 4.988443574477566e-05,
      "loss": 3.0925,
      "step": 336
    },
    {
      "epoch": 0.030708948423546566,
      "grad_norm": 4.128694534301758,
      "learning_rate": 4.988374737307949e-05,
      "loss": 2.985,
      "step": 337
    },
    {
      "epoch": 0.030800072899580828,
      "grad_norm": 2.3043148517608643,
      "learning_rate": 4.9883056962062213e-05,
      "loss": 2.9108,
      "step": 338
    },
    {
      "epoch": 0.03089119737561509,
      "grad_norm": 2.1815946102142334,
      "learning_rate": 4.9882364511780414e-05,
      "loss": 2.9105,
      "step": 339
    },
    {
      "epoch": 0.030982321851649352,
      "grad_norm": 3.693887710571289,
      "learning_rate": 4.9881670022290836e-05,
      "loss": 2.8508,
      "step": 340
    },
    {
      "epoch": 0.031073446327683617,
      "grad_norm": 3.411998748779297,
      "learning_rate": 4.9880973493650394e-05,
      "loss": 3.1107,
      "step": 341
    },
    {
      "epoch": 0.03116457080371788,
      "grad_norm": 3.362623453140259,
      "learning_rate": 4.9880274925916183e-05,
      "loss": 2.5887,
      "step": 342
    },
    {
      "epoch": 0.03125569527975214,
      "grad_norm": 2.2149298191070557,
      "learning_rate": 4.987957431914544e-05,
      "loss": 2.9536,
      "step": 343
    },
    {
      "epoch": 0.031346819755786404,
      "grad_norm": 4.359997749328613,
      "learning_rate": 4.9878871673395586e-05,
      "loss": 3.2973,
      "step": 344
    },
    {
      "epoch": 0.03143794423182067,
      "grad_norm": 4.717598915100098,
      "learning_rate": 4.987816698872421e-05,
      "loss": 1.9499,
      "step": 345
    },
    {
      "epoch": 0.03152906870785493,
      "grad_norm": 3.7561376094818115,
      "learning_rate": 4.9877460265189064e-05,
      "loss": 2.6827,
      "step": 346
    },
    {
      "epoch": 0.03162019318388919,
      "grad_norm": 3.771298408508301,
      "learning_rate": 4.9876751502848064e-05,
      "loss": 2.5356,
      "step": 347
    },
    {
      "epoch": 0.03171131765992345,
      "grad_norm": 3.1962294578552246,
      "learning_rate": 4.9876040701759286e-05,
      "loss": 2.7013,
      "step": 348
    },
    {
      "epoch": 0.03180244213595772,
      "grad_norm": 4.098245143890381,
      "learning_rate": 4.9875327861981006e-05,
      "loss": 2.8326,
      "step": 349
    },
    {
      "epoch": 0.03189356661199198,
      "grad_norm": 2.9347190856933594,
      "learning_rate": 4.9874612983571614e-05,
      "loss": 2.6662,
      "step": 350
    },
    {
      "epoch": 0.03198469108802624,
      "grad_norm": 3.6982884407043457,
      "learning_rate": 4.9873896066589725e-05,
      "loss": 3.0266,
      "step": 351
    },
    {
      "epoch": 0.03207581556406051,
      "grad_norm": 3.387371778488159,
      "learning_rate": 4.987317711109408e-05,
      "loss": 2.7588,
      "step": 352
    },
    {
      "epoch": 0.03216694004009477,
      "grad_norm": 4.407079219818115,
      "learning_rate": 4.9872456117143607e-05,
      "loss": 3.1829,
      "step": 353
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 3.283046007156372,
      "learning_rate": 4.987173308479738e-05,
      "loss": 1.7902,
      "step": 354
    },
    {
      "epoch": 0.0323491889921633,
      "grad_norm": 3.132047653198242,
      "learning_rate": 4.987100801411467e-05,
      "loss": 2.5832,
      "step": 355
    },
    {
      "epoch": 0.032440313468197556,
      "grad_norm": 3.831251859664917,
      "learning_rate": 4.9870280905154886e-05,
      "loss": 2.804,
      "step": 356
    },
    {
      "epoch": 0.03253143794423182,
      "grad_norm": 4.598052024841309,
      "learning_rate": 4.986955175797763e-05,
      "loss": 2.3582,
      "step": 357
    },
    {
      "epoch": 0.03262256242026609,
      "grad_norm": 3.8844218254089355,
      "learning_rate": 4.986882057264266e-05,
      "loss": 2.5354,
      "step": 358
    },
    {
      "epoch": 0.032713686896300345,
      "grad_norm": 3.4492027759552,
      "learning_rate": 4.986808734920988e-05,
      "loss": 2.952,
      "step": 359
    },
    {
      "epoch": 0.03280481137233461,
      "grad_norm": 3.0955710411071777,
      "learning_rate": 4.9867352087739405e-05,
      "loss": 2.7746,
      "step": 360
    },
    {
      "epoch": 0.03289593584836887,
      "grad_norm": 2.497973680496216,
      "learning_rate": 4.986661478829147e-05,
      "loss": 2.9674,
      "step": 361
    },
    {
      "epoch": 0.032987060324403135,
      "grad_norm": 4.156097412109375,
      "learning_rate": 4.986587545092651e-05,
      "loss": 3.0256,
      "step": 362
    },
    {
      "epoch": 0.0330781848004374,
      "grad_norm": 2.2689907550811768,
      "learning_rate": 4.986513407570513e-05,
      "loss": 2.888,
      "step": 363
    },
    {
      "epoch": 0.03316930927647166,
      "grad_norm": 5.262107849121094,
      "learning_rate": 4.986439066268807e-05,
      "loss": 3.0598,
      "step": 364
    },
    {
      "epoch": 0.033260433752505925,
      "grad_norm": 2.9902825355529785,
      "learning_rate": 4.9863645211936254e-05,
      "loss": 3.0898,
      "step": 365
    },
    {
      "epoch": 0.03335155822854018,
      "grad_norm": 1.6874420642852783,
      "learning_rate": 4.986289772351079e-05,
      "loss": 2.7454,
      "step": 366
    },
    {
      "epoch": 0.03344268270457445,
      "grad_norm": 3.7136764526367188,
      "learning_rate": 4.986214819747293e-05,
      "loss": 2.9232,
      "step": 367
    },
    {
      "epoch": 0.033533807180608714,
      "grad_norm": 2.174556255340576,
      "learning_rate": 4.986139663388409e-05,
      "loss": 2.9913,
      "step": 368
    },
    {
      "epoch": 0.03362493165664297,
      "grad_norm": 3.348562240600586,
      "learning_rate": 4.986064303280588e-05,
      "loss": 3.0657,
      "step": 369
    },
    {
      "epoch": 0.03371605613267724,
      "grad_norm": 2.8100669384002686,
      "learning_rate": 4.9859887394300055e-05,
      "loss": 2.8318,
      "step": 370
    },
    {
      "epoch": 0.0338071806087115,
      "grad_norm": 3.4799964427948,
      "learning_rate": 4.9859129718428536e-05,
      "loss": 2.8546,
      "step": 371
    },
    {
      "epoch": 0.03389830508474576,
      "grad_norm": 3.3681960105895996,
      "learning_rate": 4.985837000525343e-05,
      "loss": 2.4564,
      "step": 372
    },
    {
      "epoch": 0.03398942956078003,
      "grad_norm": 3.261798620223999,
      "learning_rate": 4.985760825483699e-05,
      "loss": 2.6388,
      "step": 373
    },
    {
      "epoch": 0.03408055403681429,
      "grad_norm": 2.5790557861328125,
      "learning_rate": 4.985684446724165e-05,
      "loss": 2.9411,
      "step": 374
    },
    {
      "epoch": 0.03417167851284855,
      "grad_norm": 3.773350715637207,
      "learning_rate": 4.985607864252999e-05,
      "loss": 2.8002,
      "step": 375
    },
    {
      "epoch": 0.03426280298888281,
      "grad_norm": 3.27166485786438,
      "learning_rate": 4.9855310780764794e-05,
      "loss": 2.7819,
      "step": 376
    },
    {
      "epoch": 0.034353927464917076,
      "grad_norm": 3.0358798503875732,
      "learning_rate": 4.985454088200898e-05,
      "loss": 2.918,
      "step": 377
    },
    {
      "epoch": 0.03444505194095134,
      "grad_norm": 2.6746315956115723,
      "learning_rate": 4.985376894632564e-05,
      "loss": 2.6342,
      "step": 378
    },
    {
      "epoch": 0.0345361764169856,
      "grad_norm": 3.8095743656158447,
      "learning_rate": 4.985299497377805e-05,
      "loss": 2.7959,
      "step": 379
    },
    {
      "epoch": 0.034627300893019866,
      "grad_norm": 4.985168933868408,
      "learning_rate": 4.985221896442963e-05,
      "loss": 2.5482,
      "step": 380
    },
    {
      "epoch": 0.034718425369054125,
      "grad_norm": 2.8703434467315674,
      "learning_rate": 4.9851440918343985e-05,
      "loss": 3.3517,
      "step": 381
    },
    {
      "epoch": 0.03480954984508839,
      "grad_norm": 3.232175827026367,
      "learning_rate": 4.985066083558486e-05,
      "loss": 2.5102,
      "step": 382
    },
    {
      "epoch": 0.034900674321122656,
      "grad_norm": 2.980302095413208,
      "learning_rate": 4.9849878716216215e-05,
      "loss": 2.7518,
      "step": 383
    },
    {
      "epoch": 0.034991798797156914,
      "grad_norm": 2.309779167175293,
      "learning_rate": 4.9849094560302124e-05,
      "loss": 2.9199,
      "step": 384
    },
    {
      "epoch": 0.03508292327319118,
      "grad_norm": 3.3376975059509277,
      "learning_rate": 4.984830836790686e-05,
      "loss": 2.9014,
      "step": 385
    },
    {
      "epoch": 0.03517404774922544,
      "grad_norm": 3.1377711296081543,
      "learning_rate": 4.984752013909485e-05,
      "loss": 3.1008,
      "step": 386
    },
    {
      "epoch": 0.035265172225259704,
      "grad_norm": 3.804525136947632,
      "learning_rate": 4.9846729873930706e-05,
      "loss": 2.6573,
      "step": 387
    },
    {
      "epoch": 0.03535629670129397,
      "grad_norm": 2.8694443702697754,
      "learning_rate": 4.984593757247918e-05,
      "loss": 2.8805,
      "step": 388
    },
    {
      "epoch": 0.03544742117732823,
      "grad_norm": 2.8578102588653564,
      "learning_rate": 4.9845143234805216e-05,
      "loss": 2.6417,
      "step": 389
    },
    {
      "epoch": 0.035538545653362494,
      "grad_norm": 1.6129778623580933,
      "learning_rate": 4.9844346860973896e-05,
      "loss": 2.5198,
      "step": 390
    },
    {
      "epoch": 0.03562967012939676,
      "grad_norm": 3.4736506938934326,
      "learning_rate": 4.98435484510505e-05,
      "loss": 2.0989,
      "step": 391
    },
    {
      "epoch": 0.03572079460543102,
      "grad_norm": 2.6138317584991455,
      "learning_rate": 4.984274800510046e-05,
      "loss": 2.9925,
      "step": 392
    },
    {
      "epoch": 0.035811919081465284,
      "grad_norm": 4.978623867034912,
      "learning_rate": 4.984194552318936e-05,
      "loss": 2.3243,
      "step": 393
    },
    {
      "epoch": 0.03590304355749954,
      "grad_norm": 1.921769142150879,
      "learning_rate": 4.984114100538299e-05,
      "loss": 2.95,
      "step": 394
    },
    {
      "epoch": 0.03599416803353381,
      "grad_norm": 3.2645325660705566,
      "learning_rate": 4.984033445174727e-05,
      "loss": 2.4694,
      "step": 395
    },
    {
      "epoch": 0.03608529250956807,
      "grad_norm": 2.63525390625,
      "learning_rate": 4.9839525862348304e-05,
      "loss": 2.4872,
      "step": 396
    },
    {
      "epoch": 0.03617641698560233,
      "grad_norm": 3.7979824542999268,
      "learning_rate": 4.983871523725235e-05,
      "loss": 2.399,
      "step": 397
    },
    {
      "epoch": 0.0362675414616366,
      "grad_norm": 3.695284128189087,
      "learning_rate": 4.983790257652585e-05,
      "loss": 3.2067,
      "step": 398
    },
    {
      "epoch": 0.036358665937670856,
      "grad_norm": 4.359871864318848,
      "learning_rate": 4.9837087880235414e-05,
      "loss": 2.5611,
      "step": 399
    },
    {
      "epoch": 0.03644979041370512,
      "grad_norm": 3.501504898071289,
      "learning_rate": 4.983627114844779e-05,
      "loss": 2.2389,
      "step": 400
    },
    {
      "epoch": 0.03654091488973939,
      "grad_norm": 3.637976884841919,
      "learning_rate": 4.983545238122993e-05,
      "loss": 2.4473,
      "step": 401
    },
    {
      "epoch": 0.036632039365773646,
      "grad_norm": 4.534337043762207,
      "learning_rate": 4.983463157864893e-05,
      "loss": 3.0207,
      "step": 402
    },
    {
      "epoch": 0.03672316384180791,
      "grad_norm": 4.795605659484863,
      "learning_rate": 4.983380874077204e-05,
      "loss": 2.7606,
      "step": 403
    },
    {
      "epoch": 0.03681428831784217,
      "grad_norm": 2.7595343589782715,
      "learning_rate": 4.983298386766672e-05,
      "loss": 3.0888,
      "step": 404
    },
    {
      "epoch": 0.036905412793876435,
      "grad_norm": 4.718511581420898,
      "learning_rate": 4.983215695940057e-05,
      "loss": 2.9122,
      "step": 405
    },
    {
      "epoch": 0.0369965372699107,
      "grad_norm": 3.5803143978118896,
      "learning_rate": 4.9831328016041335e-05,
      "loss": 2.9831,
      "step": 406
    },
    {
      "epoch": 0.03708766174594496,
      "grad_norm": 3.076673984527588,
      "learning_rate": 4.983049703765697e-05,
      "loss": 2.1278,
      "step": 407
    },
    {
      "epoch": 0.037178786221979225,
      "grad_norm": 2.236630439758301,
      "learning_rate": 4.9829664024315575e-05,
      "loss": 3.0333,
      "step": 408
    },
    {
      "epoch": 0.037269910698013484,
      "grad_norm": 3.2108254432678223,
      "learning_rate": 4.982882897608542e-05,
      "loss": 2.612,
      "step": 409
    },
    {
      "epoch": 0.03736103517404775,
      "grad_norm": 4.408178806304932,
      "learning_rate": 4.982799189303493e-05,
      "loss": 2.7848,
      "step": 410
    },
    {
      "epoch": 0.037452159650082015,
      "grad_norm": 3.5266246795654297,
      "learning_rate": 4.9827152775232714e-05,
      "loss": 2.8434,
      "step": 411
    },
    {
      "epoch": 0.03754328412611627,
      "grad_norm": 5.857843399047852,
      "learning_rate": 4.982631162274753e-05,
      "loss": 2.9037,
      "step": 412
    },
    {
      "epoch": 0.03763440860215054,
      "grad_norm": 2.262103796005249,
      "learning_rate": 4.982546843564834e-05,
      "loss": 2.9968,
      "step": 413
    },
    {
      "epoch": 0.0377255330781848,
      "grad_norm": 4.400634288787842,
      "learning_rate": 4.982462321400423e-05,
      "loss": 2.9611,
      "step": 414
    },
    {
      "epoch": 0.03781665755421906,
      "grad_norm": 1.9334667921066284,
      "learning_rate": 4.982377595788447e-05,
      "loss": 2.8443,
      "step": 415
    },
    {
      "epoch": 0.03790778203025333,
      "grad_norm": 3.3641562461853027,
      "learning_rate": 4.9822926667358494e-05,
      "loss": 2.8691,
      "step": 416
    },
    {
      "epoch": 0.03799890650628759,
      "grad_norm": 3.93518328666687,
      "learning_rate": 4.98220753424959e-05,
      "loss": 2.7432,
      "step": 417
    },
    {
      "epoch": 0.03809003098232185,
      "grad_norm": 3.3891704082489014,
      "learning_rate": 4.982122198336647e-05,
      "loss": 3.1939,
      "step": 418
    },
    {
      "epoch": 0.03818115545835611,
      "grad_norm": 2.4883248805999756,
      "learning_rate": 4.982036659004014e-05,
      "loss": 2.8615,
      "step": 419
    },
    {
      "epoch": 0.03827227993439038,
      "grad_norm": 2.802154064178467,
      "learning_rate": 4.9819509162587e-05,
      "loss": 3.0744,
      "step": 420
    },
    {
      "epoch": 0.03836340441042464,
      "grad_norm": 4.077276706695557,
      "learning_rate": 4.981864970107733e-05,
      "loss": 2.6231,
      "step": 421
    },
    {
      "epoch": 0.0384545288864589,
      "grad_norm": 3.337916135787964,
      "learning_rate": 4.981778820558156e-05,
      "loss": 2.8649,
      "step": 422
    },
    {
      "epoch": 0.03854565336249317,
      "grad_norm": 3.7240352630615234,
      "learning_rate": 4.98169246761703e-05,
      "loss": 3.0618,
      "step": 423
    },
    {
      "epoch": 0.038636777838527425,
      "grad_norm": 1.917153239250183,
      "learning_rate": 4.981605911291432e-05,
      "loss": 3.0332,
      "step": 424
    },
    {
      "epoch": 0.03872790231456169,
      "grad_norm": 3.458873987197876,
      "learning_rate": 4.981519151588454e-05,
      "loss": 2.77,
      "step": 425
    },
    {
      "epoch": 0.038819026790595956,
      "grad_norm": 1.8529995679855347,
      "learning_rate": 4.9814321885152085e-05,
      "loss": 2.8618,
      "step": 426
    },
    {
      "epoch": 0.038910151266630215,
      "grad_norm": 2.460031509399414,
      "learning_rate": 4.981345022078821e-05,
      "loss": 2.8602,
      "step": 427
    },
    {
      "epoch": 0.03900127574266448,
      "grad_norm": 3.4180796146392822,
      "learning_rate": 4.981257652286436e-05,
      "loss": 2.6805,
      "step": 428
    },
    {
      "epoch": 0.03909240021869874,
      "grad_norm": 3.609208345413208,
      "learning_rate": 4.981170079145213e-05,
      "loss": 2.8997,
      "step": 429
    },
    {
      "epoch": 0.039183524694733005,
      "grad_norm": 4.643214225769043,
      "learning_rate": 4.981082302662329e-05,
      "loss": 3.1809,
      "step": 430
    },
    {
      "epoch": 0.03927464917076727,
      "grad_norm": 3.2911436557769775,
      "learning_rate": 4.980994322844979e-05,
      "loss": 2.982,
      "step": 431
    },
    {
      "epoch": 0.03936577364680153,
      "grad_norm": 3.3252878189086914,
      "learning_rate": 4.980906139700372e-05,
      "loss": 3.1547,
      "step": 432
    },
    {
      "epoch": 0.039456898122835794,
      "grad_norm": 2.907970666885376,
      "learning_rate": 4.980817753235735e-05,
      "loss": 3.1306,
      "step": 433
    },
    {
      "epoch": 0.03954802259887006,
      "grad_norm": 2.1587812900543213,
      "learning_rate": 4.980729163458312e-05,
      "loss": 3.1019,
      "step": 434
    },
    {
      "epoch": 0.03963914707490432,
      "grad_norm": 2.8087103366851807,
      "learning_rate": 4.9806403703753624e-05,
      "loss": 3.092,
      "step": 435
    },
    {
      "epoch": 0.039730271550938584,
      "grad_norm": 3.1665287017822266,
      "learning_rate": 4.980551373994164e-05,
      "loss": 2.8803,
      "step": 436
    },
    {
      "epoch": 0.03982139602697284,
      "grad_norm": 3.015409231185913,
      "learning_rate": 4.980462174322011e-05,
      "loss": 3.0123,
      "step": 437
    },
    {
      "epoch": 0.03991252050300711,
      "grad_norm": 6.592101573944092,
      "learning_rate": 4.980372771366213e-05,
      "loss": 3.0624,
      "step": 438
    },
    {
      "epoch": 0.040003644979041374,
      "grad_norm": 2.965012788772583,
      "learning_rate": 4.980283165134097e-05,
      "loss": 3.0143,
      "step": 439
    },
    {
      "epoch": 0.04009476945507563,
      "grad_norm": 1.9055732488632202,
      "learning_rate": 4.980193355633006e-05,
      "loss": 2.9194,
      "step": 440
    },
    {
      "epoch": 0.0401858939311099,
      "grad_norm": 2.0998048782348633,
      "learning_rate": 4.980103342870301e-05,
      "loss": 2.6373,
      "step": 441
    },
    {
      "epoch": 0.040277018407144156,
      "grad_norm": 1.6069968938827515,
      "learning_rate": 4.980013126853358e-05,
      "loss": 2.4255,
      "step": 442
    },
    {
      "epoch": 0.04036814288317842,
      "grad_norm": 2.8056509494781494,
      "learning_rate": 4.9799227075895714e-05,
      "loss": 2.4602,
      "step": 443
    },
    {
      "epoch": 0.04045926735921269,
      "grad_norm": 2.874981641769409,
      "learning_rate": 4.979832085086352e-05,
      "loss": 2.6648,
      "step": 444
    },
    {
      "epoch": 0.040550391835246946,
      "grad_norm": 4.484694004058838,
      "learning_rate": 4.979741259351125e-05,
      "loss": 3.4229,
      "step": 445
    },
    {
      "epoch": 0.04064151631128121,
      "grad_norm": 3.391519546508789,
      "learning_rate": 4.979650230391335e-05,
      "loss": 2.7623,
      "step": 446
    },
    {
      "epoch": 0.04073264078731547,
      "grad_norm": 3.0873026847839355,
      "learning_rate": 4.979558998214442e-05,
      "loss": 2.5945,
      "step": 447
    },
    {
      "epoch": 0.040823765263349736,
      "grad_norm": 2.205383777618408,
      "learning_rate": 4.979467562827923e-05,
      "loss": 2.9883,
      "step": 448
    },
    {
      "epoch": 0.040914889739384,
      "grad_norm": 3.873610019683838,
      "learning_rate": 4.979375924239271e-05,
      "loss": 3.1105,
      "step": 449
    },
    {
      "epoch": 0.04100601421541826,
      "grad_norm": 2.3900392055511475,
      "learning_rate": 4.979284082455996e-05,
      "loss": 3.0243,
      "step": 450
    },
    {
      "epoch": 0.041097138691452526,
      "grad_norm": 3.442873477935791,
      "learning_rate": 4.979192037485626e-05,
      "loss": 2.9851,
      "step": 451
    },
    {
      "epoch": 0.041188263167486784,
      "grad_norm": 2.941448450088501,
      "learning_rate": 4.979099789335703e-05,
      "loss": 2.648,
      "step": 452
    },
    {
      "epoch": 0.04127938764352105,
      "grad_norm": 2.6990129947662354,
      "learning_rate": 4.979007338013788e-05,
      "loss": 2.765,
      "step": 453
    },
    {
      "epoch": 0.041370512119555315,
      "grad_norm": 2.8547112941741943,
      "learning_rate": 4.978914683527458e-05,
      "loss": 2.6851,
      "step": 454
    },
    {
      "epoch": 0.041461636595589574,
      "grad_norm": 1.7790765762329102,
      "learning_rate": 4.978821825884306e-05,
      "loss": 2.897,
      "step": 455
    },
    {
      "epoch": 0.04155276107162384,
      "grad_norm": 3.662311553955078,
      "learning_rate": 4.978728765091941e-05,
      "loss": 2.3728,
      "step": 456
    },
    {
      "epoch": 0.0416438855476581,
      "grad_norm": 3.8467187881469727,
      "learning_rate": 4.978635501157991e-05,
      "loss": 2.6218,
      "step": 457
    },
    {
      "epoch": 0.041735010023692364,
      "grad_norm": 3.8221049308776855,
      "learning_rate": 4.978542034090099e-05,
      "loss": 2.816,
      "step": 458
    },
    {
      "epoch": 0.04182613449972663,
      "grad_norm": 4.882442951202393,
      "learning_rate": 4.9784483638959254e-05,
      "loss": 2.9258,
      "step": 459
    },
    {
      "epoch": 0.04191725897576089,
      "grad_norm": 2.9506404399871826,
      "learning_rate": 4.978354490583146e-05,
      "loss": 2.9122,
      "step": 460
    },
    {
      "epoch": 0.04200838345179515,
      "grad_norm": 3.596014976501465,
      "learning_rate": 4.978260414159455e-05,
      "loss": 2.9199,
      "step": 461
    },
    {
      "epoch": 0.04209950792782941,
      "grad_norm": 2.6444709300994873,
      "learning_rate": 4.978166134632562e-05,
      "loss": 2.9953,
      "step": 462
    },
    {
      "epoch": 0.04219063240386368,
      "grad_norm": 3.1696035861968994,
      "learning_rate": 4.978071652010193e-05,
      "loss": 3.0076,
      "step": 463
    },
    {
      "epoch": 0.04228175687989794,
      "grad_norm": 1.9968247413635254,
      "learning_rate": 4.977976966300092e-05,
      "loss": 2.8129,
      "step": 464
    },
    {
      "epoch": 0.0423728813559322,
      "grad_norm": 3.2720398902893066,
      "learning_rate": 4.9778820775100185e-05,
      "loss": 3.01,
      "step": 465
    },
    {
      "epoch": 0.04246400583196647,
      "grad_norm": 3.994105577468872,
      "learning_rate": 4.9777869856477485e-05,
      "loss": 2.5902,
      "step": 466
    },
    {
      "epoch": 0.042555130308000726,
      "grad_norm": 3.4857635498046875,
      "learning_rate": 4.977691690721076e-05,
      "loss": 2.8994,
      "step": 467
    },
    {
      "epoch": 0.04264625478403499,
      "grad_norm": 3.149409294128418,
      "learning_rate": 4.977596192737811e-05,
      "loss": 2.7171,
      "step": 468
    },
    {
      "epoch": 0.04273737926006926,
      "grad_norm": 3.7346558570861816,
      "learning_rate": 4.9775004917057786e-05,
      "loss": 3.0065,
      "step": 469
    },
    {
      "epoch": 0.042828503736103515,
      "grad_norm": 4.905463695526123,
      "learning_rate": 4.977404587632824e-05,
      "loss": 2.5023,
      "step": 470
    },
    {
      "epoch": 0.04291962821213778,
      "grad_norm": 3.6714894771575928,
      "learning_rate": 4.9773084805268045e-05,
      "loss": 2.1294,
      "step": 471
    },
    {
      "epoch": 0.043010752688172046,
      "grad_norm": 2.8621413707733154,
      "learning_rate": 4.977212170395598e-05,
      "loss": 2.8082,
      "step": 472
    },
    {
      "epoch": 0.043101877164206305,
      "grad_norm": 2.972755193710327,
      "learning_rate": 4.9771156572470966e-05,
      "loss": 2.8794,
      "step": 473
    },
    {
      "epoch": 0.04319300164024057,
      "grad_norm": 1.98660409450531,
      "learning_rate": 4.97701894108921e-05,
      "loss": 2.9151,
      "step": 474
    },
    {
      "epoch": 0.04328412611627483,
      "grad_norm": 3.258434534072876,
      "learning_rate": 4.9769220219298666e-05,
      "loss": 2.8181,
      "step": 475
    },
    {
      "epoch": 0.043375250592309095,
      "grad_norm": 2.2653932571411133,
      "learning_rate": 4.9768248997770063e-05,
      "loss": 3.051,
      "step": 476
    },
    {
      "epoch": 0.04346637506834336,
      "grad_norm": 4.378432273864746,
      "learning_rate": 4.97672757463859e-05,
      "loss": 2.7252,
      "step": 477
    },
    {
      "epoch": 0.04355749954437762,
      "grad_norm": 5.332482814788818,
      "learning_rate": 4.976630046522594e-05,
      "loss": 2.9468,
      "step": 478
    },
    {
      "epoch": 0.043648624020411884,
      "grad_norm": 2.8985297679901123,
      "learning_rate": 4.9765323154370114e-05,
      "loss": 2.782,
      "step": 479
    },
    {
      "epoch": 0.04373974849644614,
      "grad_norm": 4.3086137771606445,
      "learning_rate": 4.976434381389851e-05,
      "loss": 3.0355,
      "step": 480
    },
    {
      "epoch": 0.04383087297248041,
      "grad_norm": 4.562249660491943,
      "learning_rate": 4.976336244389138e-05,
      "loss": 2.6953,
      "step": 481
    },
    {
      "epoch": 0.043921997448514674,
      "grad_norm": 3.0404744148254395,
      "learning_rate": 4.9762379044429174e-05,
      "loss": 2.9207,
      "step": 482
    },
    {
      "epoch": 0.04401312192454893,
      "grad_norm": 2.2012205123901367,
      "learning_rate": 4.9761393615592465e-05,
      "loss": 2.8434,
      "step": 483
    },
    {
      "epoch": 0.0441042464005832,
      "grad_norm": 2.0561892986297607,
      "learning_rate": 4.9760406157462024e-05,
      "loss": 2.7924,
      "step": 484
    },
    {
      "epoch": 0.04419537087661746,
      "grad_norm": 7.559401988983154,
      "learning_rate": 4.975941667011877e-05,
      "loss": 2.9499,
      "step": 485
    },
    {
      "epoch": 0.04428649535265172,
      "grad_norm": 3.387563943862915,
      "learning_rate": 4.9758425153643804e-05,
      "loss": 2.935,
      "step": 486
    },
    {
      "epoch": 0.04437761982868599,
      "grad_norm": 3.8939521312713623,
      "learning_rate": 4.975743160811839e-05,
      "loss": 3.1682,
      "step": 487
    },
    {
      "epoch": 0.04446874430472025,
      "grad_norm": 4.640250205993652,
      "learning_rate": 4.975643603362393e-05,
      "loss": 3.1556,
      "step": 488
    },
    {
      "epoch": 0.04455986878075451,
      "grad_norm": 2.0823843479156494,
      "learning_rate": 4.975543843024203e-05,
      "loss": 2.9787,
      "step": 489
    },
    {
      "epoch": 0.04465099325678877,
      "grad_norm": 3.4816336631774902,
      "learning_rate": 4.975443879805445e-05,
      "loss": 3.2459,
      "step": 490
    },
    {
      "epoch": 0.044742117732823036,
      "grad_norm": 4.104045867919922,
      "learning_rate": 4.97534371371431e-05,
      "loss": 2.8059,
      "step": 491
    },
    {
      "epoch": 0.0448332422088573,
      "grad_norm": 5.515511989593506,
      "learning_rate": 4.9752433447590084e-05,
      "loss": 3.98,
      "step": 492
    },
    {
      "epoch": 0.04492436668489156,
      "grad_norm": 3.279454231262207,
      "learning_rate": 4.975142772947766e-05,
      "loss": 3.1761,
      "step": 493
    },
    {
      "epoch": 0.045015491160925826,
      "grad_norm": 4.770516395568848,
      "learning_rate": 4.975041998288824e-05,
      "loss": 3.169,
      "step": 494
    },
    {
      "epoch": 0.045106615636960085,
      "grad_norm": 4.414744853973389,
      "learning_rate": 4.9749410207904416e-05,
      "loss": 3.3102,
      "step": 495
    },
    {
      "epoch": 0.04519774011299435,
      "grad_norm": 5.29826021194458,
      "learning_rate": 4.974839840460895e-05,
      "loss": 3.1497,
      "step": 496
    },
    {
      "epoch": 0.045288864589028616,
      "grad_norm": 3.6789612770080566,
      "learning_rate": 4.974738457308475e-05,
      "loss": 3.1284,
      "step": 497
    },
    {
      "epoch": 0.045379989065062874,
      "grad_norm": 2.8582963943481445,
      "learning_rate": 4.974636871341492e-05,
      "loss": 3.0947,
      "step": 498
    },
    {
      "epoch": 0.04547111354109714,
      "grad_norm": 3.199500799179077,
      "learning_rate": 4.97453508256827e-05,
      "loss": 3.2182,
      "step": 499
    },
    {
      "epoch": 0.0455622380171314,
      "grad_norm": 3.2001290321350098,
      "learning_rate": 4.9744330909971506e-05,
      "loss": 3.39,
      "step": 500
    },
    {
      "epoch": 0.045653362493165664,
      "grad_norm": 2.053631544113159,
      "learning_rate": 4.9743308966364945e-05,
      "loss": 3.2044,
      "step": 501
    },
    {
      "epoch": 0.04574448696919993,
      "grad_norm": 3.347944498062134,
      "learning_rate": 4.9742284994946756e-05,
      "loss": 3.3669,
      "step": 502
    },
    {
      "epoch": 0.04583561144523419,
      "grad_norm": 4.525710105895996,
      "learning_rate": 4.974125899580086e-05,
      "loss": 3.661,
      "step": 503
    },
    {
      "epoch": 0.045926735921268454,
      "grad_norm": 3.3910508155822754,
      "learning_rate": 4.974023096901133e-05,
      "loss": 3.3766,
      "step": 504
    },
    {
      "epoch": 0.04601786039730271,
      "grad_norm": 4.090876579284668,
      "learning_rate": 4.973920091466243e-05,
      "loss": 3.4127,
      "step": 505
    },
    {
      "epoch": 0.04610898487333698,
      "grad_norm": 3.523660898208618,
      "learning_rate": 4.973816883283858e-05,
      "loss": 3.4372,
      "step": 506
    },
    {
      "epoch": 0.04620010934937124,
      "grad_norm": 3.0479791164398193,
      "learning_rate": 4.9737134723624355e-05,
      "loss": 3.7846,
      "step": 507
    },
    {
      "epoch": 0.0462912338254055,
      "grad_norm": 1.953797459602356,
      "learning_rate": 4.973609858710451e-05,
      "loss": 3.1759,
      "step": 508
    },
    {
      "epoch": 0.04638235830143977,
      "grad_norm": 3.4201056957244873,
      "learning_rate": 4.973506042336395e-05,
      "loss": 3.7478,
      "step": 509
    },
    {
      "epoch": 0.046473482777474026,
      "grad_norm": 4.29667854309082,
      "learning_rate": 4.9734020232487766e-05,
      "loss": 2.8707,
      "step": 510
    },
    {
      "epoch": 0.04656460725350829,
      "grad_norm": 3.7853410243988037,
      "learning_rate": 4.973297801456121e-05,
      "loss": 3.1466,
      "step": 511
    },
    {
      "epoch": 0.04665573172954256,
      "grad_norm": 3.550114870071411,
      "learning_rate": 4.973193376966968e-05,
      "loss": 3.5852,
      "step": 512
    },
    {
      "epoch": 0.046746856205576816,
      "grad_norm": 2.124420642852783,
      "learning_rate": 4.9730887497898766e-05,
      "loss": 3.3123,
      "step": 513
    },
    {
      "epoch": 0.04683798068161108,
      "grad_norm": 5.119099140167236,
      "learning_rate": 4.9729839199334215e-05,
      "loss": 3.0232,
      "step": 514
    },
    {
      "epoch": 0.04692910515764535,
      "grad_norm": 1.975109338760376,
      "learning_rate": 4.9728788874061936e-05,
      "loss": 3.2217,
      "step": 515
    },
    {
      "epoch": 0.047020229633679606,
      "grad_norm": 2.8429102897644043,
      "learning_rate": 4.9727736522168016e-05,
      "loss": 3.5701,
      "step": 516
    },
    {
      "epoch": 0.04711135410971387,
      "grad_norm": 2.3540258407592773,
      "learning_rate": 4.972668214373869e-05,
      "loss": 3.1378,
      "step": 517
    },
    {
      "epoch": 0.04720247858574813,
      "grad_norm": 1.4242401123046875,
      "learning_rate": 4.972562573886037e-05,
      "loss": 3.0776,
      "step": 518
    },
    {
      "epoch": 0.047293603061782395,
      "grad_norm": 2.1467764377593994,
      "learning_rate": 4.972456730761963e-05,
      "loss": 3.1369,
      "step": 519
    },
    {
      "epoch": 0.04738472753781666,
      "grad_norm": 1.5915725231170654,
      "learning_rate": 4.972350685010322e-05,
      "loss": 3.0371,
      "step": 520
    },
    {
      "epoch": 0.04747585201385092,
      "grad_norm": 3.358949899673462,
      "learning_rate": 4.972244436639804e-05,
      "loss": 3.225,
      "step": 521
    },
    {
      "epoch": 0.047566976489885185,
      "grad_norm": 2.1057491302490234,
      "learning_rate": 4.972137985659117e-05,
      "loss": 3.2478,
      "step": 522
    },
    {
      "epoch": 0.047658100965919444,
      "grad_norm": 1.7229844331741333,
      "learning_rate": 4.9720313320769854e-05,
      "loss": 3.1378,
      "step": 523
    },
    {
      "epoch": 0.04774922544195371,
      "grad_norm": 4.693446636199951,
      "learning_rate": 4.971924475902149e-05,
      "loss": 3.6186,
      "step": 524
    },
    {
      "epoch": 0.047840349917987975,
      "grad_norm": 2.06803822517395,
      "learning_rate": 4.971817417143366e-05,
      "loss": 3.3783,
      "step": 525
    },
    {
      "epoch": 0.04793147439402223,
      "grad_norm": 1.9626054763793945,
      "learning_rate": 4.971710155809409e-05,
      "loss": 3.1188,
      "step": 526
    },
    {
      "epoch": 0.0480225988700565,
      "grad_norm": 5.128235340118408,
      "learning_rate": 4.9716026919090705e-05,
      "loss": 2.7121,
      "step": 527
    },
    {
      "epoch": 0.04811372334609076,
      "grad_norm": 1.7978111505508423,
      "learning_rate": 4.971495025451156e-05,
      "loss": 3.2038,
      "step": 528
    },
    {
      "epoch": 0.04820484782212502,
      "grad_norm": 2.185279130935669,
      "learning_rate": 4.971387156444489e-05,
      "loss": 3.2331,
      "step": 529
    },
    {
      "epoch": 0.04829597229815929,
      "grad_norm": 2.912400007247925,
      "learning_rate": 4.971279084897912e-05,
      "loss": 3.3246,
      "step": 530
    },
    {
      "epoch": 0.04838709677419355,
      "grad_norm": 2.451106548309326,
      "learning_rate": 4.971170810820279e-05,
      "loss": 3.2453,
      "step": 531
    },
    {
      "epoch": 0.04847822125022781,
      "grad_norm": 3.062638282775879,
      "learning_rate": 4.9710623342204646e-05,
      "loss": 3.1878,
      "step": 532
    },
    {
      "epoch": 0.04856934572626207,
      "grad_norm": 4.197722434997559,
      "learning_rate": 4.97095365510736e-05,
      "loss": 3.5296,
      "step": 533
    },
    {
      "epoch": 0.04866047020229634,
      "grad_norm": 5.098058700561523,
      "learning_rate": 4.97084477348987e-05,
      "loss": 4.8548,
      "step": 534
    },
    {
      "epoch": 0.0487515946783306,
      "grad_norm": 2.903400182723999,
      "learning_rate": 4.9707356893769194e-05,
      "loss": 3.2411,
      "step": 535
    },
    {
      "epoch": 0.04884271915436486,
      "grad_norm": 3.2839369773864746,
      "learning_rate": 4.970626402777447e-05,
      "loss": 3.5929,
      "step": 536
    },
    {
      "epoch": 0.048933843630399126,
      "grad_norm": 2.5680384635925293,
      "learning_rate": 4.970516913700411e-05,
      "loss": 3.4145,
      "step": 537
    },
    {
      "epoch": 0.049024968106433385,
      "grad_norm": 2.191035509109497,
      "learning_rate": 4.9704072221547824e-05,
      "loss": 3.3342,
      "step": 538
    },
    {
      "epoch": 0.04911609258246765,
      "grad_norm": 4.748012065887451,
      "learning_rate": 4.970297328149551e-05,
      "loss": 3.2415,
      "step": 539
    },
    {
      "epoch": 0.049207217058501916,
      "grad_norm": 3.8018529415130615,
      "learning_rate": 4.970187231693725e-05,
      "loss": 3.5361,
      "step": 540
    },
    {
      "epoch": 0.049298341534536175,
      "grad_norm": 3.128706216812134,
      "learning_rate": 4.970076932796326e-05,
      "loss": 3.2684,
      "step": 541
    },
    {
      "epoch": 0.04938946601057044,
      "grad_norm": 2.324488401412964,
      "learning_rate": 4.969966431466393e-05,
      "loss": 3.143,
      "step": 542
    },
    {
      "epoch": 0.0494805904866047,
      "grad_norm": 4.210093975067139,
      "learning_rate": 4.969855727712982e-05,
      "loss": 3.2956,
      "step": 543
    },
    {
      "epoch": 0.049571714962638964,
      "grad_norm": 2.552192449569702,
      "learning_rate": 4.969744821545166e-05,
      "loss": 3.52,
      "step": 544
    },
    {
      "epoch": 0.04966283943867323,
      "grad_norm": 3.2044615745544434,
      "learning_rate": 4.9696337129720346e-05,
      "loss": 3.5377,
      "step": 545
    },
    {
      "epoch": 0.04975396391470749,
      "grad_norm": 3.112868547439575,
      "learning_rate": 4.969522402002693e-05,
      "loss": 3.6119,
      "step": 546
    },
    {
      "epoch": 0.049845088390741754,
      "grad_norm": 2.6442015171051025,
      "learning_rate": 4.969410888646264e-05,
      "loss": 3.3822,
      "step": 547
    },
    {
      "epoch": 0.04993621286677601,
      "grad_norm": 4.498085975646973,
      "learning_rate": 4.969299172911887e-05,
      "loss": 3.6879,
      "step": 548
    },
    {
      "epoch": 0.05002733734281028,
      "grad_norm": 4.011466979980469,
      "learning_rate": 4.969187254808715e-05,
      "loss": 3.55,
      "step": 549
    },
    {
      "epoch": 0.050118461818844544,
      "grad_norm": 4.503020286560059,
      "learning_rate": 4.969075134345924e-05,
      "loss": 3.6521,
      "step": 550
    },
    {
      "epoch": 0.0502095862948788,
      "grad_norm": 2.6148521900177,
      "learning_rate": 4.9689628115326994e-05,
      "loss": 3.0139,
      "step": 551
    },
    {
      "epoch": 0.05030071077091307,
      "grad_norm": 2.978384017944336,
      "learning_rate": 4.9688502863782484e-05,
      "loss": 3.0377,
      "step": 552
    },
    {
      "epoch": 0.050391835246947334,
      "grad_norm": 2.7750418186187744,
      "learning_rate": 4.9687375588917925e-05,
      "loss": 3.5164,
      "step": 553
    },
    {
      "epoch": 0.05048295972298159,
      "grad_norm": 3.843651056289673,
      "learning_rate": 4.96862462908257e-05,
      "loss": 3.3884,
      "step": 554
    },
    {
      "epoch": 0.05057408419901586,
      "grad_norm": 2.7308478355407715,
      "learning_rate": 4.968511496959835e-05,
      "loss": 3.5797,
      "step": 555
    },
    {
      "epoch": 0.050665208675050116,
      "grad_norm": 3.842102289199829,
      "learning_rate": 4.968398162532861e-05,
      "loss": 3.537,
      "step": 556
    },
    {
      "epoch": 0.05075633315108438,
      "grad_norm": 4.337724685668945,
      "learning_rate": 4.968284625810935e-05,
      "loss": 3.3409,
      "step": 557
    },
    {
      "epoch": 0.05084745762711865,
      "grad_norm": 4.96396017074585,
      "learning_rate": 4.9681708868033616e-05,
      "loss": 3.6793,
      "step": 558
    },
    {
      "epoch": 0.050938582103152906,
      "grad_norm": 2.3788044452667236,
      "learning_rate": 4.9680569455194634e-05,
      "loss": 3.3635,
      "step": 559
    },
    {
      "epoch": 0.05102970657918717,
      "grad_norm": 3.7733585834503174,
      "learning_rate": 4.967942801968577e-05,
      "loss": 3.3418,
      "step": 560
    },
    {
      "epoch": 0.05112083105522143,
      "grad_norm": 2.370511293411255,
      "learning_rate": 4.9678284561600575e-05,
      "loss": 3.3194,
      "step": 561
    },
    {
      "epoch": 0.051211955531255696,
      "grad_norm": 2.5621142387390137,
      "learning_rate": 4.9677139081032754e-05,
      "loss": 3.2493,
      "step": 562
    },
    {
      "epoch": 0.05130308000728996,
      "grad_norm": 2.7476348876953125,
      "learning_rate": 4.96759915780762e-05,
      "loss": 3.5329,
      "step": 563
    },
    {
      "epoch": 0.05139420448332422,
      "grad_norm": 2.813443422317505,
      "learning_rate": 4.9674842052824934e-05,
      "loss": 3.4796,
      "step": 564
    },
    {
      "epoch": 0.051485328959358485,
      "grad_norm": 2.8847901821136475,
      "learning_rate": 4.967369050537317e-05,
      "loss": 3.1291,
      "step": 565
    },
    {
      "epoch": 0.051576453435392744,
      "grad_norm": 2.9698753356933594,
      "learning_rate": 4.96725369358153e-05,
      "loss": 3.4165,
      "step": 566
    },
    {
      "epoch": 0.05166757791142701,
      "grad_norm": 4.517305374145508,
      "learning_rate": 4.9671381344245846e-05,
      "loss": 3.1617,
      "step": 567
    },
    {
      "epoch": 0.051758702387461275,
      "grad_norm": 2.740018129348755,
      "learning_rate": 4.9670223730759515e-05,
      "loss": 3.2746,
      "step": 568
    },
    {
      "epoch": 0.051849826863495534,
      "grad_norm": 3.409982681274414,
      "learning_rate": 4.966906409545118e-05,
      "loss": 3.1811,
      "step": 569
    },
    {
      "epoch": 0.0519409513395298,
      "grad_norm": 5.607142925262451,
      "learning_rate": 4.9667902438415876e-05,
      "loss": 3.5238,
      "step": 570
    },
    {
      "epoch": 0.05203207581556406,
      "grad_norm": 3.8322174549102783,
      "learning_rate": 4.966673875974881e-05,
      "loss": 3.4367,
      "step": 571
    },
    {
      "epoch": 0.05212320029159832,
      "grad_norm": 4.471311569213867,
      "learning_rate": 4.9665573059545346e-05,
      "loss": 3.3449,
      "step": 572
    },
    {
      "epoch": 0.05221432476763259,
      "grad_norm": 2.2863168716430664,
      "learning_rate": 4.966440533790102e-05,
      "loss": 3.4894,
      "step": 573
    },
    {
      "epoch": 0.05230544924366685,
      "grad_norm": 3.153233289718628,
      "learning_rate": 4.966323559491153e-05,
      "loss": 3.0977,
      "step": 574
    },
    {
      "epoch": 0.05239657371970111,
      "grad_norm": 3.909994602203369,
      "learning_rate": 4.9662063830672735e-05,
      "loss": 3.3355,
      "step": 575
    },
    {
      "epoch": 0.05248769819573537,
      "grad_norm": 3.271372079849243,
      "learning_rate": 4.966089004528068e-05,
      "loss": 2.9002,
      "step": 576
    },
    {
      "epoch": 0.05257882267176964,
      "grad_norm": 3.8702852725982666,
      "learning_rate": 4.965971423883155e-05,
      "loss": 3.2961,
      "step": 577
    },
    {
      "epoch": 0.0526699471478039,
      "grad_norm": 2.3711459636688232,
      "learning_rate": 4.965853641142171e-05,
      "loss": 3.3116,
      "step": 578
    },
    {
      "epoch": 0.05276107162383816,
      "grad_norm": 1.8924840688705444,
      "learning_rate": 4.965735656314769e-05,
      "loss": 3.2245,
      "step": 579
    },
    {
      "epoch": 0.05285219609987243,
      "grad_norm": 2.0791070461273193,
      "learning_rate": 4.9656174694106186e-05,
      "loss": 3.2511,
      "step": 580
    },
    {
      "epoch": 0.052943320575906685,
      "grad_norm": 3.904510259628296,
      "learning_rate": 4.9654990804394045e-05,
      "loss": 2.3235,
      "step": 581
    },
    {
      "epoch": 0.05303444505194095,
      "grad_norm": 2.213655710220337,
      "learning_rate": 4.9653804894108294e-05,
      "loss": 3.1096,
      "step": 582
    },
    {
      "epoch": 0.05312556952797522,
      "grad_norm": 2.9452335834503174,
      "learning_rate": 4.965261696334613e-05,
      "loss": 3.2722,
      "step": 583
    },
    {
      "epoch": 0.053216694004009475,
      "grad_norm": 4.414241790771484,
      "learning_rate": 4.965142701220491e-05,
      "loss": 3.0292,
      "step": 584
    },
    {
      "epoch": 0.05330781848004374,
      "grad_norm": 3.2087106704711914,
      "learning_rate": 4.965023504078215e-05,
      "loss": 2.8201,
      "step": 585
    },
    {
      "epoch": 0.053398942956078,
      "grad_norm": 2.4379191398620605,
      "learning_rate": 4.964904104917554e-05,
      "loss": 2.8888,
      "step": 586
    },
    {
      "epoch": 0.053490067432112265,
      "grad_norm": 3.739722728729248,
      "learning_rate": 4.964784503748293e-05,
      "loss": 3.3616,
      "step": 587
    },
    {
      "epoch": 0.05358119190814653,
      "grad_norm": 3.223637104034424,
      "learning_rate": 4.9646647005802333e-05,
      "loss": 3.1671,
      "step": 588
    },
    {
      "epoch": 0.05367231638418079,
      "grad_norm": 2.3534278869628906,
      "learning_rate": 4.9645446954231936e-05,
      "loss": 3.1879,
      "step": 589
    },
    {
      "epoch": 0.053763440860215055,
      "grad_norm": 2.515484094619751,
      "learning_rate": 4.964424488287009e-05,
      "loss": 3.3132,
      "step": 590
    },
    {
      "epoch": 0.05385456533624931,
      "grad_norm": 3.4469587802886963,
      "learning_rate": 4.964304079181532e-05,
      "loss": 3.0847,
      "step": 591
    },
    {
      "epoch": 0.05394568981228358,
      "grad_norm": 3.8803462982177734,
      "learning_rate": 4.964183468116629e-05,
      "loss": 3.5651,
      "step": 592
    },
    {
      "epoch": 0.054036814288317844,
      "grad_norm": 3.385795831680298,
      "learning_rate": 4.9640626551021846e-05,
      "loss": 3.4961,
      "step": 593
    },
    {
      "epoch": 0.0541279387643521,
      "grad_norm": 2.582401990890503,
      "learning_rate": 4.9639416401481e-05,
      "loss": 3.5228,
      "step": 594
    },
    {
      "epoch": 0.05421906324038637,
      "grad_norm": 4.507946014404297,
      "learning_rate": 4.9638204232642945e-05,
      "loss": 4.4401,
      "step": 595
    },
    {
      "epoch": 0.054310187716420634,
      "grad_norm": 1.9351048469543457,
      "learning_rate": 4.9636990044607e-05,
      "loss": 3.2011,
      "step": 596
    },
    {
      "epoch": 0.05440131219245489,
      "grad_norm": 3.384533166885376,
      "learning_rate": 4.9635773837472686e-05,
      "loss": 3.3884,
      "step": 597
    },
    {
      "epoch": 0.05449243666848916,
      "grad_norm": 2.9044880867004395,
      "learning_rate": 4.963455561133967e-05,
      "loss": 3.0926,
      "step": 598
    },
    {
      "epoch": 0.05458356114452342,
      "grad_norm": 3.2804243564605713,
      "learning_rate": 4.96333353663078e-05,
      "loss": 3.2308,
      "step": 599
    },
    {
      "epoch": 0.05467468562055768,
      "grad_norm": 2.781702756881714,
      "learning_rate": 4.9632113102477066e-05,
      "loss": 3.2847,
      "step": 600
    },
    {
      "epoch": 0.05476581009659195,
      "grad_norm": 1.6848156452178955,
      "learning_rate": 4.963088881994764e-05,
      "loss": 3.0225,
      "step": 601
    },
    {
      "epoch": 0.054856934572626206,
      "grad_norm": 2.8916399478912354,
      "learning_rate": 4.962966251881987e-05,
      "loss": 3.2519,
      "step": 602
    },
    {
      "epoch": 0.05494805904866047,
      "grad_norm": 5.240880489349365,
      "learning_rate": 4.962843419919424e-05,
      "loss": 4.7183,
      "step": 603
    },
    {
      "epoch": 0.05503918352469473,
      "grad_norm": 4.498986721038818,
      "learning_rate": 4.962720386117143e-05,
      "loss": 4.3066,
      "step": 604
    },
    {
      "epoch": 0.055130308000728996,
      "grad_norm": 3.2168045043945312,
      "learning_rate": 4.962597150485226e-05,
      "loss": 3.2982,
      "step": 605
    },
    {
      "epoch": 0.05522143247676326,
      "grad_norm": 4.392197132110596,
      "learning_rate": 4.962473713033773e-05,
      "loss": 3.4625,
      "step": 606
    },
    {
      "epoch": 0.05531255695279752,
      "grad_norm": 2.5749943256378174,
      "learning_rate": 4.9623500737729e-05,
      "loss": 3.2704,
      "step": 607
    },
    {
      "epoch": 0.055403681428831786,
      "grad_norm": 2.41140079498291,
      "learning_rate": 4.96222623271274e-05,
      "loss": 2.9841,
      "step": 608
    },
    {
      "epoch": 0.055494805904866044,
      "grad_norm": 2.583230972290039,
      "learning_rate": 4.962102189863442e-05,
      "loss": 3.3146,
      "step": 609
    },
    {
      "epoch": 0.05558593038090031,
      "grad_norm": 2.78155779838562,
      "learning_rate": 4.9619779452351736e-05,
      "loss": 3.2742,
      "step": 610
    },
    {
      "epoch": 0.055677054856934576,
      "grad_norm": 2.217174768447876,
      "learning_rate": 4.9618534988381136e-05,
      "loss": 3.2342,
      "step": 611
    },
    {
      "epoch": 0.055768179332968834,
      "grad_norm": 3.625903367996216,
      "learning_rate": 4.9617288506824635e-05,
      "loss": 3.3274,
      "step": 612
    },
    {
      "epoch": 0.0558593038090031,
      "grad_norm": 3.1227903366088867,
      "learning_rate": 4.961604000778438e-05,
      "loss": 3.4848,
      "step": 613
    },
    {
      "epoch": 0.05595042828503736,
      "grad_norm": 3.104952812194824,
      "learning_rate": 4.961478949136269e-05,
      "loss": 3.0356,
      "step": 614
    },
    {
      "epoch": 0.056041552761071624,
      "grad_norm": 1.903206467628479,
      "learning_rate": 4.961353695766206e-05,
      "loss": 3.18,
      "step": 615
    },
    {
      "epoch": 0.05613267723710589,
      "grad_norm": 3.809479236602783,
      "learning_rate": 4.961228240678512e-05,
      "loss": 2.2945,
      "step": 616
    },
    {
      "epoch": 0.05622380171314015,
      "grad_norm": 1.6982084512710571,
      "learning_rate": 4.961102583883469e-05,
      "loss": 3.097,
      "step": 617
    },
    {
      "epoch": 0.056314926189174414,
      "grad_norm": 2.1733193397521973,
      "learning_rate": 4.960976725391376e-05,
      "loss": 3.1999,
      "step": 618
    },
    {
      "epoch": 0.05640605066520867,
      "grad_norm": 1.8412457704544067,
      "learning_rate": 4.960850665212548e-05,
      "loss": 3.1114,
      "step": 619
    },
    {
      "epoch": 0.05649717514124294,
      "grad_norm": 3.182962417602539,
      "learning_rate": 4.9607244033573156e-05,
      "loss": 3.5988,
      "step": 620
    },
    {
      "epoch": 0.0565882996172772,
      "grad_norm": 1.3487292528152466,
      "learning_rate": 4.960597939836025e-05,
      "loss": 2.8788,
      "step": 621
    },
    {
      "epoch": 0.05667942409331146,
      "grad_norm": 1.620439052581787,
      "learning_rate": 4.960471274659042e-05,
      "loss": 3.1345,
      "step": 622
    },
    {
      "epoch": 0.05677054856934573,
      "grad_norm": 3.5589683055877686,
      "learning_rate": 4.9603444078367475e-05,
      "loss": 4.3533,
      "step": 623
    },
    {
      "epoch": 0.056861673045379986,
      "grad_norm": 3.436901330947876,
      "learning_rate": 4.960217339379537e-05,
      "loss": 3.3396,
      "step": 624
    },
    {
      "epoch": 0.05695279752141425,
      "grad_norm": 2.2627270221710205,
      "learning_rate": 4.960090069297827e-05,
      "loss": 3.1737,
      "step": 625
    },
    {
      "epoch": 0.05704392199744852,
      "grad_norm": 1.921242594718933,
      "learning_rate": 4.9599625976020446e-05,
      "loss": 3.1868,
      "step": 626
    },
    {
      "epoch": 0.057135046473482776,
      "grad_norm": 4.410575866699219,
      "learning_rate": 4.9598349243026394e-05,
      "loss": 3.0296,
      "step": 627
    },
    {
      "epoch": 0.05722617094951704,
      "grad_norm": 1.6845407485961914,
      "learning_rate": 4.959707049410073e-05,
      "loss": 3.1197,
      "step": 628
    },
    {
      "epoch": 0.0573172954255513,
      "grad_norm": 2.5050525665283203,
      "learning_rate": 4.9595789729348263e-05,
      "loss": 3.5628,
      "step": 629
    },
    {
      "epoch": 0.057408419901585565,
      "grad_norm": 1.3204115629196167,
      "learning_rate": 4.9594506948873945e-05,
      "loss": 3.0306,
      "step": 630
    },
    {
      "epoch": 0.05749954437761983,
      "grad_norm": 2.239015579223633,
      "learning_rate": 4.9593222152782916e-05,
      "loss": 3.204,
      "step": 631
    },
    {
      "epoch": 0.05759066885365409,
      "grad_norm": 3.869081497192383,
      "learning_rate": 4.9591935341180464e-05,
      "loss": 3.3636,
      "step": 632
    },
    {
      "epoch": 0.057681793329688355,
      "grad_norm": 2.7307801246643066,
      "learning_rate": 4.959064651417204e-05,
      "loss": 3.1173,
      "step": 633
    },
    {
      "epoch": 0.057772917805722614,
      "grad_norm": 1.6466689109802246,
      "learning_rate": 4.9589355671863295e-05,
      "loss": 3.0867,
      "step": 634
    },
    {
      "epoch": 0.05786404228175688,
      "grad_norm": 3.217461109161377,
      "learning_rate": 4.9588062814359996e-05,
      "loss": 3.4342,
      "step": 635
    },
    {
      "epoch": 0.057955166757791145,
      "grad_norm": 3.0771045684814453,
      "learning_rate": 4.958676794176811e-05,
      "loss": 3.5423,
      "step": 636
    },
    {
      "epoch": 0.0580462912338254,
      "grad_norm": 4.25571346282959,
      "learning_rate": 4.958547105419374e-05,
      "loss": 3.4624,
      "step": 637
    },
    {
      "epoch": 0.05813741570985967,
      "grad_norm": 2.3620123863220215,
      "learning_rate": 4.958417215174318e-05,
      "loss": 3.1722,
      "step": 638
    },
    {
      "epoch": 0.058228540185893934,
      "grad_norm": 2.8196592330932617,
      "learning_rate": 4.958287123452289e-05,
      "loss": 3.4062,
      "step": 639
    },
    {
      "epoch": 0.05831966466192819,
      "grad_norm": 2.7436940670013428,
      "learning_rate": 4.958156830263948e-05,
      "loss": 3.3989,
      "step": 640
    },
    {
      "epoch": 0.05841078913796246,
      "grad_norm": 3.0599286556243896,
      "learning_rate": 4.958026335619972e-05,
      "loss": 2.8931,
      "step": 641
    },
    {
      "epoch": 0.05850191361399672,
      "grad_norm": 1.745510458946228,
      "learning_rate": 4.957895639531056e-05,
      "loss": 3.0919,
      "step": 642
    },
    {
      "epoch": 0.05859303809003098,
      "grad_norm": 3.2269985675811768,
      "learning_rate": 4.957764742007912e-05,
      "loss": 3.264,
      "step": 643
    },
    {
      "epoch": 0.05868416256606525,
      "grad_norm": 2.882855176925659,
      "learning_rate": 4.957633643061267e-05,
      "loss": 3.3968,
      "step": 644
    },
    {
      "epoch": 0.05877528704209951,
      "grad_norm": 3.923797130584717,
      "learning_rate": 4.9575023427018645e-05,
      "loss": 3.1769,
      "step": 645
    },
    {
      "epoch": 0.05886641151813377,
      "grad_norm": 1.8065714836120605,
      "learning_rate": 4.9573708409404665e-05,
      "loss": 3.1882,
      "step": 646
    },
    {
      "epoch": 0.05895753599416803,
      "grad_norm": 2.9968409538269043,
      "learning_rate": 4.957239137787848e-05,
      "loss": 3.5354,
      "step": 647
    },
    {
      "epoch": 0.0590486604702023,
      "grad_norm": 2.20745587348938,
      "learning_rate": 4.957107233254805e-05,
      "loss": 3.2484,
      "step": 648
    },
    {
      "epoch": 0.05913978494623656,
      "grad_norm": 3.963139057159424,
      "learning_rate": 4.9569751273521454e-05,
      "loss": 4.0875,
      "step": 649
    },
    {
      "epoch": 0.05923090942227082,
      "grad_norm": 3.877814531326294,
      "learning_rate": 4.956842820090697e-05,
      "loss": 4.6056,
      "step": 650
    },
    {
      "epoch": 0.059322033898305086,
      "grad_norm": 3.671600818634033,
      "learning_rate": 4.956710311481303e-05,
      "loss": 3.2384,
      "step": 651
    },
    {
      "epoch": 0.059413158374339345,
      "grad_norm": 3.7011606693267822,
      "learning_rate": 4.956577601534822e-05,
      "loss": 3.5084,
      "step": 652
    },
    {
      "epoch": 0.05950428285037361,
      "grad_norm": 1.532228708267212,
      "learning_rate": 4.956444690262131e-05,
      "loss": 3.1647,
      "step": 653
    },
    {
      "epoch": 0.059595407326407876,
      "grad_norm": 2.7572762966156006,
      "learning_rate": 4.956311577674123e-05,
      "loss": 3.1905,
      "step": 654
    },
    {
      "epoch": 0.059686531802442135,
      "grad_norm": 2.9054203033447266,
      "learning_rate": 4.956178263781706e-05,
      "loss": 3.45,
      "step": 655
    },
    {
      "epoch": 0.0597776562784764,
      "grad_norm": 2.3471450805664062,
      "learning_rate": 4.9560447485958065e-05,
      "loss": 3.4931,
      "step": 656
    },
    {
      "epoch": 0.05986878075451066,
      "grad_norm": 3.769836664199829,
      "learning_rate": 4.955911032127365e-05,
      "loss": 3.5662,
      "step": 657
    },
    {
      "epoch": 0.059959905230544924,
      "grad_norm": 2.2852306365966797,
      "learning_rate": 4.955777114387342e-05,
      "loss": 3.3788,
      "step": 658
    },
    {
      "epoch": 0.06005102970657919,
      "grad_norm": 2.383812665939331,
      "learning_rate": 4.9556429953867124e-05,
      "loss": 3.0372,
      "step": 659
    },
    {
      "epoch": 0.06014215418261345,
      "grad_norm": 4.165440559387207,
      "learning_rate": 4.9555086751364666e-05,
      "loss": 4.6025,
      "step": 660
    },
    {
      "epoch": 0.060233278658647714,
      "grad_norm": 2.7133188247680664,
      "learning_rate": 4.955374153647613e-05,
      "loss": 3.4581,
      "step": 661
    },
    {
      "epoch": 0.06032440313468197,
      "grad_norm": 2.894537925720215,
      "learning_rate": 4.955239430931177e-05,
      "loss": 3.2701,
      "step": 662
    },
    {
      "epoch": 0.06041552761071624,
      "grad_norm": 2.548617362976074,
      "learning_rate": 4.955104506998199e-05,
      "loss": 3.5049,
      "step": 663
    },
    {
      "epoch": 0.060506652086750504,
      "grad_norm": 1.5685203075408936,
      "learning_rate": 4.9549693818597365e-05,
      "loss": 3.0841,
      "step": 664
    },
    {
      "epoch": 0.06059777656278476,
      "grad_norm": 3.0904016494750977,
      "learning_rate": 4.954834055526864e-05,
      "loss": 3.1755,
      "step": 665
    },
    {
      "epoch": 0.06068890103881903,
      "grad_norm": 2.390272855758667,
      "learning_rate": 4.954698528010671e-05,
      "loss": 3.0737,
      "step": 666
    },
    {
      "epoch": 0.060780025514853286,
      "grad_norm": 2.9072399139404297,
      "learning_rate": 4.954562799322266e-05,
      "loss": 3.426,
      "step": 667
    },
    {
      "epoch": 0.06087114999088755,
      "grad_norm": 4.058100700378418,
      "learning_rate": 4.9544268694727714e-05,
      "loss": 3.303,
      "step": 668
    },
    {
      "epoch": 0.06096227446692182,
      "grad_norm": 2.9293103218078613,
      "learning_rate": 4.9542907384733277e-05,
      "loss": 3.5832,
      "step": 669
    },
    {
      "epoch": 0.061053398942956076,
      "grad_norm": 3.660994529724121,
      "learning_rate": 4.9541544063350916e-05,
      "loss": 3.4459,
      "step": 670
    },
    {
      "epoch": 0.06114452341899034,
      "grad_norm": 1.5523614883422852,
      "learning_rate": 4.954017873069235e-05,
      "loss": 3.2077,
      "step": 671
    },
    {
      "epoch": 0.0612356478950246,
      "grad_norm": 3.498552083969116,
      "learning_rate": 4.953881138686948e-05,
      "loss": 3.3605,
      "step": 672
    },
    {
      "epoch": 0.061326772371058866,
      "grad_norm": 1.5058245658874512,
      "learning_rate": 4.953744203199437e-05,
      "loss": 2.9763,
      "step": 673
    },
    {
      "epoch": 0.06141789684709313,
      "grad_norm": 3.1755263805389404,
      "learning_rate": 4.9536070666179236e-05,
      "loss": 3.2367,
      "step": 674
    },
    {
      "epoch": 0.06150902132312739,
      "grad_norm": 4.108938694000244,
      "learning_rate": 4.953469728953647e-05,
      "loss": 3.3441,
      "step": 675
    },
    {
      "epoch": 0.061600145799161656,
      "grad_norm": 2.8655855655670166,
      "learning_rate": 4.9533321902178634e-05,
      "loss": 3.4349,
      "step": 676
    },
    {
      "epoch": 0.06169127027519592,
      "grad_norm": 1.3662402629852295,
      "learning_rate": 4.953194450421843e-05,
      "loss": 3.052,
      "step": 677
    },
    {
      "epoch": 0.06178239475123018,
      "grad_norm": 3.1429646015167236,
      "learning_rate": 4.9530565095768744e-05,
      "loss": 3.3275,
      "step": 678
    },
    {
      "epoch": 0.061873519227264445,
      "grad_norm": 2.9324707984924316,
      "learning_rate": 4.952918367694264e-05,
      "loss": 2.9269,
      "step": 679
    },
    {
      "epoch": 0.061964643703298704,
      "grad_norm": 1.5480892658233643,
      "learning_rate": 4.952780024785331e-05,
      "loss": 3.1536,
      "step": 680
    },
    {
      "epoch": 0.06205576817933297,
      "grad_norm": 3.543039321899414,
      "learning_rate": 4.9526414808614154e-05,
      "loss": 4.6384,
      "step": 681
    },
    {
      "epoch": 0.062146892655367235,
      "grad_norm": 3.411710739135742,
      "learning_rate": 4.9525027359338696e-05,
      "loss": 3.9351,
      "step": 682
    },
    {
      "epoch": 0.062238017131401493,
      "grad_norm": 4.721251487731934,
      "learning_rate": 4.952363790014064e-05,
      "loss": 2.8042,
      "step": 683
    },
    {
      "epoch": 0.06232914160743576,
      "grad_norm": 2.0499167442321777,
      "learning_rate": 4.952224643113388e-05,
      "loss": 3.0585,
      "step": 684
    },
    {
      "epoch": 0.06242026608347002,
      "grad_norm": 2.8441872596740723,
      "learning_rate": 4.9520852952432426e-05,
      "loss": 3.6444,
      "step": 685
    },
    {
      "epoch": 0.06251139055950428,
      "grad_norm": 3.619750499725342,
      "learning_rate": 4.9519457464150496e-05,
      "loss": 3.1606,
      "step": 686
    },
    {
      "epoch": 0.06260251503553854,
      "grad_norm": 3.968768358230591,
      "learning_rate": 4.951805996640245e-05,
      "loss": 3.4814,
      "step": 687
    },
    {
      "epoch": 0.06269363951157281,
      "grad_norm": 2.2831945419311523,
      "learning_rate": 4.9516660459302827e-05,
      "loss": 3.1286,
      "step": 688
    },
    {
      "epoch": 0.06278476398760707,
      "grad_norm": 4.824334144592285,
      "learning_rate": 4.9515258942966315e-05,
      "loss": 3.2481,
      "step": 689
    },
    {
      "epoch": 0.06287588846364134,
      "grad_norm": 3.6594996452331543,
      "learning_rate": 4.951385541750777e-05,
      "loss": 2.9803,
      "step": 690
    },
    {
      "epoch": 0.06296701293967559,
      "grad_norm": 4.994578838348389,
      "learning_rate": 4.951244988304221e-05,
      "loss": 2.7129,
      "step": 691
    },
    {
      "epoch": 0.06305813741570986,
      "grad_norm": 3.323155403137207,
      "learning_rate": 4.9511042339684846e-05,
      "loss": 3.1815,
      "step": 692
    },
    {
      "epoch": 0.06314926189174412,
      "grad_norm": 3.1093785762786865,
      "learning_rate": 4.950963278755102e-05,
      "loss": 3.32,
      "step": 693
    },
    {
      "epoch": 0.06324038636777839,
      "grad_norm": 2.944016456604004,
      "learning_rate": 4.950822122675625e-05,
      "loss": 3.1048,
      "step": 694
    },
    {
      "epoch": 0.06333151084381265,
      "grad_norm": 1.7950677871704102,
      "learning_rate": 4.950680765741622e-05,
      "loss": 3.1967,
      "step": 695
    },
    {
      "epoch": 0.0634226353198469,
      "grad_norm": 2.822021961212158,
      "learning_rate": 4.950539207964677e-05,
      "loss": 2.8707,
      "step": 696
    },
    {
      "epoch": 0.06351375979588117,
      "grad_norm": 3.3690731525421143,
      "learning_rate": 4.950397449356392e-05,
      "loss": 3.277,
      "step": 697
    },
    {
      "epoch": 0.06360488427191544,
      "grad_norm": 3.850304126739502,
      "learning_rate": 4.9502554899283845e-05,
      "loss": 3.3475,
      "step": 698
    },
    {
      "epoch": 0.0636960087479497,
      "grad_norm": 3.161121129989624,
      "learning_rate": 4.9501133296922897e-05,
      "loss": 2.9737,
      "step": 699
    },
    {
      "epoch": 0.06378713322398397,
      "grad_norm": 1.786126971244812,
      "learning_rate": 4.949970968659757e-05,
      "loss": 3.1964,
      "step": 700
    },
    {
      "epoch": 0.06387825770001823,
      "grad_norm": 2.7346184253692627,
      "learning_rate": 4.949828406842453e-05,
      "loss": 3.3159,
      "step": 701
    },
    {
      "epoch": 0.06396938217605248,
      "grad_norm": 3.1189444065093994,
      "learning_rate": 4.9496856442520623e-05,
      "loss": 3.3436,
      "step": 702
    },
    {
      "epoch": 0.06406050665208675,
      "grad_norm": 1.5578367710113525,
      "learning_rate": 4.949542680900284e-05,
      "loss": 3.093,
      "step": 703
    },
    {
      "epoch": 0.06415163112812101,
      "grad_norm": 2.1540582180023193,
      "learning_rate": 4.9493995167988355e-05,
      "loss": 3.2466,
      "step": 704
    },
    {
      "epoch": 0.06424275560415528,
      "grad_norm": 3.0360822677612305,
      "learning_rate": 4.949256151959449e-05,
      "loss": 2.8197,
      "step": 705
    },
    {
      "epoch": 0.06433388008018955,
      "grad_norm": 5.320927143096924,
      "learning_rate": 4.9491125863938735e-05,
      "loss": 3.4521,
      "step": 706
    },
    {
      "epoch": 0.0644250045562238,
      "grad_norm": 3.2697649002075195,
      "learning_rate": 4.948968820113875e-05,
      "loss": 3.3638,
      "step": 707
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 2.503573417663574,
      "learning_rate": 4.948824853131236e-05,
      "loss": 3.3617,
      "step": 708
    },
    {
      "epoch": 0.06460725350829233,
      "grad_norm": 1.5332664251327515,
      "learning_rate": 4.948680685457756e-05,
      "loss": 3.0453,
      "step": 709
    },
    {
      "epoch": 0.0646983779843266,
      "grad_norm": 2.2039079666137695,
      "learning_rate": 4.948536317105248e-05,
      "loss": 3.3395,
      "step": 710
    },
    {
      "epoch": 0.06478950246036086,
      "grad_norm": 2.7374370098114014,
      "learning_rate": 4.948391748085545e-05,
      "loss": 3.3615,
      "step": 711
    },
    {
      "epoch": 0.06488062693639511,
      "grad_norm": 2.1900599002838135,
      "learning_rate": 4.948246978410495e-05,
      "loss": 3.2993,
      "step": 712
    },
    {
      "epoch": 0.06497175141242938,
      "grad_norm": 1.8759992122650146,
      "learning_rate": 4.948102008091962e-05,
      "loss": 3.1306,
      "step": 713
    },
    {
      "epoch": 0.06506287588846364,
      "grad_norm": 2.68538498878479,
      "learning_rate": 4.9479568371418274e-05,
      "loss": 3.3092,
      "step": 714
    },
    {
      "epoch": 0.06515400036449791,
      "grad_norm": 2.6900649070739746,
      "learning_rate": 4.947811465571988e-05,
      "loss": 3.2655,
      "step": 715
    },
    {
      "epoch": 0.06524512484053217,
      "grad_norm": 2.2141432762145996,
      "learning_rate": 4.947665893394357e-05,
      "loss": 3.165,
      "step": 716
    },
    {
      "epoch": 0.06533624931656642,
      "grad_norm": 2.6907012462615967,
      "learning_rate": 4.947520120620865e-05,
      "loss": 3.1901,
      "step": 717
    },
    {
      "epoch": 0.06542737379260069,
      "grad_norm": 2.0056562423706055,
      "learning_rate": 4.9473741472634606e-05,
      "loss": 3.2852,
      "step": 718
    },
    {
      "epoch": 0.06551849826863496,
      "grad_norm": 1.5069571733474731,
      "learning_rate": 4.947227973334104e-05,
      "loss": 3.0089,
      "step": 719
    },
    {
      "epoch": 0.06560962274466922,
      "grad_norm": 3.0702645778656006,
      "learning_rate": 4.947081598844777e-05,
      "loss": 3.191,
      "step": 720
    },
    {
      "epoch": 0.06570074722070349,
      "grad_norm": 3.3552052974700928,
      "learning_rate": 4.946935023807474e-05,
      "loss": 4.4409,
      "step": 721
    },
    {
      "epoch": 0.06579187169673774,
      "grad_norm": 5.324817180633545,
      "learning_rate": 4.946788248234209e-05,
      "loss": 3.2887,
      "step": 722
    },
    {
      "epoch": 0.065882996172772,
      "grad_norm": 1.67562735080719,
      "learning_rate": 4.9466412721370084e-05,
      "loss": 3.1257,
      "step": 723
    },
    {
      "epoch": 0.06597412064880627,
      "grad_norm": 3.6829192638397217,
      "learning_rate": 4.9464940955279195e-05,
      "loss": 3.0727,
      "step": 724
    },
    {
      "epoch": 0.06606524512484054,
      "grad_norm": 2.184438705444336,
      "learning_rate": 4.946346718419004e-05,
      "loss": 3.2543,
      "step": 725
    },
    {
      "epoch": 0.0661563696008748,
      "grad_norm": 1.7749693393707275,
      "learning_rate": 4.9461991408223386e-05,
      "loss": 3.107,
      "step": 726
    },
    {
      "epoch": 0.06624749407690905,
      "grad_norm": 3.877955675125122,
      "learning_rate": 4.946051362750018e-05,
      "loss": 3.0837,
      "step": 727
    },
    {
      "epoch": 0.06633861855294332,
      "grad_norm": 2.6731202602386475,
      "learning_rate": 4.9459033842141554e-05,
      "loss": 2.9075,
      "step": 728
    },
    {
      "epoch": 0.06642974302897758,
      "grad_norm": 2.0825181007385254,
      "learning_rate": 4.9457552052268764e-05,
      "loss": 3.2235,
      "step": 729
    },
    {
      "epoch": 0.06652086750501185,
      "grad_norm": 3.4630510807037354,
      "learning_rate": 4.945606825800325e-05,
      "loss": 4.399,
      "step": 730
    },
    {
      "epoch": 0.06661199198104611,
      "grad_norm": 1.572504997253418,
      "learning_rate": 4.9454582459466615e-05,
      "loss": 2.988,
      "step": 731
    },
    {
      "epoch": 0.06670311645708037,
      "grad_norm": 3.3033382892608643,
      "learning_rate": 4.945309465678063e-05,
      "loss": 3.272,
      "step": 732
    },
    {
      "epoch": 0.06679424093311463,
      "grad_norm": 1.9305294752120972,
      "learning_rate": 4.945160485006722e-05,
      "loss": 3.1268,
      "step": 733
    },
    {
      "epoch": 0.0668853654091489,
      "grad_norm": 2.0527968406677246,
      "learning_rate": 4.9450113039448484e-05,
      "loss": 3.1133,
      "step": 734
    },
    {
      "epoch": 0.06697648988518316,
      "grad_norm": 3.5695364475250244,
      "learning_rate": 4.944861922504669e-05,
      "loss": 3.2998,
      "step": 735
    },
    {
      "epoch": 0.06706761436121743,
      "grad_norm": 2.7028732299804688,
      "learning_rate": 4.944712340698424e-05,
      "loss": 3.1459,
      "step": 736
    },
    {
      "epoch": 0.06715873883725168,
      "grad_norm": 1.6403956413269043,
      "learning_rate": 4.9445625585383746e-05,
      "loss": 3.1004,
      "step": 737
    },
    {
      "epoch": 0.06724986331328595,
      "grad_norm": 2.2989110946655273,
      "learning_rate": 4.9444125760367956e-05,
      "loss": 3.3623,
      "step": 738
    },
    {
      "epoch": 0.06734098778932021,
      "grad_norm": 3.925218343734741,
      "learning_rate": 4.944262393205977e-05,
      "loss": 3.4177,
      "step": 739
    },
    {
      "epoch": 0.06743211226535448,
      "grad_norm": 2.743499517440796,
      "learning_rate": 4.944112010058229e-05,
      "loss": 3.4295,
      "step": 740
    },
    {
      "epoch": 0.06752323674138874,
      "grad_norm": 2.836487293243408,
      "learning_rate": 4.943961426605874e-05,
      "loss": 3.1732,
      "step": 741
    },
    {
      "epoch": 0.067614361217423,
      "grad_norm": 3.4316787719726562,
      "learning_rate": 4.943810642861255e-05,
      "loss": 3.2019,
      "step": 742
    },
    {
      "epoch": 0.06770548569345726,
      "grad_norm": 1.637211799621582,
      "learning_rate": 4.943659658836728e-05,
      "loss": 3.0372,
      "step": 743
    },
    {
      "epoch": 0.06779661016949153,
      "grad_norm": 2.633004665374756,
      "learning_rate": 4.9435084745446666e-05,
      "loss": 3.3982,
      "step": 744
    },
    {
      "epoch": 0.06788773464552579,
      "grad_norm": 3.1574134826660156,
      "learning_rate": 4.9433570899974626e-05,
      "loss": 4.3972,
      "step": 745
    },
    {
      "epoch": 0.06797885912156006,
      "grad_norm": 3.46399188041687,
      "learning_rate": 4.94320550520752e-05,
      "loss": 3.5156,
      "step": 746
    },
    {
      "epoch": 0.06806998359759431,
      "grad_norm": 1.8419183492660522,
      "learning_rate": 4.943053720187264e-05,
      "loss": 3.1658,
      "step": 747
    },
    {
      "epoch": 0.06816110807362857,
      "grad_norm": 4.034026622772217,
      "learning_rate": 4.942901734949133e-05,
      "loss": 3.2022,
      "step": 748
    },
    {
      "epoch": 0.06825223254966284,
      "grad_norm": 2.7348647117614746,
      "learning_rate": 4.942749549505582e-05,
      "loss": 3.2519,
      "step": 749
    },
    {
      "epoch": 0.0683433570256971,
      "grad_norm": 5.112464904785156,
      "learning_rate": 4.9425971638690847e-05,
      "loss": 3.2507,
      "step": 750
    },
    {
      "epoch": 0.06843448150173137,
      "grad_norm": 3.64758563041687,
      "learning_rate": 4.942444578052129e-05,
      "loss": 2.8225,
      "step": 751
    },
    {
      "epoch": 0.06852560597776562,
      "grad_norm": 2.541335344314575,
      "learning_rate": 4.942291792067221e-05,
      "loss": 3.1085,
      "step": 752
    },
    {
      "epoch": 0.06861673045379989,
      "grad_norm": 3.1781222820281982,
      "learning_rate": 4.9421388059268794e-05,
      "loss": 3.4272,
      "step": 753
    },
    {
      "epoch": 0.06870785492983415,
      "grad_norm": 2.2702085971832275,
      "learning_rate": 4.941985619643645e-05,
      "loss": 3.2569,
      "step": 754
    },
    {
      "epoch": 0.06879897940586842,
      "grad_norm": 5.204946517944336,
      "learning_rate": 4.94183223323007e-05,
      "loss": 3.3751,
      "step": 755
    },
    {
      "epoch": 0.06889010388190268,
      "grad_norm": 2.0559349060058594,
      "learning_rate": 4.941678646698726e-05,
      "loss": 3.0242,
      "step": 756
    },
    {
      "epoch": 0.06898122835793694,
      "grad_norm": 3.680403470993042,
      "learning_rate": 4.941524860062201e-05,
      "loss": 3.3072,
      "step": 757
    },
    {
      "epoch": 0.0690723528339712,
      "grad_norm": 2.779707908630371,
      "learning_rate": 4.941370873333096e-05,
      "loss": 2.8916,
      "step": 758
    },
    {
      "epoch": 0.06916347731000547,
      "grad_norm": 2.8263614177703857,
      "learning_rate": 4.941216686524032e-05,
      "loss": 3.3456,
      "step": 759
    },
    {
      "epoch": 0.06925460178603973,
      "grad_norm": 2.906216621398926,
      "learning_rate": 4.941062299647645e-05,
      "loss": 3.3625,
      "step": 760
    },
    {
      "epoch": 0.069345726262074,
      "grad_norm": 3.632577419281006,
      "learning_rate": 4.9409077127165895e-05,
      "loss": 3.2432,
      "step": 761
    },
    {
      "epoch": 0.06943685073810825,
      "grad_norm": 3.2788524627685547,
      "learning_rate": 4.940752925743531e-05,
      "loss": 3.2008,
      "step": 762
    },
    {
      "epoch": 0.06952797521414252,
      "grad_norm": 2.848799467086792,
      "learning_rate": 4.9405979387411576e-05,
      "loss": 3.6153,
      "step": 763
    },
    {
      "epoch": 0.06961909969017678,
      "grad_norm": 1.9642467498779297,
      "learning_rate": 4.940442751722171e-05,
      "loss": 3.1354,
      "step": 764
    },
    {
      "epoch": 0.06971022416621105,
      "grad_norm": 2.173759698867798,
      "learning_rate": 4.9402873646992876e-05,
      "loss": 3.0818,
      "step": 765
    },
    {
      "epoch": 0.06980134864224531,
      "grad_norm": 3.0131309032440186,
      "learning_rate": 4.940131777685243e-05,
      "loss": 3.4091,
      "step": 766
    },
    {
      "epoch": 0.06989247311827956,
      "grad_norm": 2.9783716201782227,
      "learning_rate": 4.939975990692789e-05,
      "loss": 3.2632,
      "step": 767
    },
    {
      "epoch": 0.06998359759431383,
      "grad_norm": 3.359174966812134,
      "learning_rate": 4.9398200037346907e-05,
      "loss": 3.057,
      "step": 768
    },
    {
      "epoch": 0.0700747220703481,
      "grad_norm": 3.2321484088897705,
      "learning_rate": 4.939663816823735e-05,
      "loss": 2.8868,
      "step": 769
    },
    {
      "epoch": 0.07016584654638236,
      "grad_norm": 2.782243013381958,
      "learning_rate": 4.9395074299727196e-05,
      "loss": 3.6189,
      "step": 770
    },
    {
      "epoch": 0.07025697102241663,
      "grad_norm": 3.496765375137329,
      "learning_rate": 4.939350843194462e-05,
      "loss": 3.4184,
      "step": 771
    },
    {
      "epoch": 0.07034809549845088,
      "grad_norm": 3.07650089263916,
      "learning_rate": 4.939194056501795e-05,
      "loss": 3.1974,
      "step": 772
    },
    {
      "epoch": 0.07043921997448514,
      "grad_norm": 2.057051181793213,
      "learning_rate": 4.939037069907567e-05,
      "loss": 3.3996,
      "step": 773
    },
    {
      "epoch": 0.07053034445051941,
      "grad_norm": 1.907810926437378,
      "learning_rate": 4.938879883424645e-05,
      "loss": 3.0664,
      "step": 774
    },
    {
      "epoch": 0.07062146892655367,
      "grad_norm": 3.811920166015625,
      "learning_rate": 4.93872249706591e-05,
      "loss": 3.3717,
      "step": 775
    },
    {
      "epoch": 0.07071259340258794,
      "grad_norm": 2.526494026184082,
      "learning_rate": 4.938564910844261e-05,
      "loss": 3.2352,
      "step": 776
    },
    {
      "epoch": 0.07080371787862219,
      "grad_norm": 3.059999465942383,
      "learning_rate": 4.938407124772613e-05,
      "loss": 3.541,
      "step": 777
    },
    {
      "epoch": 0.07089484235465646,
      "grad_norm": 3.959871530532837,
      "learning_rate": 4.9382491388638976e-05,
      "loss": 3.5196,
      "step": 778
    },
    {
      "epoch": 0.07098596683069072,
      "grad_norm": 2.6813833713531494,
      "learning_rate": 4.93809095313106e-05,
      "loss": 3.1128,
      "step": 779
    },
    {
      "epoch": 0.07107709130672499,
      "grad_norm": 3.876431465148926,
      "learning_rate": 4.937932567587067e-05,
      "loss": 3.2911,
      "step": 780
    },
    {
      "epoch": 0.07116821578275925,
      "grad_norm": 1.5844011306762695,
      "learning_rate": 4.9377739822448975e-05,
      "loss": 3.0431,
      "step": 781
    },
    {
      "epoch": 0.07125934025879352,
      "grad_norm": 4.27528190612793,
      "learning_rate": 4.937615197117549e-05,
      "loss": 3.429,
      "step": 782
    },
    {
      "epoch": 0.07135046473482777,
      "grad_norm": 3.49869441986084,
      "learning_rate": 4.937456212218034e-05,
      "loss": 3.5458,
      "step": 783
    },
    {
      "epoch": 0.07144158921086204,
      "grad_norm": 2.712157964706421,
      "learning_rate": 4.9372970275593805e-05,
      "loss": 3.2802,
      "step": 784
    },
    {
      "epoch": 0.0715327136868963,
      "grad_norm": 3.354679584503174,
      "learning_rate": 4.937137643154637e-05,
      "loss": 3.3316,
      "step": 785
    },
    {
      "epoch": 0.07162383816293057,
      "grad_norm": 4.650734901428223,
      "learning_rate": 4.9369780590168635e-05,
      "loss": 3.1748,
      "step": 786
    },
    {
      "epoch": 0.07171496263896483,
      "grad_norm": 4.040694713592529,
      "learning_rate": 4.93681827515914e-05,
      "loss": 3.3054,
      "step": 787
    },
    {
      "epoch": 0.07180608711499908,
      "grad_norm": 3.034775733947754,
      "learning_rate": 4.936658291594562e-05,
      "loss": 3.4519,
      "step": 788
    },
    {
      "epoch": 0.07189721159103335,
      "grad_norm": 3.9057462215423584,
      "learning_rate": 4.9364981083362374e-05,
      "loss": 3.2165,
      "step": 789
    },
    {
      "epoch": 0.07198833606706762,
      "grad_norm": 3.4691364765167236,
      "learning_rate": 4.9363377253972976e-05,
      "loss": 3.4187,
      "step": 790
    },
    {
      "epoch": 0.07207946054310188,
      "grad_norm": 5.106943130493164,
      "learning_rate": 4.936177142790885e-05,
      "loss": 3.1486,
      "step": 791
    },
    {
      "epoch": 0.07217058501913615,
      "grad_norm": 2.5772221088409424,
      "learning_rate": 4.9360163605301604e-05,
      "loss": 3.2757,
      "step": 792
    },
    {
      "epoch": 0.0722617094951704,
      "grad_norm": 3.403024196624756,
      "learning_rate": 4.935855378628299e-05,
      "loss": 3.2914,
      "step": 793
    },
    {
      "epoch": 0.07235283397120466,
      "grad_norm": 1.7153654098510742,
      "learning_rate": 4.935694197098496e-05,
      "loss": 3.1355,
      "step": 794
    },
    {
      "epoch": 0.07244395844723893,
      "grad_norm": 3.2709758281707764,
      "learning_rate": 4.9355328159539606e-05,
      "loss": 3.3144,
      "step": 795
    },
    {
      "epoch": 0.0725350829232732,
      "grad_norm": 2.949646472930908,
      "learning_rate": 4.935371235207917e-05,
      "loss": 3.457,
      "step": 796
    },
    {
      "epoch": 0.07262620739930746,
      "grad_norm": 3.8524835109710693,
      "learning_rate": 4.935209454873609e-05,
      "loss": 2.6742,
      "step": 797
    },
    {
      "epoch": 0.07271733187534171,
      "grad_norm": 2.2433860301971436,
      "learning_rate": 4.9350474749642946e-05,
      "loss": 3.0764,
      "step": 798
    },
    {
      "epoch": 0.07280845635137598,
      "grad_norm": 2.8999814987182617,
      "learning_rate": 4.9348852954932476e-05,
      "loss": 3.3237,
      "step": 799
    },
    {
      "epoch": 0.07289958082741024,
      "grad_norm": 1.8484467267990112,
      "learning_rate": 4.9347229164737615e-05,
      "loss": 3.1616,
      "step": 800
    },
    {
      "epoch": 0.07299070530344451,
      "grad_norm": 1.6664539575576782,
      "learning_rate": 4.934560337919143e-05,
      "loss": 2.9996,
      "step": 801
    },
    {
      "epoch": 0.07308182977947877,
      "grad_norm": 4.941806316375732,
      "learning_rate": 4.934397559842715e-05,
      "loss": 3.5425,
      "step": 802
    },
    {
      "epoch": 0.07317295425551303,
      "grad_norm": 1.4220082759857178,
      "learning_rate": 4.9342345822578184e-05,
      "loss": 3.1634,
      "step": 803
    },
    {
      "epoch": 0.07326407873154729,
      "grad_norm": 2.4295237064361572,
      "learning_rate": 4.9340714051778106e-05,
      "loss": 3.4212,
      "step": 804
    },
    {
      "epoch": 0.07335520320758156,
      "grad_norm": 1.844810962677002,
      "learning_rate": 4.933908028616063e-05,
      "loss": 3.1608,
      "step": 805
    },
    {
      "epoch": 0.07344632768361582,
      "grad_norm": 3.3480727672576904,
      "learning_rate": 4.933744452585966e-05,
      "loss": 3.4193,
      "step": 806
    },
    {
      "epoch": 0.07353745215965009,
      "grad_norm": 3.4452803134918213,
      "learning_rate": 4.9335806771009266e-05,
      "loss": 4.3444,
      "step": 807
    },
    {
      "epoch": 0.07362857663568434,
      "grad_norm": 3.2160511016845703,
      "learning_rate": 4.933416702174365e-05,
      "loss": 3.2287,
      "step": 808
    },
    {
      "epoch": 0.0737197011117186,
      "grad_norm": 1.905893325805664,
      "learning_rate": 4.9332525278197195e-05,
      "loss": 3.0757,
      "step": 809
    },
    {
      "epoch": 0.07381082558775287,
      "grad_norm": 4.715121269226074,
      "learning_rate": 4.9330881540504457e-05,
      "loss": 3.4811,
      "step": 810
    },
    {
      "epoch": 0.07390195006378714,
      "grad_norm": 3.127492904663086,
      "learning_rate": 4.932923580880015e-05,
      "loss": 3.5574,
      "step": 811
    },
    {
      "epoch": 0.0739930745398214,
      "grad_norm": 3.37953782081604,
      "learning_rate": 4.9327588083219136e-05,
      "loss": 3.4364,
      "step": 812
    },
    {
      "epoch": 0.07408419901585565,
      "grad_norm": 3.43113374710083,
      "learning_rate": 4.932593836389646e-05,
      "loss": 2.5653,
      "step": 813
    },
    {
      "epoch": 0.07417532349188992,
      "grad_norm": 2.3801136016845703,
      "learning_rate": 4.9324286650967324e-05,
      "loss": 3.1677,
      "step": 814
    },
    {
      "epoch": 0.07426644796792418,
      "grad_norm": 3.0977799892425537,
      "learning_rate": 4.932263294456708e-05,
      "loss": 3.0717,
      "step": 815
    },
    {
      "epoch": 0.07435757244395845,
      "grad_norm": 2.2414751052856445,
      "learning_rate": 4.9320977244831277e-05,
      "loss": 2.9498,
      "step": 816
    },
    {
      "epoch": 0.07444869691999272,
      "grad_norm": 3.319639205932617,
      "learning_rate": 4.931931955189559e-05,
      "loss": 3.3386,
      "step": 817
    },
    {
      "epoch": 0.07453982139602697,
      "grad_norm": 2.776702642440796,
      "learning_rate": 4.931765986589588e-05,
      "loss": 3.1402,
      "step": 818
    },
    {
      "epoch": 0.07463094587206123,
      "grad_norm": 3.072389841079712,
      "learning_rate": 4.931599818696817e-05,
      "loss": 3.1573,
      "step": 819
    },
    {
      "epoch": 0.0747220703480955,
      "grad_norm": 3.179121255874634,
      "learning_rate": 4.931433451524863e-05,
      "loss": 3.2369,
      "step": 820
    },
    {
      "epoch": 0.07481319482412976,
      "grad_norm": 3.051584005355835,
      "learning_rate": 4.9312668850873603e-05,
      "loss": 3.381,
      "step": 821
    },
    {
      "epoch": 0.07490431930016403,
      "grad_norm": 3.383882761001587,
      "learning_rate": 4.931100119397961e-05,
      "loss": 3.42,
      "step": 822
    },
    {
      "epoch": 0.07499544377619828,
      "grad_norm": 3.531190872192383,
      "learning_rate": 4.930933154470331e-05,
      "loss": 3.0216,
      "step": 823
    },
    {
      "epoch": 0.07508656825223255,
      "grad_norm": 5.148257255554199,
      "learning_rate": 4.9307659903181545e-05,
      "loss": 3.4292,
      "step": 824
    },
    {
      "epoch": 0.07517769272826681,
      "grad_norm": 1.8193916082382202,
      "learning_rate": 4.9305986269551315e-05,
      "loss": 3.1074,
      "step": 825
    },
    {
      "epoch": 0.07526881720430108,
      "grad_norm": 3.702211380004883,
      "learning_rate": 4.930431064394977e-05,
      "loss": 3.2786,
      "step": 826
    },
    {
      "epoch": 0.07535994168033534,
      "grad_norm": 1.5868266820907593,
      "learning_rate": 4.930263302651424e-05,
      "loss": 2.9994,
      "step": 827
    },
    {
      "epoch": 0.0754510661563696,
      "grad_norm": 3.0381083488464355,
      "learning_rate": 4.930095341738221e-05,
      "loss": 3.5689,
      "step": 828
    },
    {
      "epoch": 0.07554219063240386,
      "grad_norm": 4.9261884689331055,
      "learning_rate": 4.929927181669133e-05,
      "loss": 4.6381,
      "step": 829
    },
    {
      "epoch": 0.07563331510843813,
      "grad_norm": 2.415921688079834,
      "learning_rate": 4.929758822457943e-05,
      "loss": 3.4642,
      "step": 830
    },
    {
      "epoch": 0.07572443958447239,
      "grad_norm": 2.334571123123169,
      "learning_rate": 4.929590264118446e-05,
      "loss": 3.1396,
      "step": 831
    },
    {
      "epoch": 0.07581556406050666,
      "grad_norm": 3.4270524978637695,
      "learning_rate": 4.929421506664458e-05,
      "loss": 3.0609,
      "step": 832
    },
    {
      "epoch": 0.07590668853654091,
      "grad_norm": 1.8095070123672485,
      "learning_rate": 4.929252550109808e-05,
      "loss": 3.0537,
      "step": 833
    },
    {
      "epoch": 0.07599781301257517,
      "grad_norm": 2.4400718212127686,
      "learning_rate": 4.929083394468344e-05,
      "loss": 2.9386,
      "step": 834
    },
    {
      "epoch": 0.07608893748860944,
      "grad_norm": 3.1036880016326904,
      "learning_rate": 4.928914039753928e-05,
      "loss": 3.5941,
      "step": 835
    },
    {
      "epoch": 0.0761800619646437,
      "grad_norm": 2.4113924503326416,
      "learning_rate": 4.92874448598044e-05,
      "loss": 3.4679,
      "step": 836
    },
    {
      "epoch": 0.07627118644067797,
      "grad_norm": 1.8080517053604126,
      "learning_rate": 4.9285747331617746e-05,
      "loss": 3.0843,
      "step": 837
    },
    {
      "epoch": 0.07636231091671222,
      "grad_norm": 2.755985975265503,
      "learning_rate": 4.928404781311845e-05,
      "loss": 3.5471,
      "step": 838
    },
    {
      "epoch": 0.07645343539274649,
      "grad_norm": 2.892883777618408,
      "learning_rate": 4.928234630444579e-05,
      "loss": 3.2349,
      "step": 839
    },
    {
      "epoch": 0.07654455986878075,
      "grad_norm": 2.8694229125976562,
      "learning_rate": 4.92806428057392e-05,
      "loss": 3.4227,
      "step": 840
    },
    {
      "epoch": 0.07663568434481502,
      "grad_norm": 4.09429407119751,
      "learning_rate": 4.9278937317138305e-05,
      "loss": 3.4834,
      "step": 841
    },
    {
      "epoch": 0.07672680882084928,
      "grad_norm": 2.272854804992676,
      "learning_rate": 4.927722983878286e-05,
      "loss": 3.2056,
      "step": 842
    },
    {
      "epoch": 0.07681793329688354,
      "grad_norm": 5.0233330726623535,
      "learning_rate": 4.927552037081282e-05,
      "loss": 2.9908,
      "step": 843
    },
    {
      "epoch": 0.0769090577729178,
      "grad_norm": 2.9697277545928955,
      "learning_rate": 4.9273808913368256e-05,
      "loss": 3.0797,
      "step": 844
    },
    {
      "epoch": 0.07700018224895207,
      "grad_norm": 2.6238036155700684,
      "learning_rate": 4.927209546658946e-05,
      "loss": 3.1607,
      "step": 845
    },
    {
      "epoch": 0.07709130672498633,
      "grad_norm": 2.3229193687438965,
      "learning_rate": 4.9270380030616826e-05,
      "loss": 3.0519,
      "step": 846
    },
    {
      "epoch": 0.0771824312010206,
      "grad_norm": 3.1790342330932617,
      "learning_rate": 4.9268662605590963e-05,
      "loss": 3.1259,
      "step": 847
    },
    {
      "epoch": 0.07727355567705485,
      "grad_norm": 2.6441993713378906,
      "learning_rate": 4.926694319165261e-05,
      "loss": 3.2281,
      "step": 848
    },
    {
      "epoch": 0.07736468015308912,
      "grad_norm": 3.473982572555542,
      "learning_rate": 4.926522178894268e-05,
      "loss": 3.0969,
      "step": 849
    },
    {
      "epoch": 0.07745580462912338,
      "grad_norm": 3.585967540740967,
      "learning_rate": 4.926349839760225e-05,
      "loss": 3.2388,
      "step": 850
    },
    {
      "epoch": 0.07754692910515765,
      "grad_norm": 2.786681890487671,
      "learning_rate": 4.926177301777256e-05,
      "loss": 2.8739,
      "step": 851
    },
    {
      "epoch": 0.07763805358119191,
      "grad_norm": 2.578705072402954,
      "learning_rate": 4.926004564959501e-05,
      "loss": 3.1861,
      "step": 852
    },
    {
      "epoch": 0.07772917805722616,
      "grad_norm": 3.162743091583252,
      "learning_rate": 4.925831629321117e-05,
      "loss": 3.4526,
      "step": 853
    },
    {
      "epoch": 0.07782030253326043,
      "grad_norm": 2.0641379356384277,
      "learning_rate": 4.925658494876275e-05,
      "loss": 3.1193,
      "step": 854
    },
    {
      "epoch": 0.0779114270092947,
      "grad_norm": 4.980138778686523,
      "learning_rate": 4.9254851616391664e-05,
      "loss": 3.4487,
      "step": 855
    },
    {
      "epoch": 0.07800255148532896,
      "grad_norm": 1.8417590856552124,
      "learning_rate": 4.9253116296239956e-05,
      "loss": 3.1246,
      "step": 856
    },
    {
      "epoch": 0.07809367596136323,
      "grad_norm": 2.736356496810913,
      "learning_rate": 4.9251378988449835e-05,
      "loss": 3.3114,
      "step": 857
    },
    {
      "epoch": 0.07818480043739748,
      "grad_norm": 2.5761330127716064,
      "learning_rate": 4.924963969316369e-05,
      "loss": 3.3988,
      "step": 858
    },
    {
      "epoch": 0.07827592491343174,
      "grad_norm": 3.1260087490081787,
      "learning_rate": 4.924789841052406e-05,
      "loss": 3.2409,
      "step": 859
    },
    {
      "epoch": 0.07836704938946601,
      "grad_norm": 1.8674402236938477,
      "learning_rate": 4.9246155140673646e-05,
      "loss": 3.0823,
      "step": 860
    },
    {
      "epoch": 0.07845817386550027,
      "grad_norm": 2.6160728931427,
      "learning_rate": 4.924440988375532e-05,
      "loss": 3.4579,
      "step": 861
    },
    {
      "epoch": 0.07854929834153454,
      "grad_norm": 1.7055904865264893,
      "learning_rate": 4.924266263991212e-05,
      "loss": 3.119,
      "step": 862
    },
    {
      "epoch": 0.0786404228175688,
      "grad_norm": 1.8979192972183228,
      "learning_rate": 4.924091340928722e-05,
      "loss": 3.1205,
      "step": 863
    },
    {
      "epoch": 0.07873154729360306,
      "grad_norm": 1.8284133672714233,
      "learning_rate": 4.923916219202399e-05,
      "loss": 2.9849,
      "step": 864
    },
    {
      "epoch": 0.07882267176963732,
      "grad_norm": 1.7913658618927002,
      "learning_rate": 4.923740898826595e-05,
      "loss": 3.0129,
      "step": 865
    },
    {
      "epoch": 0.07891379624567159,
      "grad_norm": 2.9675111770629883,
      "learning_rate": 4.9235653798156786e-05,
      "loss": 3.2939,
      "step": 866
    },
    {
      "epoch": 0.07900492072170585,
      "grad_norm": 2.1613569259643555,
      "learning_rate": 4.9233896621840326e-05,
      "loss": 3.1203,
      "step": 867
    },
    {
      "epoch": 0.07909604519774012,
      "grad_norm": 2.8138372898101807,
      "learning_rate": 4.923213745946059e-05,
      "loss": 3.3916,
      "step": 868
    },
    {
      "epoch": 0.07918716967377437,
      "grad_norm": 5.18245792388916,
      "learning_rate": 4.9230376311161744e-05,
      "loss": 3.1091,
      "step": 869
    },
    {
      "epoch": 0.07927829414980864,
      "grad_norm": 3.7926981449127197,
      "learning_rate": 4.922861317708812e-05,
      "loss": 3.0363,
      "step": 870
    },
    {
      "epoch": 0.0793694186258429,
      "grad_norm": 2.6583340167999268,
      "learning_rate": 4.9226848057384225e-05,
      "loss": 3.1699,
      "step": 871
    },
    {
      "epoch": 0.07946054310187717,
      "grad_norm": 3.3531649112701416,
      "learning_rate": 4.92250809521947e-05,
      "loss": 3.0846,
      "step": 872
    },
    {
      "epoch": 0.07955166757791143,
      "grad_norm": 2.9770283699035645,
      "learning_rate": 4.922331186166438e-05,
      "loss": 3.0176,
      "step": 873
    },
    {
      "epoch": 0.07964279205394569,
      "grad_norm": 2.4211061000823975,
      "learning_rate": 4.922154078593824e-05,
      "loss": 3.5094,
      "step": 874
    },
    {
      "epoch": 0.07973391652997995,
      "grad_norm": 2.4895503520965576,
      "learning_rate": 4.9219767725161436e-05,
      "loss": 3.4907,
      "step": 875
    },
    {
      "epoch": 0.07982504100601422,
      "grad_norm": 2.4370858669281006,
      "learning_rate": 4.9217992679479266e-05,
      "loss": 3.4705,
      "step": 876
    },
    {
      "epoch": 0.07991616548204848,
      "grad_norm": 2.213453531265259,
      "learning_rate": 4.921621564903721e-05,
      "loss": 3.1494,
      "step": 877
    },
    {
      "epoch": 0.08000728995808275,
      "grad_norm": 2.5228660106658936,
      "learning_rate": 4.9214436633980904e-05,
      "loss": 2.3643,
      "step": 878
    },
    {
      "epoch": 0.080098414434117,
      "grad_norm": 1.479423999786377,
      "learning_rate": 4.921265563445614e-05,
      "loss": 2.9752,
      "step": 879
    },
    {
      "epoch": 0.08018953891015126,
      "grad_norm": 3.9881060123443604,
      "learning_rate": 4.921087265060888e-05,
      "loss": 3.172,
      "step": 880
    },
    {
      "epoch": 0.08028066338618553,
      "grad_norm": 3.8781585693359375,
      "learning_rate": 4.920908768258524e-05,
      "loss": 3.2123,
      "step": 881
    },
    {
      "epoch": 0.0803717878622198,
      "grad_norm": 1.9390805959701538,
      "learning_rate": 4.920730073053152e-05,
      "loss": 3.156,
      "step": 882
    },
    {
      "epoch": 0.08046291233825406,
      "grad_norm": 3.341097116470337,
      "learning_rate": 4.920551179459415e-05,
      "loss": 3.2462,
      "step": 883
    },
    {
      "epoch": 0.08055403681428831,
      "grad_norm": 3.1172938346862793,
      "learning_rate": 4.9203720874919765e-05,
      "loss": 3.2327,
      "step": 884
    },
    {
      "epoch": 0.08064516129032258,
      "grad_norm": 2.6865100860595703,
      "learning_rate": 4.920192797165511e-05,
      "loss": 3.0381,
      "step": 885
    },
    {
      "epoch": 0.08073628576635684,
      "grad_norm": 1.9933525323867798,
      "learning_rate": 4.920013308494714e-05,
      "loss": 3.1288,
      "step": 886
    },
    {
      "epoch": 0.08082741024239111,
      "grad_norm": 2.4274346828460693,
      "learning_rate": 4.919833621494294e-05,
      "loss": 3.1731,
      "step": 887
    },
    {
      "epoch": 0.08091853471842538,
      "grad_norm": 3.6805949211120605,
      "learning_rate": 4.919653736178977e-05,
      "loss": 3.4796,
      "step": 888
    },
    {
      "epoch": 0.08100965919445963,
      "grad_norm": 2.9740312099456787,
      "learning_rate": 4.9194736525635074e-05,
      "loss": 3.2645,
      "step": 889
    },
    {
      "epoch": 0.08110078367049389,
      "grad_norm": 1.9813849925994873,
      "learning_rate": 4.919293370662642e-05,
      "loss": 3.1699,
      "step": 890
    },
    {
      "epoch": 0.08119190814652816,
      "grad_norm": 2.8427340984344482,
      "learning_rate": 4.9191128904911556e-05,
      "loss": 3.3489,
      "step": 891
    },
    {
      "epoch": 0.08128303262256242,
      "grad_norm": 3.997051954269409,
      "learning_rate": 4.91893221206384e-05,
      "loss": 3.4374,
      "step": 892
    },
    {
      "epoch": 0.08137415709859669,
      "grad_norm": 1.672037959098816,
      "learning_rate": 4.9187513353955016e-05,
      "loss": 3.1125,
      "step": 893
    },
    {
      "epoch": 0.08146528157463094,
      "grad_norm": 2.2593343257904053,
      "learning_rate": 4.9185702605009645e-05,
      "loss": 3.1311,
      "step": 894
    },
    {
      "epoch": 0.0815564060506652,
      "grad_norm": 3.1953940391540527,
      "learning_rate": 4.9183889873950684e-05,
      "loss": 3.366,
      "step": 895
    },
    {
      "epoch": 0.08164753052669947,
      "grad_norm": 3.4176578521728516,
      "learning_rate": 4.91820751609267e-05,
      "loss": 3.3673,
      "step": 896
    },
    {
      "epoch": 0.08173865500273374,
      "grad_norm": 2.87166166305542,
      "learning_rate": 4.9180258466086404e-05,
      "loss": 3.5602,
      "step": 897
    },
    {
      "epoch": 0.081829779478768,
      "grad_norm": 2.719068765640259,
      "learning_rate": 4.917843978957869e-05,
      "loss": 3.2119,
      "step": 898
    },
    {
      "epoch": 0.08192090395480225,
      "grad_norm": 2.7754950523376465,
      "learning_rate": 4.9176619131552604e-05,
      "loss": 3.0594,
      "step": 899
    },
    {
      "epoch": 0.08201202843083652,
      "grad_norm": 3.5347611904144287,
      "learning_rate": 4.917479649215735e-05,
      "loss": 3.3751,
      "step": 900
    },
    {
      "epoch": 0.08210315290687079,
      "grad_norm": 4.182806015014648,
      "learning_rate": 4.917297187154232e-05,
      "loss": 3.1338,
      "step": 901
    },
    {
      "epoch": 0.08219427738290505,
      "grad_norm": 1.5867587327957153,
      "learning_rate": 4.9171145269857024e-05,
      "loss": 3.0826,
      "step": 902
    },
    {
      "epoch": 0.08228540185893932,
      "grad_norm": 3.97678279876709,
      "learning_rate": 4.916931668725117e-05,
      "loss": 3.3052,
      "step": 903
    },
    {
      "epoch": 0.08237652633497357,
      "grad_norm": 3.3548977375030518,
      "learning_rate": 4.916748612387461e-05,
      "loss": 3.3696,
      "step": 904
    },
    {
      "epoch": 0.08246765081100783,
      "grad_norm": 4.031994342803955,
      "learning_rate": 4.916565357987738e-05,
      "loss": 3.0432,
      "step": 905
    },
    {
      "epoch": 0.0825587752870421,
      "grad_norm": 3.2942988872528076,
      "learning_rate": 4.916381905540966e-05,
      "loss": 3.0257,
      "step": 906
    },
    {
      "epoch": 0.08264989976307636,
      "grad_norm": 2.751410484313965,
      "learning_rate": 4.916198255062179e-05,
      "loss": 3.1613,
      "step": 907
    },
    {
      "epoch": 0.08274102423911063,
      "grad_norm": 2.4237067699432373,
      "learning_rate": 4.916014406566428e-05,
      "loss": 3.2109,
      "step": 908
    },
    {
      "epoch": 0.08283214871514488,
      "grad_norm": 1.9257638454437256,
      "learning_rate": 4.915830360068781e-05,
      "loss": 3.0888,
      "step": 909
    },
    {
      "epoch": 0.08292327319117915,
      "grad_norm": 2.7850747108459473,
      "learning_rate": 4.91564611558432e-05,
      "loss": 3.0367,
      "step": 910
    },
    {
      "epoch": 0.08301439766721341,
      "grad_norm": 2.9995596408843994,
      "learning_rate": 4.915461673128146e-05,
      "loss": 3.1854,
      "step": 911
    },
    {
      "epoch": 0.08310552214324768,
      "grad_norm": 2.685365915298462,
      "learning_rate": 4.915277032715374e-05,
      "loss": 3.491,
      "step": 912
    },
    {
      "epoch": 0.08319664661928194,
      "grad_norm": 2.090184211730957,
      "learning_rate": 4.915092194361136e-05,
      "loss": 3.3902,
      "step": 913
    },
    {
      "epoch": 0.0832877710953162,
      "grad_norm": 2.95298171043396,
      "learning_rate": 4.91490715808058e-05,
      "loss": 3.349,
      "step": 914
    },
    {
      "epoch": 0.08337889557135046,
      "grad_norm": 2.3491621017456055,
      "learning_rate": 4.914721923888871e-05,
      "loss": 3.0253,
      "step": 915
    },
    {
      "epoch": 0.08347002004738473,
      "grad_norm": 2.7936818599700928,
      "learning_rate": 4.914536491801189e-05,
      "loss": 3.1338,
      "step": 916
    },
    {
      "epoch": 0.08356114452341899,
      "grad_norm": 2.7228002548217773,
      "learning_rate": 4.914350861832732e-05,
      "loss": 3.0906,
      "step": 917
    },
    {
      "epoch": 0.08365226899945326,
      "grad_norm": 3.0175414085388184,
      "learning_rate": 4.914165033998711e-05,
      "loss": 3.4414,
      "step": 918
    },
    {
      "epoch": 0.08374339347548751,
      "grad_norm": 1.8119590282440186,
      "learning_rate": 4.9139790083143574e-05,
      "loss": 3.1763,
      "step": 919
    },
    {
      "epoch": 0.08383451795152178,
      "grad_norm": 2.5555202960968018,
      "learning_rate": 4.913792784794917e-05,
      "loss": 3.356,
      "step": 920
    },
    {
      "epoch": 0.08392564242755604,
      "grad_norm": 2.8031249046325684,
      "learning_rate": 4.913606363455649e-05,
      "loss": 3.3269,
      "step": 921
    },
    {
      "epoch": 0.0840167669035903,
      "grad_norm": 2.178687810897827,
      "learning_rate": 4.913419744311835e-05,
      "loss": 3.1791,
      "step": 922
    },
    {
      "epoch": 0.08410789137962457,
      "grad_norm": 2.583512544631958,
      "learning_rate": 4.9132329273787655e-05,
      "loss": 3.5688,
      "step": 923
    },
    {
      "epoch": 0.08419901585565882,
      "grad_norm": 2.3542723655700684,
      "learning_rate": 4.913045912671753e-05,
      "loss": 3.061,
      "step": 924
    },
    {
      "epoch": 0.08429014033169309,
      "grad_norm": 1.9939539432525635,
      "learning_rate": 4.9128587002061245e-05,
      "loss": 3.074,
      "step": 925
    },
    {
      "epoch": 0.08438126480772735,
      "grad_norm": 3.179673194885254,
      "learning_rate": 4.912671289997221e-05,
      "loss": 2.9628,
      "step": 926
    },
    {
      "epoch": 0.08447238928376162,
      "grad_norm": 4.300661563873291,
      "learning_rate": 4.912483682060403e-05,
      "loss": 3.2144,
      "step": 927
    },
    {
      "epoch": 0.08456351375979589,
      "grad_norm": 3.354478597640991,
      "learning_rate": 4.912295876411044e-05,
      "loss": 2.8357,
      "step": 928
    },
    {
      "epoch": 0.08465463823583014,
      "grad_norm": 2.492208957672119,
      "learning_rate": 4.9121078730645375e-05,
      "loss": 3.2905,
      "step": 929
    },
    {
      "epoch": 0.0847457627118644,
      "grad_norm": 3.1754820346832275,
      "learning_rate": 4.91191967203629e-05,
      "loss": 3.205,
      "step": 930
    },
    {
      "epoch": 0.08483688718789867,
      "grad_norm": 2.5363569259643555,
      "learning_rate": 4.911731273341725e-05,
      "loss": 3.395,
      "step": 931
    },
    {
      "epoch": 0.08492801166393293,
      "grad_norm": 4.300615310668945,
      "learning_rate": 4.911542676996284e-05,
      "loss": 3.0683,
      "step": 932
    },
    {
      "epoch": 0.0850191361399672,
      "grad_norm": 2.777848958969116,
      "learning_rate": 4.911353883015422e-05,
      "loss": 3.1315,
      "step": 933
    },
    {
      "epoch": 0.08511026061600145,
      "grad_norm": 3.849351406097412,
      "learning_rate": 4.9111648914146116e-05,
      "loss": 3.1234,
      "step": 934
    },
    {
      "epoch": 0.08520138509203572,
      "grad_norm": 2.4552981853485107,
      "learning_rate": 4.910975702209341e-05,
      "loss": 3.3631,
      "step": 935
    },
    {
      "epoch": 0.08529250956806998,
      "grad_norm": 3.1233198642730713,
      "learning_rate": 4.910786315415115e-05,
      "loss": 3.4195,
      "step": 936
    },
    {
      "epoch": 0.08538363404410425,
      "grad_norm": 4.6678900718688965,
      "learning_rate": 4.910596731047456e-05,
      "loss": 3.4252,
      "step": 937
    },
    {
      "epoch": 0.08547475852013851,
      "grad_norm": 2.9370291233062744,
      "learning_rate": 4.9104069491218995e-05,
      "loss": 4.3231,
      "step": 938
    },
    {
      "epoch": 0.08556588299617277,
      "grad_norm": 1.6954116821289062,
      "learning_rate": 4.910216969654e-05,
      "loss": 3.0919,
      "step": 939
    },
    {
      "epoch": 0.08565700747220703,
      "grad_norm": 1.941863775253296,
      "learning_rate": 4.9100267926593266e-05,
      "loss": 3.13,
      "step": 940
    },
    {
      "epoch": 0.0857481319482413,
      "grad_norm": 2.876239776611328,
      "learning_rate": 4.909836418153465e-05,
      "loss": 3.3744,
      "step": 941
    },
    {
      "epoch": 0.08583925642427556,
      "grad_norm": 1.5756586790084839,
      "learning_rate": 4.909645846152018e-05,
      "loss": 3.0713,
      "step": 942
    },
    {
      "epoch": 0.08593038090030983,
      "grad_norm": 3.047095775604248,
      "learning_rate": 4.909455076670601e-05,
      "loss": 3.3628,
      "step": 943
    },
    {
      "epoch": 0.08602150537634409,
      "grad_norm": 4.294236660003662,
      "learning_rate": 4.909264109724853e-05,
      "loss": 3.4089,
      "step": 944
    },
    {
      "epoch": 0.08611262985237834,
      "grad_norm": 3.7276289463043213,
      "learning_rate": 4.9090729453304197e-05,
      "loss": 3.5265,
      "step": 945
    },
    {
      "epoch": 0.08620375432841261,
      "grad_norm": 2.4728293418884277,
      "learning_rate": 4.908881583502971e-05,
      "loss": 4.0518,
      "step": 946
    },
    {
      "epoch": 0.08629487880444688,
      "grad_norm": 2.5228142738342285,
      "learning_rate": 4.908690024258188e-05,
      "loss": 3.3074,
      "step": 947
    },
    {
      "epoch": 0.08638600328048114,
      "grad_norm": 2.56369686126709,
      "learning_rate": 4.90849826761177e-05,
      "loss": 3.472,
      "step": 948
    },
    {
      "epoch": 0.0864771277565154,
      "grad_norm": 3.3140337467193604,
      "learning_rate": 4.908306313579433e-05,
      "loss": 3.027,
      "step": 949
    },
    {
      "epoch": 0.08656825223254966,
      "grad_norm": 1.8993895053863525,
      "learning_rate": 4.908114162176908e-05,
      "loss": 3.0605,
      "step": 950
    },
    {
      "epoch": 0.08665937670858392,
      "grad_norm": 4.037572383880615,
      "learning_rate": 4.907921813419942e-05,
      "loss": 3.2735,
      "step": 951
    },
    {
      "epoch": 0.08675050118461819,
      "grad_norm": 2.20011043548584,
      "learning_rate": 4.9077292673243e-05,
      "loss": 3.4346,
      "step": 952
    },
    {
      "epoch": 0.08684162566065246,
      "grad_norm": 1.5664113759994507,
      "learning_rate": 4.907536523905761e-05,
      "loss": 2.9917,
      "step": 953
    },
    {
      "epoch": 0.08693275013668672,
      "grad_norm": 1.562983512878418,
      "learning_rate": 4.907343583180122e-05,
      "loss": 2.831,
      "step": 954
    },
    {
      "epoch": 0.08702387461272097,
      "grad_norm": 1.4592325687408447,
      "learning_rate": 4.9071504451631934e-05,
      "loss": 3.1492,
      "step": 955
    },
    {
      "epoch": 0.08711499908875524,
      "grad_norm": 3.860102653503418,
      "learning_rate": 4.9069571098708045e-05,
      "loss": 3.3501,
      "step": 956
    },
    {
      "epoch": 0.0872061235647895,
      "grad_norm": 3.7838504314422607,
      "learning_rate": 4.9067635773188005e-05,
      "loss": 3.2827,
      "step": 957
    },
    {
      "epoch": 0.08729724804082377,
      "grad_norm": 1.798142671585083,
      "learning_rate": 4.906569847523042e-05,
      "loss": 3.2199,
      "step": 958
    },
    {
      "epoch": 0.08738837251685803,
      "grad_norm": 2.6328585147857666,
      "learning_rate": 4.906375920499405e-05,
      "loss": 3.1803,
      "step": 959
    },
    {
      "epoch": 0.08747949699289229,
      "grad_norm": 2.4833974838256836,
      "learning_rate": 4.906181796263784e-05,
      "loss": 3.7958,
      "step": 960
    },
    {
      "epoch": 0.08757062146892655,
      "grad_norm": 2.0607047080993652,
      "learning_rate": 4.9059874748320876e-05,
      "loss": 3.1254,
      "step": 961
    },
    {
      "epoch": 0.08766174594496082,
      "grad_norm": 2.7997632026672363,
      "learning_rate": 4.90579295622024e-05,
      "loss": 2.953,
      "step": 962
    },
    {
      "epoch": 0.08775287042099508,
      "grad_norm": 2.249958038330078,
      "learning_rate": 4.905598240444185e-05,
      "loss": 3.4737,
      "step": 963
    },
    {
      "epoch": 0.08784399489702935,
      "grad_norm": 3.1202094554901123,
      "learning_rate": 4.9054033275198794e-05,
      "loss": 3.1832,
      "step": 964
    },
    {
      "epoch": 0.0879351193730636,
      "grad_norm": 1.4089468717575073,
      "learning_rate": 4.905208217463296e-05,
      "loss": 3.0008,
      "step": 965
    },
    {
      "epoch": 0.08802624384909787,
      "grad_norm": 2.194896697998047,
      "learning_rate": 4.905012910290426e-05,
      "loss": 3.1747,
      "step": 966
    },
    {
      "epoch": 0.08811736832513213,
      "grad_norm": 1.8730498552322388,
      "learning_rate": 4.904817406017275e-05,
      "loss": 3.0702,
      "step": 967
    },
    {
      "epoch": 0.0882084928011664,
      "grad_norm": 1.5486280918121338,
      "learning_rate": 4.904621704659866e-05,
      "loss": 3.0648,
      "step": 968
    },
    {
      "epoch": 0.08829961727720066,
      "grad_norm": 2.200500965118408,
      "learning_rate": 4.9044258062342376e-05,
      "loss": 3.095,
      "step": 969
    },
    {
      "epoch": 0.08839074175323491,
      "grad_norm": 4.8551788330078125,
      "learning_rate": 4.904229710756444e-05,
      "loss": 4.232,
      "step": 970
    },
    {
      "epoch": 0.08848186622926918,
      "grad_norm": 1.4722237586975098,
      "learning_rate": 4.904033418242555e-05,
      "loss": 3.021,
      "step": 971
    },
    {
      "epoch": 0.08857299070530344,
      "grad_norm": 2.9047417640686035,
      "learning_rate": 4.9038369287086594e-05,
      "loss": 2.9605,
      "step": 972
    },
    {
      "epoch": 0.08866411518133771,
      "grad_norm": 2.3891854286193848,
      "learning_rate": 4.9036402421708596e-05,
      "loss": 3.4125,
      "step": 973
    },
    {
      "epoch": 0.08875523965737198,
      "grad_norm": 3.597698211669922,
      "learning_rate": 4.903443358645274e-05,
      "loss": 3.4755,
      "step": 974
    },
    {
      "epoch": 0.08884636413340623,
      "grad_norm": 2.3222601413726807,
      "learning_rate": 4.903246278148039e-05,
      "loss": 3.1024,
      "step": 975
    },
    {
      "epoch": 0.0889374886094405,
      "grad_norm": 1.7623449563980103,
      "learning_rate": 4.903049000695305e-05,
      "loss": 3.0337,
      "step": 976
    },
    {
      "epoch": 0.08902861308547476,
      "grad_norm": 4.266841411590576,
      "learning_rate": 4.9028515263032415e-05,
      "loss": 3.3937,
      "step": 977
    },
    {
      "epoch": 0.08911973756150902,
      "grad_norm": 1.746504783630371,
      "learning_rate": 4.902653854988031e-05,
      "loss": 3.0249,
      "step": 978
    },
    {
      "epoch": 0.08921086203754329,
      "grad_norm": 3.4824695587158203,
      "learning_rate": 4.9024559867658734e-05,
      "loss": 3.1459,
      "step": 979
    },
    {
      "epoch": 0.08930198651357754,
      "grad_norm": 3.1984243392944336,
      "learning_rate": 4.9022579216529854e-05,
      "loss": 3.1946,
      "step": 980
    },
    {
      "epoch": 0.08939311098961181,
      "grad_norm": 2.4650838375091553,
      "learning_rate": 4.902059659665599e-05,
      "loss": 3.3177,
      "step": 981
    },
    {
      "epoch": 0.08948423546564607,
      "grad_norm": 3.3806710243225098,
      "learning_rate": 4.9018612008199616e-05,
      "loss": 3.4283,
      "step": 982
    },
    {
      "epoch": 0.08957535994168034,
      "grad_norm": 2.031496286392212,
      "learning_rate": 4.9016625451323396e-05,
      "loss": 3.2034,
      "step": 983
    },
    {
      "epoch": 0.0896664844177146,
      "grad_norm": 1.8821396827697754,
      "learning_rate": 4.9014636926190116e-05,
      "loss": 3.0086,
      "step": 984
    },
    {
      "epoch": 0.08975760889374886,
      "grad_norm": 3.310356378555298,
      "learning_rate": 4.901264643296276e-05,
      "loss": 3.2155,
      "step": 985
    },
    {
      "epoch": 0.08984873336978312,
      "grad_norm": 2.422724723815918,
      "learning_rate": 4.9010653971804444e-05,
      "loss": 3.3122,
      "step": 986
    },
    {
      "epoch": 0.08993985784581739,
      "grad_norm": 4.234830856323242,
      "learning_rate": 4.9008659542878464e-05,
      "loss": 3.5449,
      "step": 987
    },
    {
      "epoch": 0.09003098232185165,
      "grad_norm": 1.48231840133667,
      "learning_rate": 4.900666314634828e-05,
      "loss": 3.0412,
      "step": 988
    },
    {
      "epoch": 0.09012210679788592,
      "grad_norm": 2.9697065353393555,
      "learning_rate": 4.900466478237748e-05,
      "loss": 3.2992,
      "step": 989
    },
    {
      "epoch": 0.09021323127392017,
      "grad_norm": 2.7923099994659424,
      "learning_rate": 4.900266445112986e-05,
      "loss": 3.4856,
      "step": 990
    },
    {
      "epoch": 0.09030435574995443,
      "grad_norm": 2.1255149841308594,
      "learning_rate": 4.900066215276936e-05,
      "loss": 2.9898,
      "step": 991
    },
    {
      "epoch": 0.0903954802259887,
      "grad_norm": 2.3270339965820312,
      "learning_rate": 4.899865788746005e-05,
      "loss": 3.127,
      "step": 992
    },
    {
      "epoch": 0.09048660470202297,
      "grad_norm": 2.2811200618743896,
      "learning_rate": 4.899665165536621e-05,
      "loss": 3.3315,
      "step": 993
    },
    {
      "epoch": 0.09057772917805723,
      "grad_norm": 2.0090272426605225,
      "learning_rate": 4.8994643456652244e-05,
      "loss": 3.1308,
      "step": 994
    },
    {
      "epoch": 0.09066885365409148,
      "grad_norm": 5.991725921630859,
      "learning_rate": 4.8992633291482746e-05,
      "loss": 2.8415,
      "step": 995
    },
    {
      "epoch": 0.09075997813012575,
      "grad_norm": 1.7398011684417725,
      "learning_rate": 4.899062116002244e-05,
      "loss": 3.247,
      "step": 996
    },
    {
      "epoch": 0.09085110260616001,
      "grad_norm": 2.6200222969055176,
      "learning_rate": 4.898860706243625e-05,
      "loss": 3.0824,
      "step": 997
    },
    {
      "epoch": 0.09094222708219428,
      "grad_norm": 2.7721424102783203,
      "learning_rate": 4.898659099888921e-05,
      "loss": 4.5453,
      "step": 998
    },
    {
      "epoch": 0.09103335155822855,
      "grad_norm": 4.053179740905762,
      "learning_rate": 4.8984572969546575e-05,
      "loss": 3.4542,
      "step": 999
    },
    {
      "epoch": 0.0911244760342628,
      "grad_norm": 3.0186972618103027,
      "learning_rate": 4.8982552974573717e-05,
      "loss": 3.1511,
      "step": 1000
    },
    {
      "epoch": 0.09121560051029706,
      "grad_norm": 2.732668876647949,
      "learning_rate": 4.8980531014136175e-05,
      "loss": 3.0888,
      "step": 1001
    },
    {
      "epoch": 0.09130672498633133,
      "grad_norm": 2.304547071456909,
      "learning_rate": 4.897850708839966e-05,
      "loss": 3.3741,
      "step": 1002
    },
    {
      "epoch": 0.0913978494623656,
      "grad_norm": 2.8242197036743164,
      "learning_rate": 4.897648119753006e-05,
      "loss": 3.3513,
      "step": 1003
    },
    {
      "epoch": 0.09148897393839986,
      "grad_norm": 3.2186803817749023,
      "learning_rate": 4.897445334169337e-05,
      "loss": 4.7131,
      "step": 1004
    },
    {
      "epoch": 0.09158009841443411,
      "grad_norm": 2.906078577041626,
      "learning_rate": 4.897242352105581e-05,
      "loss": 3.0597,
      "step": 1005
    },
    {
      "epoch": 0.09167122289046838,
      "grad_norm": 3.2613537311553955,
      "learning_rate": 4.8970391735783725e-05,
      "loss": 3.0269,
      "step": 1006
    },
    {
      "epoch": 0.09176234736650264,
      "grad_norm": 4.029659271240234,
      "learning_rate": 4.896835798604362e-05,
      "loss": 3.5813,
      "step": 1007
    },
    {
      "epoch": 0.09185347184253691,
      "grad_norm": 8.040237426757812,
      "learning_rate": 4.8966322272002174e-05,
      "loss": 3.176,
      "step": 1008
    },
    {
      "epoch": 0.09194459631857117,
      "grad_norm": 3.0369622707366943,
      "learning_rate": 4.8964284593826215e-05,
      "loss": 3.1676,
      "step": 1009
    },
    {
      "epoch": 0.09203572079460542,
      "grad_norm": 3.2260282039642334,
      "learning_rate": 4.8962244951682754e-05,
      "loss": 3.3491,
      "step": 1010
    },
    {
      "epoch": 0.09212684527063969,
      "grad_norm": 1.53379225730896,
      "learning_rate": 4.8960203345738934e-05,
      "loss": 3.0221,
      "step": 1011
    },
    {
      "epoch": 0.09221796974667396,
      "grad_norm": 4.076636791229248,
      "learning_rate": 4.895815977616208e-05,
      "loss": 3.5203,
      "step": 1012
    },
    {
      "epoch": 0.09230909422270822,
      "grad_norm": 2.1849517822265625,
      "learning_rate": 4.895611424311967e-05,
      "loss": 3.4105,
      "step": 1013
    },
    {
      "epoch": 0.09240021869874249,
      "grad_norm": 8.389893531799316,
      "learning_rate": 4.8954066746779334e-05,
      "loss": 3.2236,
      "step": 1014
    },
    {
      "epoch": 0.09249134317477674,
      "grad_norm": 2.0767617225646973,
      "learning_rate": 4.895201728730888e-05,
      "loss": 3.1362,
      "step": 1015
    },
    {
      "epoch": 0.092582467650811,
      "grad_norm": 2.6527016162872314,
      "learning_rate": 4.894996586487627e-05,
      "loss": 3.0877,
      "step": 1016
    },
    {
      "epoch": 0.09267359212684527,
      "grad_norm": 3.5784764289855957,
      "learning_rate": 4.8947912479649624e-05,
      "loss": 3.279,
      "step": 1017
    },
    {
      "epoch": 0.09276471660287954,
      "grad_norm": 1.6435048580169678,
      "learning_rate": 4.894585713179723e-05,
      "loss": 2.9978,
      "step": 1018
    },
    {
      "epoch": 0.0928558410789138,
      "grad_norm": 1.3273972272872925,
      "learning_rate": 4.894379982148753e-05,
      "loss": 2.998,
      "step": 1019
    },
    {
      "epoch": 0.09294696555494805,
      "grad_norm": 1.7817779779434204,
      "learning_rate": 4.894174054888912e-05,
      "loss": 2.9691,
      "step": 1020
    },
    {
      "epoch": 0.09303809003098232,
      "grad_norm": 2.4242284297943115,
      "learning_rate": 4.893967931417078e-05,
      "loss": 3.228,
      "step": 1021
    },
    {
      "epoch": 0.09312921450701658,
      "grad_norm": 2.9169692993164062,
      "learning_rate": 4.8937616117501414e-05,
      "loss": 3.5446,
      "step": 1022
    },
    {
      "epoch": 0.09322033898305085,
      "grad_norm": 3.6334569454193115,
      "learning_rate": 4.893555095905014e-05,
      "loss": 3.0,
      "step": 1023
    },
    {
      "epoch": 0.09331146345908511,
      "grad_norm": 2.8542470932006836,
      "learning_rate": 4.8933483838986184e-05,
      "loss": 3.1218,
      "step": 1024
    },
    {
      "epoch": 0.09340258793511938,
      "grad_norm": 2.078474283218384,
      "learning_rate": 4.8931414757478954e-05,
      "loss": 3.2325,
      "step": 1025
    },
    {
      "epoch": 0.09349371241115363,
      "grad_norm": 3.3290367126464844,
      "learning_rate": 4.8929343714698026e-05,
      "loss": 3.1449,
      "step": 1026
    },
    {
      "epoch": 0.0935848368871879,
      "grad_norm": 2.655738592147827,
      "learning_rate": 4.892727071081314e-05,
      "loss": 4.3078,
      "step": 1027
    },
    {
      "epoch": 0.09367596136322216,
      "grad_norm": 2.936398983001709,
      "learning_rate": 4.8925195745994165e-05,
      "loss": 2.9901,
      "step": 1028
    },
    {
      "epoch": 0.09376708583925643,
      "grad_norm": 1.61790931224823,
      "learning_rate": 4.892311882041117e-05,
      "loss": 2.9575,
      "step": 1029
    },
    {
      "epoch": 0.0938582103152907,
      "grad_norm": 4.781036853790283,
      "learning_rate": 4.892103993423436e-05,
      "loss": 3.3626,
      "step": 1030
    },
    {
      "epoch": 0.09394933479132495,
      "grad_norm": 4.162670612335205,
      "learning_rate": 4.891895908763411e-05,
      "loss": 3.5073,
      "step": 1031
    },
    {
      "epoch": 0.09404045926735921,
      "grad_norm": 2.5747599601745605,
      "learning_rate": 4.8916876280780946e-05,
      "loss": 2.6645,
      "step": 1032
    },
    {
      "epoch": 0.09413158374339348,
      "grad_norm": 2.0610013008117676,
      "learning_rate": 4.8914791513845575e-05,
      "loss": 2.8624,
      "step": 1033
    },
    {
      "epoch": 0.09422270821942774,
      "grad_norm": 1.6727491617202759,
      "learning_rate": 4.8912704786998844e-05,
      "loss": 3.0615,
      "step": 1034
    },
    {
      "epoch": 0.09431383269546201,
      "grad_norm": 2.5787103176116943,
      "learning_rate": 4.8910616100411774e-05,
      "loss": 3.1513,
      "step": 1035
    },
    {
      "epoch": 0.09440495717149626,
      "grad_norm": 2.7966387271881104,
      "learning_rate": 4.890852545425553e-05,
      "loss": 3.2184,
      "step": 1036
    },
    {
      "epoch": 0.09449608164753052,
      "grad_norm": 3.2339022159576416,
      "learning_rate": 4.8906432848701464e-05,
      "loss": 3.219,
      "step": 1037
    },
    {
      "epoch": 0.09458720612356479,
      "grad_norm": 3.6414124965667725,
      "learning_rate": 4.8904338283921056e-05,
      "loss": 3.0272,
      "step": 1038
    },
    {
      "epoch": 0.09467833059959906,
      "grad_norm": 1.915804147720337,
      "learning_rate": 4.890224176008598e-05,
      "loss": 3.0635,
      "step": 1039
    },
    {
      "epoch": 0.09476945507563332,
      "grad_norm": 1.533538579940796,
      "learning_rate": 4.890014327736804e-05,
      "loss": 3.1068,
      "step": 1040
    },
    {
      "epoch": 0.09486057955166757,
      "grad_norm": 4.12912130355835,
      "learning_rate": 4.889804283593923e-05,
      "loss": 3.1286,
      "step": 1041
    },
    {
      "epoch": 0.09495170402770184,
      "grad_norm": 3.413926362991333,
      "learning_rate": 4.889594043597168e-05,
      "loss": 4.3663,
      "step": 1042
    },
    {
      "epoch": 0.0950428285037361,
      "grad_norm": 3.632355213165283,
      "learning_rate": 4.8893836077637686e-05,
      "loss": 3.6121,
      "step": 1043
    },
    {
      "epoch": 0.09513395297977037,
      "grad_norm": 3.041640043258667,
      "learning_rate": 4.8891729761109726e-05,
      "loss": 3.0379,
      "step": 1044
    },
    {
      "epoch": 0.09522507745580464,
      "grad_norm": 2.1716883182525635,
      "learning_rate": 4.88896214865604e-05,
      "loss": 3.0752,
      "step": 1045
    },
    {
      "epoch": 0.09531620193183889,
      "grad_norm": 1.7440366744995117,
      "learning_rate": 4.88875112541625e-05,
      "loss": 2.9915,
      "step": 1046
    },
    {
      "epoch": 0.09540732640787315,
      "grad_norm": 2.8066303730010986,
      "learning_rate": 4.888539906408897e-05,
      "loss": 3.2162,
      "step": 1047
    },
    {
      "epoch": 0.09549845088390742,
      "grad_norm": 2.6546630859375,
      "learning_rate": 4.888328491651291e-05,
      "loss": 3.1054,
      "step": 1048
    },
    {
      "epoch": 0.09558957535994168,
      "grad_norm": 2.986856460571289,
      "learning_rate": 4.888116881160757e-05,
      "loss": 3.1334,
      "step": 1049
    },
    {
      "epoch": 0.09568069983597595,
      "grad_norm": 4.76503324508667,
      "learning_rate": 4.8879050749546395e-05,
      "loss": 3.461,
      "step": 1050
    },
    {
      "epoch": 0.0957718243120102,
      "grad_norm": 1.329960823059082,
      "learning_rate": 4.8876930730502954e-05,
      "loss": 3.0356,
      "step": 1051
    },
    {
      "epoch": 0.09586294878804447,
      "grad_norm": 2.466423511505127,
      "learning_rate": 4.887480875465099e-05,
      "loss": 3.3193,
      "step": 1052
    },
    {
      "epoch": 0.09595407326407873,
      "grad_norm": 3.3241379261016846,
      "learning_rate": 4.887268482216442e-05,
      "loss": 3.4059,
      "step": 1053
    },
    {
      "epoch": 0.096045197740113,
      "grad_norm": 2.425245523452759,
      "learning_rate": 4.88705589332173e-05,
      "loss": 3.0792,
      "step": 1054
    },
    {
      "epoch": 0.09613632221614726,
      "grad_norm": 2.820553779602051,
      "learning_rate": 4.886843108798386e-05,
      "loss": 3.1892,
      "step": 1055
    },
    {
      "epoch": 0.09622744669218151,
      "grad_norm": 2.662749767303467,
      "learning_rate": 4.886630128663847e-05,
      "loss": 3.4359,
      "step": 1056
    },
    {
      "epoch": 0.09631857116821578,
      "grad_norm": 3.7689478397369385,
      "learning_rate": 4.8864169529355694e-05,
      "loss": 3.3501,
      "step": 1057
    },
    {
      "epoch": 0.09640969564425005,
      "grad_norm": 2.9950053691864014,
      "learning_rate": 4.8862035816310225e-05,
      "loss": 3.0473,
      "step": 1058
    },
    {
      "epoch": 0.09650082012028431,
      "grad_norm": 3.828263521194458,
      "learning_rate": 4.885990014767694e-05,
      "loss": 3.7086,
      "step": 1059
    },
    {
      "epoch": 0.09659194459631858,
      "grad_norm": 2.4364869594573975,
      "learning_rate": 4.885776252363086e-05,
      "loss": 4.1158,
      "step": 1060
    },
    {
      "epoch": 0.09668306907235283,
      "grad_norm": 3.0693185329437256,
      "learning_rate": 4.8855622944347174e-05,
      "loss": 4.4396,
      "step": 1061
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 2.158339738845825,
      "learning_rate": 4.885348141000122e-05,
      "loss": 3.1364,
      "step": 1062
    },
    {
      "epoch": 0.09686531802442136,
      "grad_norm": 3.3291866779327393,
      "learning_rate": 4.885133792076852e-05,
      "loss": 3.4187,
      "step": 1063
    },
    {
      "epoch": 0.09695644250045563,
      "grad_norm": 3.016261100769043,
      "learning_rate": 4.884919247682473e-05,
      "loss": 3.4883,
      "step": 1064
    },
    {
      "epoch": 0.09704756697648989,
      "grad_norm": 1.6200766563415527,
      "learning_rate": 4.8847045078345674e-05,
      "loss": 3.0487,
      "step": 1065
    },
    {
      "epoch": 0.09713869145252414,
      "grad_norm": 2.354325771331787,
      "learning_rate": 4.884489572550736e-05,
      "loss": 3.2557,
      "step": 1066
    },
    {
      "epoch": 0.09722981592855841,
      "grad_norm": 4.061933994293213,
      "learning_rate": 4.884274441848592e-05,
      "loss": 3.4442,
      "step": 1067
    },
    {
      "epoch": 0.09732094040459267,
      "grad_norm": 4.645877838134766,
      "learning_rate": 4.884059115745766e-05,
      "loss": 3.0568,
      "step": 1068
    },
    {
      "epoch": 0.09741206488062694,
      "grad_norm": 1.73179292678833,
      "learning_rate": 4.883843594259905e-05,
      "loss": 3.125,
      "step": 1069
    },
    {
      "epoch": 0.0975031893566612,
      "grad_norm": 1.4215937852859497,
      "learning_rate": 4.883627877408673e-05,
      "loss": 2.8963,
      "step": 1070
    },
    {
      "epoch": 0.09759431383269546,
      "grad_norm": 3.7663443088531494,
      "learning_rate": 4.8834119652097475e-05,
      "loss": 4.3777,
      "step": 1071
    },
    {
      "epoch": 0.09768543830872972,
      "grad_norm": 1.6245098114013672,
      "learning_rate": 4.883195857680824e-05,
      "loss": 3.0648,
      "step": 1072
    },
    {
      "epoch": 0.09777656278476399,
      "grad_norm": 3.0550179481506348,
      "learning_rate": 4.882979554839613e-05,
      "loss": 3.2421,
      "step": 1073
    },
    {
      "epoch": 0.09786768726079825,
      "grad_norm": 3.2408952713012695,
      "learning_rate": 4.8827630567038416e-05,
      "loss": 2.8971,
      "step": 1074
    },
    {
      "epoch": 0.09795881173683252,
      "grad_norm": 7.056894302368164,
      "learning_rate": 4.882546363291253e-05,
      "loss": 3.2447,
      "step": 1075
    },
    {
      "epoch": 0.09804993621286677,
      "grad_norm": 4.1067914962768555,
      "learning_rate": 4.882329474619606e-05,
      "loss": 3.3844,
      "step": 1076
    },
    {
      "epoch": 0.09814106068890104,
      "grad_norm": 2.7459664344787598,
      "learning_rate": 4.882112390706675e-05,
      "loss": 2.8364,
      "step": 1077
    },
    {
      "epoch": 0.0982321851649353,
      "grad_norm": 1.7303998470306396,
      "learning_rate": 4.8818951115702506e-05,
      "loss": 3.1362,
      "step": 1078
    },
    {
      "epoch": 0.09832330964096957,
      "grad_norm": 2.4760732650756836,
      "learning_rate": 4.88167763722814e-05,
      "loss": 3.5898,
      "step": 1079
    },
    {
      "epoch": 0.09841443411700383,
      "grad_norm": 1.439005970954895,
      "learning_rate": 4.8814599676981667e-05,
      "loss": 3.0291,
      "step": 1080
    },
    {
      "epoch": 0.09850555859303808,
      "grad_norm": 3.3156778812408447,
      "learning_rate": 4.881242102998169e-05,
      "loss": 2.9964,
      "step": 1081
    },
    {
      "epoch": 0.09859668306907235,
      "grad_norm": 2.405925750732422,
      "learning_rate": 4.881024043146002e-05,
      "loss": 3.549,
      "step": 1082
    },
    {
      "epoch": 0.09868780754510662,
      "grad_norm": 1.8694865703582764,
      "learning_rate": 4.880805788159537e-05,
      "loss": 3.3145,
      "step": 1083
    },
    {
      "epoch": 0.09877893202114088,
      "grad_norm": 3.397982358932495,
      "learning_rate": 4.880587338056659e-05,
      "loss": 3.1483,
      "step": 1084
    },
    {
      "epoch": 0.09887005649717515,
      "grad_norm": 3.9848830699920654,
      "learning_rate": 4.8803686928552736e-05,
      "loss": 3.3601,
      "step": 1085
    },
    {
      "epoch": 0.0989611809732094,
      "grad_norm": 2.076350688934326,
      "learning_rate": 4.880149852573297e-05,
      "loss": 2.9707,
      "step": 1086
    },
    {
      "epoch": 0.09905230544924366,
      "grad_norm": 4.586529731750488,
      "learning_rate": 4.8799308172286665e-05,
      "loss": 3.0142,
      "step": 1087
    },
    {
      "epoch": 0.09914342992527793,
      "grad_norm": 2.224879503250122,
      "learning_rate": 4.8797115868393304e-05,
      "loss": 3.2586,
      "step": 1088
    },
    {
      "epoch": 0.0992345544013122,
      "grad_norm": 2.13420033454895,
      "learning_rate": 4.879492161423257e-05,
      "loss": 3.3615,
      "step": 1089
    },
    {
      "epoch": 0.09932567887734646,
      "grad_norm": 2.3102781772613525,
      "learning_rate": 4.8792725409984295e-05,
      "loss": 3.2469,
      "step": 1090
    },
    {
      "epoch": 0.09941680335338071,
      "grad_norm": 2.7327070236206055,
      "learning_rate": 4.8790527255828453e-05,
      "loss": 3.0008,
      "step": 1091
    },
    {
      "epoch": 0.09950792782941498,
      "grad_norm": 4.016688823699951,
      "learning_rate": 4.8788327151945204e-05,
      "loss": 3.3525,
      "step": 1092
    },
    {
      "epoch": 0.09959905230544924,
      "grad_norm": 2.56805682182312,
      "learning_rate": 4.878612509851484e-05,
      "loss": 2.7244,
      "step": 1093
    },
    {
      "epoch": 0.09969017678148351,
      "grad_norm": 2.7914106845855713,
      "learning_rate": 4.878392109571784e-05,
      "loss": 2.9815,
      "step": 1094
    },
    {
      "epoch": 0.09978130125751777,
      "grad_norm": 1.5488040447235107,
      "learning_rate": 4.878171514373483e-05,
      "loss": 2.9438,
      "step": 1095
    },
    {
      "epoch": 0.09987242573355203,
      "grad_norm": 2.1465392112731934,
      "learning_rate": 4.87795072427466e-05,
      "loss": 3.2417,
      "step": 1096
    },
    {
      "epoch": 0.09996355020958629,
      "grad_norm": 2.0856845378875732,
      "learning_rate": 4.877729739293409e-05,
      "loss": 2.9681,
      "step": 1097
    },
    {
      "epoch": 0.10005467468562056,
      "grad_norm": 2.8549904823303223,
      "learning_rate": 4.87750855944784e-05,
      "loss": 4.3383,
      "step": 1098
    },
    {
      "epoch": 0.10014579916165482,
      "grad_norm": 3.344149351119995,
      "learning_rate": 4.87728718475608e-05,
      "loss": 2.9673,
      "step": 1099
    },
    {
      "epoch": 0.10023692363768909,
      "grad_norm": 3.0725624561309814,
      "learning_rate": 4.877065615236272e-05,
      "loss": 3.4142,
      "step": 1100
    },
    {
      "epoch": 0.10032804811372334,
      "grad_norm": 3.4187722206115723,
      "learning_rate": 4.876843850906574e-05,
      "loss": 3.392,
      "step": 1101
    },
    {
      "epoch": 0.1004191725897576,
      "grad_norm": 1.4068093299865723,
      "learning_rate": 4.8766218917851614e-05,
      "loss": 2.9884,
      "step": 1102
    },
    {
      "epoch": 0.10051029706579187,
      "grad_norm": 1.7465176582336426,
      "learning_rate": 4.876399737890223e-05,
      "loss": 3.1005,
      "step": 1103
    },
    {
      "epoch": 0.10060142154182614,
      "grad_norm": 2.4753262996673584,
      "learning_rate": 4.876177389239967e-05,
      "loss": 3.3825,
      "step": 1104
    },
    {
      "epoch": 0.1006925460178604,
      "grad_norm": 2.6038968563079834,
      "learning_rate": 4.8759548458526145e-05,
      "loss": 3.4349,
      "step": 1105
    },
    {
      "epoch": 0.10078367049389467,
      "grad_norm": 3.703859806060791,
      "learning_rate": 4.8757321077464035e-05,
      "loss": 3.128,
      "step": 1106
    },
    {
      "epoch": 0.10087479496992892,
      "grad_norm": 2.0796546936035156,
      "learning_rate": 4.87550917493959e-05,
      "loss": 3.1497,
      "step": 1107
    },
    {
      "epoch": 0.10096591944596318,
      "grad_norm": 1.381535291671753,
      "learning_rate": 4.8752860474504424e-05,
      "loss": 2.9456,
      "step": 1108
    },
    {
      "epoch": 0.10105704392199745,
      "grad_norm": 1.740310549736023,
      "learning_rate": 4.875062725297248e-05,
      "loss": 3.454,
      "step": 1109
    },
    {
      "epoch": 0.10114816839803172,
      "grad_norm": 2.3114092350006104,
      "learning_rate": 4.874839208498309e-05,
      "loss": 3.149,
      "step": 1110
    },
    {
      "epoch": 0.10123929287406598,
      "grad_norm": 2.582498788833618,
      "learning_rate": 4.8746154970719414e-05,
      "loss": 3.0988,
      "step": 1111
    },
    {
      "epoch": 0.10133041735010023,
      "grad_norm": 1.5047119855880737,
      "learning_rate": 4.874391591036482e-05,
      "loss": 3.0241,
      "step": 1112
    },
    {
      "epoch": 0.1014215418261345,
      "grad_norm": 2.3863258361816406,
      "learning_rate": 4.87416749041028e-05,
      "loss": 3.3233,
      "step": 1113
    },
    {
      "epoch": 0.10151266630216876,
      "grad_norm": 2.8434104919433594,
      "learning_rate": 4.8739431952117e-05,
      "loss": 3.3524,
      "step": 1114
    },
    {
      "epoch": 0.10160379077820303,
      "grad_norm": 2.5264041423797607,
      "learning_rate": 4.8737187054591256e-05,
      "loss": 3.2051,
      "step": 1115
    },
    {
      "epoch": 0.1016949152542373,
      "grad_norm": 1.7804640531539917,
      "learning_rate": 4.873494021170953e-05,
      "loss": 2.987,
      "step": 1116
    },
    {
      "epoch": 0.10178603973027155,
      "grad_norm": 1.650615930557251,
      "learning_rate": 4.873269142365598e-05,
      "loss": 2.9908,
      "step": 1117
    },
    {
      "epoch": 0.10187716420630581,
      "grad_norm": 1.6238685846328735,
      "learning_rate": 4.873044069061489e-05,
      "loss": 3.0443,
      "step": 1118
    },
    {
      "epoch": 0.10196828868234008,
      "grad_norm": 2.299797773361206,
      "learning_rate": 4.87281880127707e-05,
      "loss": 3.355,
      "step": 1119
    },
    {
      "epoch": 0.10205941315837434,
      "grad_norm": 1.3027498722076416,
      "learning_rate": 4.872593339030806e-05,
      "loss": 2.98,
      "step": 1120
    },
    {
      "epoch": 0.10215053763440861,
      "grad_norm": 1.9283638000488281,
      "learning_rate": 4.872367682341173e-05,
      "loss": 3.3087,
      "step": 1121
    },
    {
      "epoch": 0.10224166211044286,
      "grad_norm": 3.2081453800201416,
      "learning_rate": 4.872141831226664e-05,
      "loss": 3.0544,
      "step": 1122
    },
    {
      "epoch": 0.10233278658647713,
      "grad_norm": 3.2388205528259277,
      "learning_rate": 4.871915785705788e-05,
      "loss": 3.2673,
      "step": 1123
    },
    {
      "epoch": 0.10242391106251139,
      "grad_norm": 2.2480404376983643,
      "learning_rate": 4.871689545797072e-05,
      "loss": 3.2273,
      "step": 1124
    },
    {
      "epoch": 0.10251503553854566,
      "grad_norm": 3.065657615661621,
      "learning_rate": 4.871463111519056e-05,
      "loss": 3.3631,
      "step": 1125
    },
    {
      "epoch": 0.10260616001457992,
      "grad_norm": 2.148409843444824,
      "learning_rate": 4.8712364828902965e-05,
      "loss": 2.8683,
      "step": 1126
    },
    {
      "epoch": 0.10269728449061417,
      "grad_norm": 2.8003501892089844,
      "learning_rate": 4.8710096599293695e-05,
      "loss": 3.2381,
      "step": 1127
    },
    {
      "epoch": 0.10278840896664844,
      "grad_norm": 2.558243989944458,
      "learning_rate": 4.870782642654861e-05,
      "loss": 3.3222,
      "step": 1128
    },
    {
      "epoch": 0.1028795334426827,
      "grad_norm": 4.242537975311279,
      "learning_rate": 4.870555431085377e-05,
      "loss": 3.296,
      "step": 1129
    },
    {
      "epoch": 0.10297065791871697,
      "grad_norm": 2.546668291091919,
      "learning_rate": 4.8703280252395385e-05,
      "loss": 3.1536,
      "step": 1130
    },
    {
      "epoch": 0.10306178239475124,
      "grad_norm": 1.5829936265945435,
      "learning_rate": 4.870100425135982e-05,
      "loss": 2.9867,
      "step": 1131
    },
    {
      "epoch": 0.10315290687078549,
      "grad_norm": 2.930858850479126,
      "learning_rate": 4.869872630793361e-05,
      "loss": 3.2111,
      "step": 1132
    },
    {
      "epoch": 0.10324403134681975,
      "grad_norm": 2.89884877204895,
      "learning_rate": 4.869644642230343e-05,
      "loss": 3.1203,
      "step": 1133
    },
    {
      "epoch": 0.10333515582285402,
      "grad_norm": 4.641218662261963,
      "learning_rate": 4.869416459465615e-05,
      "loss": 3.4105,
      "step": 1134
    },
    {
      "epoch": 0.10342628029888828,
      "grad_norm": 3.085787534713745,
      "learning_rate": 4.869188082517874e-05,
      "loss": 3.2745,
      "step": 1135
    },
    {
      "epoch": 0.10351740477492255,
      "grad_norm": 1.8495999574661255,
      "learning_rate": 4.8689595114058375e-05,
      "loss": 3.032,
      "step": 1136
    },
    {
      "epoch": 0.1036085292509568,
      "grad_norm": 1.8329887390136719,
      "learning_rate": 4.86873074614824e-05,
      "loss": 3.0344,
      "step": 1137
    },
    {
      "epoch": 0.10369965372699107,
      "grad_norm": 1.369670033454895,
      "learning_rate": 4.868501786763827e-05,
      "loss": 3.0423,
      "step": 1138
    },
    {
      "epoch": 0.10379077820302533,
      "grad_norm": 3.01965594291687,
      "learning_rate": 4.868272633271363e-05,
      "loss": 3.0057,
      "step": 1139
    },
    {
      "epoch": 0.1038819026790596,
      "grad_norm": 3.3625049591064453,
      "learning_rate": 4.868043285689631e-05,
      "loss": 3.0685,
      "step": 1140
    },
    {
      "epoch": 0.10397302715509386,
      "grad_norm": 2.6331353187561035,
      "learning_rate": 4.867813744037423e-05,
      "loss": 3.3065,
      "step": 1141
    },
    {
      "epoch": 0.10406415163112812,
      "grad_norm": 2.193513870239258,
      "learning_rate": 4.867584008333553e-05,
      "loss": 3.1842,
      "step": 1142
    },
    {
      "epoch": 0.10415527610716238,
      "grad_norm": 3.555842638015747,
      "learning_rate": 4.867354078596848e-05,
      "loss": 3.3865,
      "step": 1143
    },
    {
      "epoch": 0.10424640058319665,
      "grad_norm": 1.3614274263381958,
      "learning_rate": 4.867123954846152e-05,
      "loss": 3.1375,
      "step": 1144
    },
    {
      "epoch": 0.10433752505923091,
      "grad_norm": 2.8560686111450195,
      "learning_rate": 4.8668936371003246e-05,
      "loss": 3.347,
      "step": 1145
    },
    {
      "epoch": 0.10442864953526518,
      "grad_norm": 3.2532799243927,
      "learning_rate": 4.8666631253782405e-05,
      "loss": 3.5448,
      "step": 1146
    },
    {
      "epoch": 0.10451977401129943,
      "grad_norm": 2.5030109882354736,
      "learning_rate": 4.866432419698792e-05,
      "loss": 3.1628,
      "step": 1147
    },
    {
      "epoch": 0.1046108984873337,
      "grad_norm": 2.1041152477264404,
      "learning_rate": 4.866201520080886e-05,
      "loss": 3.211,
      "step": 1148
    },
    {
      "epoch": 0.10470202296336796,
      "grad_norm": 3.0748095512390137,
      "learning_rate": 4.8659704265434466e-05,
      "loss": 3.3296,
      "step": 1149
    },
    {
      "epoch": 0.10479314743940223,
      "grad_norm": 1.9824806451797485,
      "learning_rate": 4.865739139105411e-05,
      "loss": 3.087,
      "step": 1150
    },
    {
      "epoch": 0.10488427191543649,
      "grad_norm": 2.9198248386383057,
      "learning_rate": 4.8655076577857344e-05,
      "loss": 2.9909,
      "step": 1151
    },
    {
      "epoch": 0.10497539639147074,
      "grad_norm": 3.1498401165008545,
      "learning_rate": 4.8652759826033886e-05,
      "loss": 4.2349,
      "step": 1152
    },
    {
      "epoch": 0.10506652086750501,
      "grad_norm": 2.7998111248016357,
      "learning_rate": 4.86504411357736e-05,
      "loss": 3.1282,
      "step": 1153
    },
    {
      "epoch": 0.10515764534353927,
      "grad_norm": 1.2687691450119019,
      "learning_rate": 4.864812050726651e-05,
      "loss": 2.9539,
      "step": 1154
    },
    {
      "epoch": 0.10524876981957354,
      "grad_norm": 2.9933853149414062,
      "learning_rate": 4.86457979407028e-05,
      "loss": 3.2156,
      "step": 1155
    },
    {
      "epoch": 0.1053398942956078,
      "grad_norm": 3.073864698410034,
      "learning_rate": 4.864347343627281e-05,
      "loss": 3.0966,
      "step": 1156
    },
    {
      "epoch": 0.10543101877164206,
      "grad_norm": 3.451331615447998,
      "learning_rate": 4.864114699416706e-05,
      "loss": 3.4785,
      "step": 1157
    },
    {
      "epoch": 0.10552214324767632,
      "grad_norm": 1.6930314302444458,
      "learning_rate": 4.863881861457619e-05,
      "loss": 3.0383,
      "step": 1158
    },
    {
      "epoch": 0.10561326772371059,
      "grad_norm": 3.148261785507202,
      "learning_rate": 4.8636488297691025e-05,
      "loss": 2.4165,
      "step": 1159
    },
    {
      "epoch": 0.10570439219974485,
      "grad_norm": 3.410334587097168,
      "learning_rate": 4.863415604370255e-05,
      "loss": 3.277,
      "step": 1160
    },
    {
      "epoch": 0.10579551667577912,
      "grad_norm": 2.989598274230957,
      "learning_rate": 4.8631821852801894e-05,
      "loss": 3.1124,
      "step": 1161
    },
    {
      "epoch": 0.10588664115181337,
      "grad_norm": 2.705728530883789,
      "learning_rate": 4.8629485725180364e-05,
      "loss": 3.4259,
      "step": 1162
    },
    {
      "epoch": 0.10597776562784764,
      "grad_norm": 2.8298377990722656,
      "learning_rate": 4.862714766102941e-05,
      "loss": 2.6366,
      "step": 1163
    },
    {
      "epoch": 0.1060688901038819,
      "grad_norm": 2.781217098236084,
      "learning_rate": 4.862480766054064e-05,
      "loss": 3.2281,
      "step": 1164
    },
    {
      "epoch": 0.10616001457991617,
      "grad_norm": 1.5384852886199951,
      "learning_rate": 4.862246572390583e-05,
      "loss": 2.9956,
      "step": 1165
    },
    {
      "epoch": 0.10625113905595043,
      "grad_norm": 3.5114247798919678,
      "learning_rate": 4.862012185131691e-05,
      "loss": 2.7165,
      "step": 1166
    },
    {
      "epoch": 0.10634226353198468,
      "grad_norm": 2.069568634033203,
      "learning_rate": 4.861777604296597e-05,
      "loss": 3.2318,
      "step": 1167
    },
    {
      "epoch": 0.10643338800801895,
      "grad_norm": 3.111557722091675,
      "learning_rate": 4.8615428299045265e-05,
      "loss": 3.1355,
      "step": 1168
    },
    {
      "epoch": 0.10652451248405322,
      "grad_norm": 1.6879878044128418,
      "learning_rate": 4.86130786197472e-05,
      "loss": 3.1002,
      "step": 1169
    },
    {
      "epoch": 0.10661563696008748,
      "grad_norm": 3.0851385593414307,
      "learning_rate": 4.861072700526433e-05,
      "loss": 4.5198,
      "step": 1170
    },
    {
      "epoch": 0.10670676143612175,
      "grad_norm": 1.7297760248184204,
      "learning_rate": 4.860837345578938e-05,
      "loss": 3.0423,
      "step": 1171
    },
    {
      "epoch": 0.106797885912156,
      "grad_norm": 2.900595188140869,
      "learning_rate": 4.860601797151525e-05,
      "loss": 3.3863,
      "step": 1172
    },
    {
      "epoch": 0.10688901038819026,
      "grad_norm": 2.360994815826416,
      "learning_rate": 4.8603660552634965e-05,
      "loss": 3.3508,
      "step": 1173
    },
    {
      "epoch": 0.10698013486422453,
      "grad_norm": 3.4403953552246094,
      "learning_rate": 4.860130119934173e-05,
      "loss": 3.4562,
      "step": 1174
    },
    {
      "epoch": 0.1070712593402588,
      "grad_norm": 2.7521438598632812,
      "learning_rate": 4.85989399118289e-05,
      "loss": 3.066,
      "step": 1175
    },
    {
      "epoch": 0.10716238381629306,
      "grad_norm": 2.927729606628418,
      "learning_rate": 4.859657669029e-05,
      "loss": 3.3748,
      "step": 1176
    },
    {
      "epoch": 0.10725350829232731,
      "grad_norm": 2.8207826614379883,
      "learning_rate": 4.859421153491869e-05,
      "loss": 3.097,
      "step": 1177
    },
    {
      "epoch": 0.10734463276836158,
      "grad_norm": 2.950704336166382,
      "learning_rate": 4.859184444590882e-05,
      "loss": 2.8803,
      "step": 1178
    },
    {
      "epoch": 0.10743575724439584,
      "grad_norm": 2.3205230236053467,
      "learning_rate": 4.858947542345438e-05,
      "loss": 3.405,
      "step": 1179
    },
    {
      "epoch": 0.10752688172043011,
      "grad_norm": 2.2538068294525146,
      "learning_rate": 4.858710446774951e-05,
      "loss": 3.2668,
      "step": 1180
    },
    {
      "epoch": 0.10761800619646437,
      "grad_norm": 1.6739517450332642,
      "learning_rate": 4.858473157898853e-05,
      "loss": 3.096,
      "step": 1181
    },
    {
      "epoch": 0.10770913067249863,
      "grad_norm": 2.09653377532959,
      "learning_rate": 4.85823567573659e-05,
      "loss": 3.1997,
      "step": 1182
    },
    {
      "epoch": 0.10780025514853289,
      "grad_norm": 2.1584832668304443,
      "learning_rate": 4.8579980003076245e-05,
      "loss": 3.1801,
      "step": 1183
    },
    {
      "epoch": 0.10789137962456716,
      "grad_norm": 3.139174461364746,
      "learning_rate": 4.857760131631436e-05,
      "loss": 2.9841,
      "step": 1184
    },
    {
      "epoch": 0.10798250410060142,
      "grad_norm": 3.363103151321411,
      "learning_rate": 4.857522069727518e-05,
      "loss": 3.3184,
      "step": 1185
    },
    {
      "epoch": 0.10807362857663569,
      "grad_norm": 2.7438161373138428,
      "learning_rate": 4.857283814615381e-05,
      "loss": 2.904,
      "step": 1186
    },
    {
      "epoch": 0.10816475305266994,
      "grad_norm": 3.721757411956787,
      "learning_rate": 4.8570453663145506e-05,
      "loss": 2.8647,
      "step": 1187
    },
    {
      "epoch": 0.1082558775287042,
      "grad_norm": 2.8211238384246826,
      "learning_rate": 4.856806724844568e-05,
      "loss": 3.1336,
      "step": 1188
    },
    {
      "epoch": 0.10834700200473847,
      "grad_norm": 2.988065004348755,
      "learning_rate": 4.856567890224992e-05,
      "loss": 3.3704,
      "step": 1189
    },
    {
      "epoch": 0.10843812648077274,
      "grad_norm": 2.3461434841156006,
      "learning_rate": 4.856328862475396e-05,
      "loss": 3.2745,
      "step": 1190
    },
    {
      "epoch": 0.108529250956807,
      "grad_norm": 2.9137821197509766,
      "learning_rate": 4.8560896416153684e-05,
      "loss": 2.8177,
      "step": 1191
    },
    {
      "epoch": 0.10862037543284127,
      "grad_norm": 2.177649974822998,
      "learning_rate": 4.8558502276645146e-05,
      "loss": 3.0648,
      "step": 1192
    },
    {
      "epoch": 0.10871149990887552,
      "grad_norm": 4.471045970916748,
      "learning_rate": 4.8556106206424556e-05,
      "loss": 3.2198,
      "step": 1193
    },
    {
      "epoch": 0.10880262438490979,
      "grad_norm": 1.7140840291976929,
      "learning_rate": 4.855370820568829e-05,
      "loss": 3.0084,
      "step": 1194
    },
    {
      "epoch": 0.10889374886094405,
      "grad_norm": 1.9925346374511719,
      "learning_rate": 4.855130827463285e-05,
      "loss": 3.076,
      "step": 1195
    },
    {
      "epoch": 0.10898487333697832,
      "grad_norm": 4.0725555419921875,
      "learning_rate": 4.8548906413454944e-05,
      "loss": 3.1775,
      "step": 1196
    },
    {
      "epoch": 0.10907599781301258,
      "grad_norm": 4.49186897277832,
      "learning_rate": 4.85465026223514e-05,
      "loss": 3.0196,
      "step": 1197
    },
    {
      "epoch": 0.10916712228904683,
      "grad_norm": 2.624659776687622,
      "learning_rate": 4.8544096901519227e-05,
      "loss": 3.3422,
      "step": 1198
    },
    {
      "epoch": 0.1092582467650811,
      "grad_norm": 2.492645740509033,
      "learning_rate": 4.8541689251155575e-05,
      "loss": 3.1926,
      "step": 1199
    },
    {
      "epoch": 0.10934937124111536,
      "grad_norm": 1.60177743434906,
      "learning_rate": 4.853927967145777e-05,
      "loss": 2.946,
      "step": 1200
    },
    {
      "epoch": 0.10944049571714963,
      "grad_norm": 2.5063915252685547,
      "learning_rate": 4.853686816262327e-05,
      "loss": 3.1408,
      "step": 1201
    },
    {
      "epoch": 0.1095316201931839,
      "grad_norm": 8.176122665405273,
      "learning_rate": 4.8534454724849734e-05,
      "loss": 3.2513,
      "step": 1202
    },
    {
      "epoch": 0.10962274466921815,
      "grad_norm": 2.628269910812378,
      "learning_rate": 4.853203935833493e-05,
      "loss": 3.1195,
      "step": 1203
    },
    {
      "epoch": 0.10971386914525241,
      "grad_norm": 2.0619876384735107,
      "learning_rate": 4.8529622063276814e-05,
      "loss": 3.3001,
      "step": 1204
    },
    {
      "epoch": 0.10980499362128668,
      "grad_norm": 3.177309989929199,
      "learning_rate": 4.85272028398735e-05,
      "loss": 3.09,
      "step": 1205
    },
    {
      "epoch": 0.10989611809732094,
      "grad_norm": 2.6613833904266357,
      "learning_rate": 4.852478168832323e-05,
      "loss": 3.297,
      "step": 1206
    },
    {
      "epoch": 0.10998724257335521,
      "grad_norm": 2.257078170776367,
      "learning_rate": 4.852235860882446e-05,
      "loss": 2.2332,
      "step": 1207
    },
    {
      "epoch": 0.11007836704938946,
      "grad_norm": 3.8638627529144287,
      "learning_rate": 4.851993360157575e-05,
      "loss": 2.7631,
      "step": 1208
    },
    {
      "epoch": 0.11016949152542373,
      "grad_norm": 2.2826056480407715,
      "learning_rate": 4.851750666677584e-05,
      "loss": 3.3047,
      "step": 1209
    },
    {
      "epoch": 0.11026061600145799,
      "grad_norm": 4.1844706535339355,
      "learning_rate": 4.851507780462362e-05,
      "loss": 3.2165,
      "step": 1210
    },
    {
      "epoch": 0.11035174047749226,
      "grad_norm": 2.7843410968780518,
      "learning_rate": 4.8512647015318166e-05,
      "loss": 2.7579,
      "step": 1211
    },
    {
      "epoch": 0.11044286495352652,
      "grad_norm": 1.784948706626892,
      "learning_rate": 4.851021429905868e-05,
      "loss": 3.0934,
      "step": 1212
    },
    {
      "epoch": 0.11053398942956077,
      "grad_norm": 2.8713064193725586,
      "learning_rate": 4.850777965604453e-05,
      "loss": 2.8885,
      "step": 1213
    },
    {
      "epoch": 0.11062511390559504,
      "grad_norm": 2.985374927520752,
      "learning_rate": 4.850534308647524e-05,
      "loss": 3.232,
      "step": 1214
    },
    {
      "epoch": 0.1107162383816293,
      "grad_norm": 1.9292786121368408,
      "learning_rate": 4.8502904590550514e-05,
      "loss": 3.1795,
      "step": 1215
    },
    {
      "epoch": 0.11080736285766357,
      "grad_norm": 3.1823911666870117,
      "learning_rate": 4.850046416847018e-05,
      "loss": 3.2059,
      "step": 1216
    },
    {
      "epoch": 0.11089848733369784,
      "grad_norm": 3.464905023574829,
      "learning_rate": 4.849802182043425e-05,
      "loss": 2.51,
      "step": 1217
    },
    {
      "epoch": 0.11098961180973209,
      "grad_norm": 1.3138779401779175,
      "learning_rate": 4.8495577546642864e-05,
      "loss": 2.9226,
      "step": 1218
    },
    {
      "epoch": 0.11108073628576635,
      "grad_norm": 2.682544469833374,
      "learning_rate": 4.849313134729637e-05,
      "loss": 3.2597,
      "step": 1219
    },
    {
      "epoch": 0.11117186076180062,
      "grad_norm": 1.8260408639907837,
      "learning_rate": 4.8490683222595224e-05,
      "loss": 2.9877,
      "step": 1220
    },
    {
      "epoch": 0.11126298523783489,
      "grad_norm": 2.5833330154418945,
      "learning_rate": 4.848823317274007e-05,
      "loss": 3.2531,
      "step": 1221
    },
    {
      "epoch": 0.11135410971386915,
      "grad_norm": 3.7257158756256104,
      "learning_rate": 4.848578119793169e-05,
      "loss": 3.3707,
      "step": 1222
    },
    {
      "epoch": 0.1114452341899034,
      "grad_norm": 2.2442314624786377,
      "learning_rate": 4.848332729837103e-05,
      "loss": 3.151,
      "step": 1223
    },
    {
      "epoch": 0.11153635866593767,
      "grad_norm": 2.0687997341156006,
      "learning_rate": 4.8480871474259215e-05,
      "loss": 3.2912,
      "step": 1224
    },
    {
      "epoch": 0.11162748314197193,
      "grad_norm": 2.783472776412964,
      "learning_rate": 4.847841372579749e-05,
      "loss": 2.5138,
      "step": 1225
    },
    {
      "epoch": 0.1117186076180062,
      "grad_norm": 2.2932865619659424,
      "learning_rate": 4.847595405318729e-05,
      "loss": 2.9144,
      "step": 1226
    },
    {
      "epoch": 0.11180973209404046,
      "grad_norm": 2.7115135192871094,
      "learning_rate": 4.847349245663019e-05,
      "loss": 2.9571,
      "step": 1227
    },
    {
      "epoch": 0.11190085657007472,
      "grad_norm": 2.5347988605499268,
      "learning_rate": 4.847102893632792e-05,
      "loss": 3.4235,
      "step": 1228
    },
    {
      "epoch": 0.11199198104610898,
      "grad_norm": 2.7291646003723145,
      "learning_rate": 4.8468563492482395e-05,
      "loss": 3.1117,
      "step": 1229
    },
    {
      "epoch": 0.11208310552214325,
      "grad_norm": 2.545180320739746,
      "learning_rate": 4.8466096125295644e-05,
      "loss": 3.1981,
      "step": 1230
    },
    {
      "epoch": 0.11217422999817751,
      "grad_norm": 1.7681077718734741,
      "learning_rate": 4.84636268349699e-05,
      "loss": 3.0884,
      "step": 1231
    },
    {
      "epoch": 0.11226535447421178,
      "grad_norm": 2.8129305839538574,
      "learning_rate": 4.846115562170751e-05,
      "loss": 3.1274,
      "step": 1232
    },
    {
      "epoch": 0.11235647895024603,
      "grad_norm": 3.1257529258728027,
      "learning_rate": 4.8458682485711014e-05,
      "loss": 3.1307,
      "step": 1233
    },
    {
      "epoch": 0.1124476034262803,
      "grad_norm": 3.14485502243042,
      "learning_rate": 4.8456207427183094e-05,
      "loss": 3.0517,
      "step": 1234
    },
    {
      "epoch": 0.11253872790231456,
      "grad_norm": 2.2514820098876953,
      "learning_rate": 4.8453730446326585e-05,
      "loss": 3.3268,
      "step": 1235
    },
    {
      "epoch": 0.11262985237834883,
      "grad_norm": 1.3902634382247925,
      "learning_rate": 4.845125154334449e-05,
      "loss": 3.0041,
      "step": 1236
    },
    {
      "epoch": 0.11272097685438309,
      "grad_norm": 1.5997536182403564,
      "learning_rate": 4.844877071843996e-05,
      "loss": 3.08,
      "step": 1237
    },
    {
      "epoch": 0.11281210133041734,
      "grad_norm": 3.2128071784973145,
      "learning_rate": 4.8446287971816305e-05,
      "loss": 3.4485,
      "step": 1238
    },
    {
      "epoch": 0.11290322580645161,
      "grad_norm": 2.0368711948394775,
      "learning_rate": 4.844380330367701e-05,
      "loss": 2.9709,
      "step": 1239
    },
    {
      "epoch": 0.11299435028248588,
      "grad_norm": 3.5499250888824463,
      "learning_rate": 4.84413167142257e-05,
      "loss": 3.1674,
      "step": 1240
    },
    {
      "epoch": 0.11308547475852014,
      "grad_norm": 2.089275598526001,
      "learning_rate": 4.8438828203666156e-05,
      "loss": 3.0767,
      "step": 1241
    },
    {
      "epoch": 0.1131765992345544,
      "grad_norm": 2.063582420349121,
      "learning_rate": 4.843633777220231e-05,
      "loss": 3.0162,
      "step": 1242
    },
    {
      "epoch": 0.11326772371058866,
      "grad_norm": 1.9222890138626099,
      "learning_rate": 4.843384542003828e-05,
      "loss": 3.0367,
      "step": 1243
    },
    {
      "epoch": 0.11335884818662292,
      "grad_norm": 1.74138343334198,
      "learning_rate": 4.843135114737832e-05,
      "loss": 3.1665,
      "step": 1244
    },
    {
      "epoch": 0.11344997266265719,
      "grad_norm": 1.58941650390625,
      "learning_rate": 4.8428854954426846e-05,
      "loss": 3.1407,
      "step": 1245
    },
    {
      "epoch": 0.11354109713869145,
      "grad_norm": 1.7341008186340332,
      "learning_rate": 4.842635684138843e-05,
      "loss": 3.0836,
      "step": 1246
    },
    {
      "epoch": 0.11363222161472572,
      "grad_norm": 2.3776652812957764,
      "learning_rate": 4.84238568084678e-05,
      "loss": 3.0876,
      "step": 1247
    },
    {
      "epoch": 0.11372334609075997,
      "grad_norm": 3.617215156555176,
      "learning_rate": 4.842135485586983e-05,
      "loss": 2.6156,
      "step": 1248
    },
    {
      "epoch": 0.11381447056679424,
      "grad_norm": 3.285801410675049,
      "learning_rate": 4.841885098379959e-05,
      "loss": 3.1519,
      "step": 1249
    },
    {
      "epoch": 0.1139055950428285,
      "grad_norm": 2.993448257446289,
      "learning_rate": 4.841634519246227e-05,
      "loss": 2.734,
      "step": 1250
    },
    {
      "epoch": 0.11399671951886277,
      "grad_norm": 4.181899070739746,
      "learning_rate": 4.841383748206324e-05,
      "loss": 3.2974,
      "step": 1251
    },
    {
      "epoch": 0.11408784399489703,
      "grad_norm": 2.419050455093384,
      "learning_rate": 4.8411327852808e-05,
      "loss": 3.2745,
      "step": 1252
    },
    {
      "epoch": 0.11417896847093129,
      "grad_norm": 1.4583121538162231,
      "learning_rate": 4.8408816304902235e-05,
      "loss": 2.9262,
      "step": 1253
    },
    {
      "epoch": 0.11427009294696555,
      "grad_norm": 2.53656268119812,
      "learning_rate": 4.8406302838551765e-05,
      "loss": 3.3562,
      "step": 1254
    },
    {
      "epoch": 0.11436121742299982,
      "grad_norm": 2.8883700370788574,
      "learning_rate": 4.840378745396259e-05,
      "loss": 3.121,
      "step": 1255
    },
    {
      "epoch": 0.11445234189903408,
      "grad_norm": 3.119507074356079,
      "learning_rate": 4.840127015134086e-05,
      "loss": 3.0548,
      "step": 1256
    },
    {
      "epoch": 0.11454346637506835,
      "grad_norm": 3.061594247817993,
      "learning_rate": 4.839875093089286e-05,
      "loss": 3.1823,
      "step": 1257
    },
    {
      "epoch": 0.1146345908511026,
      "grad_norm": 2.9661896228790283,
      "learning_rate": 4.839622979282506e-05,
      "loss": 3.3524,
      "step": 1258
    },
    {
      "epoch": 0.11472571532713687,
      "grad_norm": 2.240601062774658,
      "learning_rate": 4.8393706737344085e-05,
      "loss": 3.0736,
      "step": 1259
    },
    {
      "epoch": 0.11481683980317113,
      "grad_norm": 2.651048183441162,
      "learning_rate": 4.8391181764656696e-05,
      "loss": 3.296,
      "step": 1260
    },
    {
      "epoch": 0.1149079642792054,
      "grad_norm": 1.6715929508209229,
      "learning_rate": 4.838865487496983e-05,
      "loss": 2.9067,
      "step": 1261
    },
    {
      "epoch": 0.11499908875523966,
      "grad_norm": 2.577331304550171,
      "learning_rate": 4.838612606849058e-05,
      "loss": 3.1685,
      "step": 1262
    },
    {
      "epoch": 0.11509021323127391,
      "grad_norm": 3.5737011432647705,
      "learning_rate": 4.8383595345426184e-05,
      "loss": 2.9315,
      "step": 1263
    },
    {
      "epoch": 0.11518133770730818,
      "grad_norm": 4.45041036605835,
      "learning_rate": 4.838106270598405e-05,
      "loss": 3.2293,
      "step": 1264
    },
    {
      "epoch": 0.11527246218334244,
      "grad_norm": 2.778306007385254,
      "learning_rate": 4.837852815037173e-05,
      "loss": 3.1879,
      "step": 1265
    },
    {
      "epoch": 0.11536358665937671,
      "grad_norm": 2.894092321395874,
      "learning_rate": 4.837599167879695e-05,
      "loss": 3.4217,
      "step": 1266
    },
    {
      "epoch": 0.11545471113541098,
      "grad_norm": 2.804297685623169,
      "learning_rate": 4.837345329146758e-05,
      "loss": 2.3764,
      "step": 1267
    },
    {
      "epoch": 0.11554583561144523,
      "grad_norm": 1.5919743776321411,
      "learning_rate": 4.837091298859165e-05,
      "loss": 2.9899,
      "step": 1268
    },
    {
      "epoch": 0.11563696008747949,
      "grad_norm": 2.645395278930664,
      "learning_rate": 4.836837077037735e-05,
      "loss": 3.2655,
      "step": 1269
    },
    {
      "epoch": 0.11572808456351376,
      "grad_norm": 2.5275466442108154,
      "learning_rate": 4.8365826637033024e-05,
      "loss": 3.2374,
      "step": 1270
    },
    {
      "epoch": 0.11581920903954802,
      "grad_norm": 1.634635329246521,
      "learning_rate": 4.836328058876717e-05,
      "loss": 3.035,
      "step": 1271
    },
    {
      "epoch": 0.11591033351558229,
      "grad_norm": 3.141817808151245,
      "learning_rate": 4.836073262578846e-05,
      "loss": 3.0451,
      "step": 1272
    },
    {
      "epoch": 0.11600145799161656,
      "grad_norm": 1.7054616212844849,
      "learning_rate": 4.835818274830569e-05,
      "loss": 3.0899,
      "step": 1273
    },
    {
      "epoch": 0.1160925824676508,
      "grad_norm": 3.47708797454834,
      "learning_rate": 4.835563095652785e-05,
      "loss": 2.872,
      "step": 1274
    },
    {
      "epoch": 0.11618370694368507,
      "grad_norm": 3.0025618076324463,
      "learning_rate": 4.835307725066406e-05,
      "loss": 3.2133,
      "step": 1275
    },
    {
      "epoch": 0.11627483141971934,
      "grad_norm": 1.4146130084991455,
      "learning_rate": 4.83505216309236e-05,
      "loss": 2.9197,
      "step": 1276
    },
    {
      "epoch": 0.1163659558957536,
      "grad_norm": 2.802640914916992,
      "learning_rate": 4.834796409751593e-05,
      "loss": 3.0783,
      "step": 1277
    },
    {
      "epoch": 0.11645708037178787,
      "grad_norm": 3.4829201698303223,
      "learning_rate": 4.834540465065063e-05,
      "loss": 3.2377,
      "step": 1278
    },
    {
      "epoch": 0.11654820484782212,
      "grad_norm": 1.5123839378356934,
      "learning_rate": 4.8342843290537476e-05,
      "loss": 2.9014,
      "step": 1279
    },
    {
      "epoch": 0.11663932932385639,
      "grad_norm": 3.511087656021118,
      "learning_rate": 4.8340280017386375e-05,
      "loss": 3.5285,
      "step": 1280
    },
    {
      "epoch": 0.11673045379989065,
      "grad_norm": 3.651897430419922,
      "learning_rate": 4.833771483140739e-05,
      "loss": 3.474,
      "step": 1281
    },
    {
      "epoch": 0.11682157827592492,
      "grad_norm": 1.5524790287017822,
      "learning_rate": 4.833514773281076e-05,
      "loss": 2.9491,
      "step": 1282
    },
    {
      "epoch": 0.11691270275195918,
      "grad_norm": 3.1398253440856934,
      "learning_rate": 4.8332578721806856e-05,
      "loss": 4.4306,
      "step": 1283
    },
    {
      "epoch": 0.11700382722799343,
      "grad_norm": 3.3685638904571533,
      "learning_rate": 4.8330007798606236e-05,
      "loss": 3.0482,
      "step": 1284
    },
    {
      "epoch": 0.1170949517040277,
      "grad_norm": 2.788203001022339,
      "learning_rate": 4.832743496341958e-05,
      "loss": 2.9866,
      "step": 1285
    },
    {
      "epoch": 0.11718607618006197,
      "grad_norm": 1.323476791381836,
      "learning_rate": 4.8324860216457744e-05,
      "loss": 2.9145,
      "step": 1286
    },
    {
      "epoch": 0.11727720065609623,
      "grad_norm": 3.488725185394287,
      "learning_rate": 4.832228355793175e-05,
      "loss": 3.4236,
      "step": 1287
    },
    {
      "epoch": 0.1173683251321305,
      "grad_norm": 2.773366689682007,
      "learning_rate": 4.831970498805275e-05,
      "loss": 3.2463,
      "step": 1288
    },
    {
      "epoch": 0.11745944960816475,
      "grad_norm": 2.8726413249969482,
      "learning_rate": 4.8317124507032083e-05,
      "loss": 3.1034,
      "step": 1289
    },
    {
      "epoch": 0.11755057408419901,
      "grad_norm": 2.574613094329834,
      "learning_rate": 4.831454211508122e-05,
      "loss": 3.2809,
      "step": 1290
    },
    {
      "epoch": 0.11764169856023328,
      "grad_norm": 4.687624454498291,
      "learning_rate": 4.83119578124118e-05,
      "loss": 2.8164,
      "step": 1291
    },
    {
      "epoch": 0.11773282303626754,
      "grad_norm": 1.4595892429351807,
      "learning_rate": 4.830937159923562e-05,
      "loss": 3.0615,
      "step": 1292
    },
    {
      "epoch": 0.11782394751230181,
      "grad_norm": 2.886767625808716,
      "learning_rate": 4.830678347576463e-05,
      "loss": 2.8481,
      "step": 1293
    },
    {
      "epoch": 0.11791507198833606,
      "grad_norm": 3.3065526485443115,
      "learning_rate": 4.830419344221093e-05,
      "loss": 3.4463,
      "step": 1294
    },
    {
      "epoch": 0.11800619646437033,
      "grad_norm": 3.140231132507324,
      "learning_rate": 4.83016014987868e-05,
      "loss": 2.7775,
      "step": 1295
    },
    {
      "epoch": 0.1180973209404046,
      "grad_norm": 5.782069206237793,
      "learning_rate": 4.829900764570464e-05,
      "loss": 3.4923,
      "step": 1296
    },
    {
      "epoch": 0.11818844541643886,
      "grad_norm": 3.9417202472686768,
      "learning_rate": 4.8296411883177026e-05,
      "loss": 3.0629,
      "step": 1297
    },
    {
      "epoch": 0.11827956989247312,
      "grad_norm": 2.3533289432525635,
      "learning_rate": 4.829381421141671e-05,
      "loss": 3.1436,
      "step": 1298
    },
    {
      "epoch": 0.11837069436850738,
      "grad_norm": 3.0032739639282227,
      "learning_rate": 4.829121463063657e-05,
      "loss": 3.3542,
      "step": 1299
    },
    {
      "epoch": 0.11846181884454164,
      "grad_norm": 3.742929220199585,
      "learning_rate": 4.828861314104966e-05,
      "loss": 3.1316,
      "step": 1300
    },
    {
      "epoch": 0.11855294332057591,
      "grad_norm": 2.169480800628662,
      "learning_rate": 4.828600974286917e-05,
      "loss": 3.1513,
      "step": 1301
    },
    {
      "epoch": 0.11864406779661017,
      "grad_norm": 2.3254833221435547,
      "learning_rate": 4.8283404436308464e-05,
      "loss": 3.0674,
      "step": 1302
    },
    {
      "epoch": 0.11873519227264444,
      "grad_norm": 3.3510477542877197,
      "learning_rate": 4.828079722158105e-05,
      "loss": 2.7181,
      "step": 1303
    },
    {
      "epoch": 0.11882631674867869,
      "grad_norm": 2.6919658184051514,
      "learning_rate": 4.8278188098900626e-05,
      "loss": 3.0987,
      "step": 1304
    },
    {
      "epoch": 0.11891744122471296,
      "grad_norm": 3.264378786087036,
      "learning_rate": 4.827557706848099e-05,
      "loss": 3.0503,
      "step": 1305
    },
    {
      "epoch": 0.11900856570074722,
      "grad_norm": 2.856445550918579,
      "learning_rate": 4.827296413053614e-05,
      "loss": 4.3108,
      "step": 1306
    },
    {
      "epoch": 0.11909969017678149,
      "grad_norm": 3.0132710933685303,
      "learning_rate": 4.82703492852802e-05,
      "loss": 3.0884,
      "step": 1307
    },
    {
      "epoch": 0.11919081465281575,
      "grad_norm": 1.8259915113449097,
      "learning_rate": 4.826773253292749e-05,
      "loss": 3.1467,
      "step": 1308
    },
    {
      "epoch": 0.11928193912885,
      "grad_norm": 2.925410509109497,
      "learning_rate": 4.826511387369246e-05,
      "loss": 3.1775,
      "step": 1309
    },
    {
      "epoch": 0.11937306360488427,
      "grad_norm": 2.620035409927368,
      "learning_rate": 4.826249330778971e-05,
      "loss": 3.2156,
      "step": 1310
    },
    {
      "epoch": 0.11946418808091853,
      "grad_norm": 3.094045400619507,
      "learning_rate": 4.825987083543401e-05,
      "loss": 3.187,
      "step": 1311
    },
    {
      "epoch": 0.1195553125569528,
      "grad_norm": 5.414752960205078,
      "learning_rate": 4.825724645684027e-05,
      "loss": 3.3959,
      "step": 1312
    },
    {
      "epoch": 0.11964643703298707,
      "grad_norm": 3.0916340351104736,
      "learning_rate": 4.825462017222359e-05,
      "loss": 2.8089,
      "step": 1313
    },
    {
      "epoch": 0.11973756150902132,
      "grad_norm": 2.8586535453796387,
      "learning_rate": 4.825199198179919e-05,
      "loss": 3.2082,
      "step": 1314
    },
    {
      "epoch": 0.11982868598505558,
      "grad_norm": 3.6413466930389404,
      "learning_rate": 4.824936188578246e-05,
      "loss": 3.396,
      "step": 1315
    },
    {
      "epoch": 0.11991981046108985,
      "grad_norm": 1.7131729125976562,
      "learning_rate": 4.824672988438895e-05,
      "loss": 3.1427,
      "step": 1316
    },
    {
      "epoch": 0.12001093493712411,
      "grad_norm": 2.2340872287750244,
      "learning_rate": 4.824409597783438e-05,
      "loss": 3.2302,
      "step": 1317
    },
    {
      "epoch": 0.12010205941315838,
      "grad_norm": 2.6722652912139893,
      "learning_rate": 4.8241460166334577e-05,
      "loss": 2.9591,
      "step": 1318
    },
    {
      "epoch": 0.12019318388919263,
      "grad_norm": 2.3413898944854736,
      "learning_rate": 4.823882245010557e-05,
      "loss": 3.0534,
      "step": 1319
    },
    {
      "epoch": 0.1202843083652269,
      "grad_norm": 3.7826550006866455,
      "learning_rate": 4.823618282936354e-05,
      "loss": 4.7466,
      "step": 1320
    },
    {
      "epoch": 0.12037543284126116,
      "grad_norm": 1.7024617195129395,
      "learning_rate": 4.82335413043248e-05,
      "loss": 3.086,
      "step": 1321
    },
    {
      "epoch": 0.12046655731729543,
      "grad_norm": 1.9387747049331665,
      "learning_rate": 4.8230897875205844e-05,
      "loss": 3.2878,
      "step": 1322
    },
    {
      "epoch": 0.1205576817933297,
      "grad_norm": 1.4321894645690918,
      "learning_rate": 4.8228252542223305e-05,
      "loss": 3.1001,
      "step": 1323
    },
    {
      "epoch": 0.12064880626936395,
      "grad_norm": 3.9441845417022705,
      "learning_rate": 4.822560530559398e-05,
      "loss": 3.1041,
      "step": 1324
    },
    {
      "epoch": 0.12073993074539821,
      "grad_norm": 2.0349435806274414,
      "learning_rate": 4.8222956165534824e-05,
      "loss": 3.0336,
      "step": 1325
    },
    {
      "epoch": 0.12083105522143248,
      "grad_norm": 2.7970826625823975,
      "learning_rate": 4.822030512226294e-05,
      "loss": 3.1574,
      "step": 1326
    },
    {
      "epoch": 0.12092217969746674,
      "grad_norm": 2.463871717453003,
      "learning_rate": 4.821765217599559e-05,
      "loss": 3.2166,
      "step": 1327
    },
    {
      "epoch": 0.12101330417350101,
      "grad_norm": 2.5402960777282715,
      "learning_rate": 4.82149973269502e-05,
      "loss": 3.0926,
      "step": 1328
    },
    {
      "epoch": 0.12110442864953526,
      "grad_norm": 3.7119953632354736,
      "learning_rate": 4.821234057534434e-05,
      "loss": 3.3525,
      "step": 1329
    },
    {
      "epoch": 0.12119555312556952,
      "grad_norm": 1.9446157217025757,
      "learning_rate": 4.820968192139575e-05,
      "loss": 3.0707,
      "step": 1330
    },
    {
      "epoch": 0.12128667760160379,
      "grad_norm": 3.0048012733459473,
      "learning_rate": 4.82070213653223e-05,
      "loss": 3.2723,
      "step": 1331
    },
    {
      "epoch": 0.12137780207763806,
      "grad_norm": 3.404109239578247,
      "learning_rate": 4.820435890734204e-05,
      "loss": 4.3627,
      "step": 1332
    },
    {
      "epoch": 0.12146892655367232,
      "grad_norm": 2.606018304824829,
      "learning_rate": 4.820169454767318e-05,
      "loss": 3.1892,
      "step": 1333
    },
    {
      "epoch": 0.12156005102970657,
      "grad_norm": 3.522080183029175,
      "learning_rate": 4.819902828653406e-05,
      "loss": 3.3526,
      "step": 1334
    },
    {
      "epoch": 0.12165117550574084,
      "grad_norm": 4.494570732116699,
      "learning_rate": 4.8196360124143204e-05,
      "loss": 3.1921,
      "step": 1335
    },
    {
      "epoch": 0.1217422999817751,
      "grad_norm": 2.7442262172698975,
      "learning_rate": 4.819369006071927e-05,
      "loss": 3.2029,
      "step": 1336
    },
    {
      "epoch": 0.12183342445780937,
      "grad_norm": 2.947127342224121,
      "learning_rate": 4.819101809648108e-05,
      "loss": 3.2069,
      "step": 1337
    },
    {
      "epoch": 0.12192454893384364,
      "grad_norm": 3.326021432876587,
      "learning_rate": 4.818834423164762e-05,
      "loss": 3.2037,
      "step": 1338
    },
    {
      "epoch": 0.12201567340987789,
      "grad_norm": 2.297687292098999,
      "learning_rate": 4.818566846643801e-05,
      "loss": 3.1991,
      "step": 1339
    },
    {
      "epoch": 0.12210679788591215,
      "grad_norm": 3.1498403549194336,
      "learning_rate": 4.8182990801071546e-05,
      "loss": 3.605,
      "step": 1340
    },
    {
      "epoch": 0.12219792236194642,
      "grad_norm": 2.481204032897949,
      "learning_rate": 4.8180311235767684e-05,
      "loss": 3.0523,
      "step": 1341
    },
    {
      "epoch": 0.12228904683798068,
      "grad_norm": 1.7611827850341797,
      "learning_rate": 4.817762977074601e-05,
      "loss": 2.9838,
      "step": 1342
    },
    {
      "epoch": 0.12238017131401495,
      "grad_norm": 2.524806261062622,
      "learning_rate": 4.8174946406226286e-05,
      "loss": 3.1246,
      "step": 1343
    },
    {
      "epoch": 0.1224712957900492,
      "grad_norm": 1.8765568733215332,
      "learning_rate": 4.817226114242843e-05,
      "loss": 3.097,
      "step": 1344
    },
    {
      "epoch": 0.12256242026608347,
      "grad_norm": 2.483398675918579,
      "learning_rate": 4.816957397957249e-05,
      "loss": 3.2328,
      "step": 1345
    },
    {
      "epoch": 0.12265354474211773,
      "grad_norm": 2.1173160076141357,
      "learning_rate": 4.816688491787872e-05,
      "loss": 3.182,
      "step": 1346
    },
    {
      "epoch": 0.122744669218152,
      "grad_norm": 3.149275779724121,
      "learning_rate": 4.816419395756747e-05,
      "loss": 3.2735,
      "step": 1347
    },
    {
      "epoch": 0.12283579369418626,
      "grad_norm": 1.985774278640747,
      "learning_rate": 4.8161501098859295e-05,
      "loss": 2.91,
      "step": 1348
    },
    {
      "epoch": 0.12292691817022051,
      "grad_norm": 3.5181996822357178,
      "learning_rate": 4.8158806341974875e-05,
      "loss": 3.3365,
      "step": 1349
    },
    {
      "epoch": 0.12301804264625478,
      "grad_norm": 3.0273499488830566,
      "learning_rate": 4.8156109687135064e-05,
      "loss": 3.1726,
      "step": 1350
    },
    {
      "epoch": 0.12310916712228905,
      "grad_norm": 2.0603082180023193,
      "learning_rate": 4.8153411134560856e-05,
      "loss": 3.2063,
      "step": 1351
    },
    {
      "epoch": 0.12320029159832331,
      "grad_norm": 2.9190120697021484,
      "learning_rate": 4.8150710684473407e-05,
      "loss": 3.339,
      "step": 1352
    },
    {
      "epoch": 0.12329141607435758,
      "grad_norm": 1.935994029045105,
      "learning_rate": 4.814800833709403e-05,
      "loss": 3.0274,
      "step": 1353
    },
    {
      "epoch": 0.12338254055039184,
      "grad_norm": 2.194535493850708,
      "learning_rate": 4.814530409264421e-05,
      "loss": 2.8919,
      "step": 1354
    },
    {
      "epoch": 0.1234736650264261,
      "grad_norm": 2.7491567134857178,
      "learning_rate": 4.814259795134555e-05,
      "loss": 3.4111,
      "step": 1355
    },
    {
      "epoch": 0.12356478950246036,
      "grad_norm": 2.234680414199829,
      "learning_rate": 4.8139889913419825e-05,
      "loss": 3.2018,
      "step": 1356
    },
    {
      "epoch": 0.12365591397849462,
      "grad_norm": 2.1068971157073975,
      "learning_rate": 4.8137179979088995e-05,
      "loss": 2.811,
      "step": 1357
    },
    {
      "epoch": 0.12374703845452889,
      "grad_norm": 2.9462037086486816,
      "learning_rate": 4.8134468148575126e-05,
      "loss": 3.0475,
      "step": 1358
    },
    {
      "epoch": 0.12383816293056316,
      "grad_norm": 2.1699352264404297,
      "learning_rate": 4.813175442210047e-05,
      "loss": 2.9973,
      "step": 1359
    },
    {
      "epoch": 0.12392928740659741,
      "grad_norm": 2.727155923843384,
      "learning_rate": 4.8129038799887436e-05,
      "loss": 3.1522,
      "step": 1360
    },
    {
      "epoch": 0.12402041188263167,
      "grad_norm": 3.1219122409820557,
      "learning_rate": 4.812632128215857e-05,
      "loss": 3.1755,
      "step": 1361
    },
    {
      "epoch": 0.12411153635866594,
      "grad_norm": 1.593032956123352,
      "learning_rate": 4.8123601869136594e-05,
      "loss": 3.0699,
      "step": 1362
    },
    {
      "epoch": 0.1242026608347002,
      "grad_norm": 3.5900216102600098,
      "learning_rate": 4.8120880561044355e-05,
      "loss": 2.9064,
      "step": 1363
    },
    {
      "epoch": 0.12429378531073447,
      "grad_norm": 3.540292739868164,
      "learning_rate": 4.81181573581049e-05,
      "loss": 3.166,
      "step": 1364
    },
    {
      "epoch": 0.12438490978676872,
      "grad_norm": 2.823420763015747,
      "learning_rate": 4.811543226054138e-05,
      "loss": 2.9087,
      "step": 1365
    },
    {
      "epoch": 0.12447603426280299,
      "grad_norm": 2.7216222286224365,
      "learning_rate": 4.811270526857715e-05,
      "loss": 3.2342,
      "step": 1366
    },
    {
      "epoch": 0.12456715873883725,
      "grad_norm": 1.7820650339126587,
      "learning_rate": 4.810997638243569e-05,
      "loss": 2.9718,
      "step": 1367
    },
    {
      "epoch": 0.12465828321487152,
      "grad_norm": 3.528695821762085,
      "learning_rate": 4.8107245602340635e-05,
      "loss": 3.2545,
      "step": 1368
    },
    {
      "epoch": 0.12474940769090578,
      "grad_norm": 2.6391193866729736,
      "learning_rate": 4.8104512928515795e-05,
      "loss": 3.3742,
      "step": 1369
    },
    {
      "epoch": 0.12484053216694004,
      "grad_norm": 2.5269813537597656,
      "learning_rate": 4.8101778361185115e-05,
      "loss": 2.9321,
      "step": 1370
    },
    {
      "epoch": 0.1249316566429743,
      "grad_norm": 1.620867371559143,
      "learning_rate": 4.809904190057271e-05,
      "loss": 2.9661,
      "step": 1371
    },
    {
      "epoch": 0.12502278111900855,
      "grad_norm": 3.2925851345062256,
      "learning_rate": 4.809630354690284e-05,
      "loss": 3.0233,
      "step": 1372
    },
    {
      "epoch": 0.12511390559504282,
      "grad_norm": 1.9607048034667969,
      "learning_rate": 4.809356330039992e-05,
      "loss": 3.0341,
      "step": 1373
    },
    {
      "epoch": 0.12520503007107708,
      "grad_norm": 2.5559041500091553,
      "learning_rate": 4.809082116128853e-05,
      "loss": 3.0071,
      "step": 1374
    },
    {
      "epoch": 0.12529615454711135,
      "grad_norm": 2.898434638977051,
      "learning_rate": 4.8088077129793395e-05,
      "loss": 3.0887,
      "step": 1375
    },
    {
      "epoch": 0.12538727902314561,
      "grad_norm": 4.129066467285156,
      "learning_rate": 4.80853312061394e-05,
      "loss": 2.9477,
      "step": 1376
    },
    {
      "epoch": 0.12547840349917988,
      "grad_norm": 3.0416951179504395,
      "learning_rate": 4.80825833905516e-05,
      "loss": 3.064,
      "step": 1377
    },
    {
      "epoch": 0.12556952797521415,
      "grad_norm": 1.4269506931304932,
      "learning_rate": 4.8079833683255166e-05,
      "loss": 2.9104,
      "step": 1378
    },
    {
      "epoch": 0.1256606524512484,
      "grad_norm": 1.795159935951233,
      "learning_rate": 4.8077082084475455e-05,
      "loss": 3.0528,
      "step": 1379
    },
    {
      "epoch": 0.12575177692728268,
      "grad_norm": 1.8280433416366577,
      "learning_rate": 4.8074328594437976e-05,
      "loss": 3.0135,
      "step": 1380
    },
    {
      "epoch": 0.12584290140331694,
      "grad_norm": 2.7084426879882812,
      "learning_rate": 4.807157321336838e-05,
      "loss": 3.3247,
      "step": 1381
    },
    {
      "epoch": 0.12593402587935118,
      "grad_norm": 1.4713351726531982,
      "learning_rate": 4.8068815941492493e-05,
      "loss": 2.9893,
      "step": 1382
    },
    {
      "epoch": 0.12602515035538545,
      "grad_norm": 1.647141456604004,
      "learning_rate": 4.806605677903627e-05,
      "loss": 3.0282,
      "step": 1383
    },
    {
      "epoch": 0.1261162748314197,
      "grad_norm": 2.6358165740966797,
      "learning_rate": 4.806329572622585e-05,
      "loss": 3.0908,
      "step": 1384
    },
    {
      "epoch": 0.12620739930745398,
      "grad_norm": 2.0468578338623047,
      "learning_rate": 4.80605327832875e-05,
      "loss": 3.0642,
      "step": 1385
    },
    {
      "epoch": 0.12629852378348824,
      "grad_norm": 2.854619026184082,
      "learning_rate": 4.8057767950447676e-05,
      "loss": 3.3726,
      "step": 1386
    },
    {
      "epoch": 0.1263896482595225,
      "grad_norm": 2.3269712924957275,
      "learning_rate": 4.805500122793293e-05,
      "loss": 3.0171,
      "step": 1387
    },
    {
      "epoch": 0.12648077273555677,
      "grad_norm": 1.5846275091171265,
      "learning_rate": 4.805223261597004e-05,
      "loss": 3.0804,
      "step": 1388
    },
    {
      "epoch": 0.12657189721159104,
      "grad_norm": 3.1986141204833984,
      "learning_rate": 4.8049462114785884e-05,
      "loss": 3.3668,
      "step": 1389
    },
    {
      "epoch": 0.1266630216876253,
      "grad_norm": 3.514010429382324,
      "learning_rate": 4.804668972460752e-05,
      "loss": 2.3334,
      "step": 1390
    },
    {
      "epoch": 0.12675414616365957,
      "grad_norm": 3.47501540184021,
      "learning_rate": 4.804391544566216e-05,
      "loss": 2.9559,
      "step": 1391
    },
    {
      "epoch": 0.1268452706396938,
      "grad_norm": 2.4494612216949463,
      "learning_rate": 4.804113927817716e-05,
      "loss": 3.2474,
      "step": 1392
    },
    {
      "epoch": 0.12693639511572807,
      "grad_norm": 2.538818836212158,
      "learning_rate": 4.8038361222380054e-05,
      "loss": 3.3579,
      "step": 1393
    },
    {
      "epoch": 0.12702751959176234,
      "grad_norm": 3.1854991912841797,
      "learning_rate": 4.8035581278498496e-05,
      "loss": 3.2522,
      "step": 1394
    },
    {
      "epoch": 0.1271186440677966,
      "grad_norm": 3.3268232345581055,
      "learning_rate": 4.803279944676032e-05,
      "loss": 3.4415,
      "step": 1395
    },
    {
      "epoch": 0.12720976854383087,
      "grad_norm": 3.4658143520355225,
      "learning_rate": 4.803001572739352e-05,
      "loss": 3.2007,
      "step": 1396
    },
    {
      "epoch": 0.12730089301986514,
      "grad_norm": 2.278604507446289,
      "learning_rate": 4.802723012062622e-05,
      "loss": 3.1008,
      "step": 1397
    },
    {
      "epoch": 0.1273920174958994,
      "grad_norm": 3.4236605167388916,
      "learning_rate": 4.8024442626686706e-05,
      "loss": 3.1024,
      "step": 1398
    },
    {
      "epoch": 0.12748314197193367,
      "grad_norm": 1.7195907831192017,
      "learning_rate": 4.802165324580344e-05,
      "loss": 3.0942,
      "step": 1399
    },
    {
      "epoch": 0.12757426644796793,
      "grad_norm": 5.269683837890625,
      "learning_rate": 4.801886197820501e-05,
      "loss": 3.244,
      "step": 1400
    },
    {
      "epoch": 0.1276653909240022,
      "grad_norm": 3.6192774772644043,
      "learning_rate": 4.801606882412017e-05,
      "loss": 3.4044,
      "step": 1401
    },
    {
      "epoch": 0.12775651540003646,
      "grad_norm": 2.8494386672973633,
      "learning_rate": 4.8013273783777844e-05,
      "loss": 3.2665,
      "step": 1402
    },
    {
      "epoch": 0.1278476398760707,
      "grad_norm": 2.5437350273132324,
      "learning_rate": 4.801047685740709e-05,
      "loss": 3.0269,
      "step": 1403
    },
    {
      "epoch": 0.12793876435210497,
      "grad_norm": 3.878979444503784,
      "learning_rate": 4.800767804523713e-05,
      "loss": 2.5677,
      "step": 1404
    },
    {
      "epoch": 0.12802988882813923,
      "grad_norm": 3.141019821166992,
      "learning_rate": 4.800487734749732e-05,
      "loss": 3.2695,
      "step": 1405
    },
    {
      "epoch": 0.1281210133041735,
      "grad_norm": 2.654679536819458,
      "learning_rate": 4.8002074764417204e-05,
      "loss": 3.1931,
      "step": 1406
    },
    {
      "epoch": 0.12821213778020776,
      "grad_norm": 4.664015293121338,
      "learning_rate": 4.799927029622647e-05,
      "loss": 3.3309,
      "step": 1407
    },
    {
      "epoch": 0.12830326225624203,
      "grad_norm": 2.804391384124756,
      "learning_rate": 4.799646394315494e-05,
      "loss": 3.2404,
      "step": 1408
    },
    {
      "epoch": 0.1283943867322763,
      "grad_norm": 2.2764317989349365,
      "learning_rate": 4.799365570543262e-05,
      "loss": 3.1464,
      "step": 1409
    },
    {
      "epoch": 0.12848551120831056,
      "grad_norm": 2.904567003250122,
      "learning_rate": 4.799084558328965e-05,
      "loss": 3.3084,
      "step": 1410
    },
    {
      "epoch": 0.12857663568434483,
      "grad_norm": 1.6689000129699707,
      "learning_rate": 4.7988033576956315e-05,
      "loss": 2.981,
      "step": 1411
    },
    {
      "epoch": 0.1286677601603791,
      "grad_norm": 1.643546462059021,
      "learning_rate": 4.7985219686663096e-05,
      "loss": 3.1424,
      "step": 1412
    },
    {
      "epoch": 0.12875888463641333,
      "grad_norm": 2.7469589710235596,
      "learning_rate": 4.7982403912640594e-05,
      "loss": 3.0977,
      "step": 1413
    },
    {
      "epoch": 0.1288500091124476,
      "grad_norm": 2.6127498149871826,
      "learning_rate": 4.797958625511956e-05,
      "loss": 3.3504,
      "step": 1414
    },
    {
      "epoch": 0.12894113358848186,
      "grad_norm": 1.3221989870071411,
      "learning_rate": 4.7976766714330936e-05,
      "loss": 2.9263,
      "step": 1415
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 2.6175034046173096,
      "learning_rate": 4.7973945290505766e-05,
      "loss": 3.2393,
      "step": 1416
    },
    {
      "epoch": 0.1291233825405504,
      "grad_norm": 1.7804768085479736,
      "learning_rate": 4.79711219838753e-05,
      "loss": 3.0257,
      "step": 1417
    },
    {
      "epoch": 0.12921450701658466,
      "grad_norm": 3.141484498977661,
      "learning_rate": 4.796829679467091e-05,
      "loss": 3.3176,
      "step": 1418
    },
    {
      "epoch": 0.12930563149261892,
      "grad_norm": 2.9118833541870117,
      "learning_rate": 4.796546972312413e-05,
      "loss": 3.1751,
      "step": 1419
    },
    {
      "epoch": 0.1293967559686532,
      "grad_norm": 2.3245956897735596,
      "learning_rate": 4.796264076946665e-05,
      "loss": 3.2194,
      "step": 1420
    },
    {
      "epoch": 0.12948788044468745,
      "grad_norm": 1.7661198377609253,
      "learning_rate": 4.795980993393032e-05,
      "loss": 2.9074,
      "step": 1421
    },
    {
      "epoch": 0.12957900492072172,
      "grad_norm": 1.461868166923523,
      "learning_rate": 4.795697721674713e-05,
      "loss": 3.0488,
      "step": 1422
    },
    {
      "epoch": 0.12967012939675596,
      "grad_norm": 3.5125927925109863,
      "learning_rate": 4.795414261814923e-05,
      "loss": 3.3445,
      "step": 1423
    },
    {
      "epoch": 0.12976125387279022,
      "grad_norm": 5.004077434539795,
      "learning_rate": 4.795130613836894e-05,
      "loss": 2.4742,
      "step": 1424
    },
    {
      "epoch": 0.1298523783488245,
      "grad_norm": 2.956143856048584,
      "learning_rate": 4.7948467777638716e-05,
      "loss": 3.1681,
      "step": 1425
    },
    {
      "epoch": 0.12994350282485875,
      "grad_norm": 1.55666983127594,
      "learning_rate": 4.7945627536191166e-05,
      "loss": 3.0337,
      "step": 1426
    },
    {
      "epoch": 0.13003462730089302,
      "grad_norm": 3.5222396850585938,
      "learning_rate": 4.7942785414259064e-05,
      "loss": 4.1952,
      "step": 1427
    },
    {
      "epoch": 0.13012575177692728,
      "grad_norm": 1.7636314630508423,
      "learning_rate": 4.7939941412075326e-05,
      "loss": 3.1187,
      "step": 1428
    },
    {
      "epoch": 0.13021687625296155,
      "grad_norm": 1.582393765449524,
      "learning_rate": 4.7937095529873046e-05,
      "loss": 3.0345,
      "step": 1429
    },
    {
      "epoch": 0.13030800072899582,
      "grad_norm": 2.269049882888794,
      "learning_rate": 4.793424776788544e-05,
      "loss": 2.9892,
      "step": 1430
    },
    {
      "epoch": 0.13039912520503008,
      "grad_norm": 2.121647596359253,
      "learning_rate": 4.7931398126345895e-05,
      "loss": 3.2096,
      "step": 1431
    },
    {
      "epoch": 0.13049024968106435,
      "grad_norm": 2.4415335655212402,
      "learning_rate": 4.7928546605487956e-05,
      "loss": 3.3201,
      "step": 1432
    },
    {
      "epoch": 0.13058137415709858,
      "grad_norm": 1.8279386758804321,
      "learning_rate": 4.7925693205545306e-05,
      "loss": 3.0392,
      "step": 1433
    },
    {
      "epoch": 0.13067249863313285,
      "grad_norm": 2.7299156188964844,
      "learning_rate": 4.792283792675181e-05,
      "loss": 3.3905,
      "step": 1434
    },
    {
      "epoch": 0.13076362310916712,
      "grad_norm": 2.9097607135772705,
      "learning_rate": 4.791998076934145e-05,
      "loss": 3.1524,
      "step": 1435
    },
    {
      "epoch": 0.13085474758520138,
      "grad_norm": 1.3054111003875732,
      "learning_rate": 4.79171217335484e-05,
      "loss": 3.0252,
      "step": 1436
    },
    {
      "epoch": 0.13094587206123565,
      "grad_norm": 2.88234543800354,
      "learning_rate": 4.7914260819606956e-05,
      "loss": 3.0998,
      "step": 1437
    },
    {
      "epoch": 0.1310369965372699,
      "grad_norm": 1.7457668781280518,
      "learning_rate": 4.791139802775158e-05,
      "loss": 3.1138,
      "step": 1438
    },
    {
      "epoch": 0.13112812101330418,
      "grad_norm": 2.7463388442993164,
      "learning_rate": 4.79085333582169e-05,
      "loss": 3.1257,
      "step": 1439
    },
    {
      "epoch": 0.13121924548933844,
      "grad_norm": 2.5777535438537598,
      "learning_rate": 4.790566681123768e-05,
      "loss": 3.1941,
      "step": 1440
    },
    {
      "epoch": 0.1313103699653727,
      "grad_norm": 4.522150993347168,
      "learning_rate": 4.7902798387048845e-05,
      "loss": 2.92,
      "step": 1441
    },
    {
      "epoch": 0.13140149444140697,
      "grad_norm": 1.9641166925430298,
      "learning_rate": 4.789992808588547e-05,
      "loss": 3.0691,
      "step": 1442
    },
    {
      "epoch": 0.1314926189174412,
      "grad_norm": 1.9266825914382935,
      "learning_rate": 4.78970559079828e-05,
      "loss": 3.0564,
      "step": 1443
    },
    {
      "epoch": 0.13158374339347548,
      "grad_norm": 3.1203954219818115,
      "learning_rate": 4.78941818535762e-05,
      "loss": 2.7019,
      "step": 1444
    },
    {
      "epoch": 0.13167486786950974,
      "grad_norm": 4.034260272979736,
      "learning_rate": 4.7891305922901235e-05,
      "loss": 3.2111,
      "step": 1445
    },
    {
      "epoch": 0.131765992345544,
      "grad_norm": 1.3861982822418213,
      "learning_rate": 4.7888428116193585e-05,
      "loss": 3.1494,
      "step": 1446
    },
    {
      "epoch": 0.13185711682157827,
      "grad_norm": 2.3220410346984863,
      "learning_rate": 4.78855484336891e-05,
      "loss": 3.4374,
      "step": 1447
    },
    {
      "epoch": 0.13194824129761254,
      "grad_norm": 2.328646183013916,
      "learning_rate": 4.788266687562378e-05,
      "loss": 2.9784,
      "step": 1448
    },
    {
      "epoch": 0.1320393657736468,
      "grad_norm": 2.595676898956299,
      "learning_rate": 4.7879783442233776e-05,
      "loss": 3.2522,
      "step": 1449
    },
    {
      "epoch": 0.13213049024968107,
      "grad_norm": 3.259542942047119,
      "learning_rate": 4.787689813375541e-05,
      "loss": 3.4706,
      "step": 1450
    },
    {
      "epoch": 0.13222161472571534,
      "grad_norm": 3.3692314624786377,
      "learning_rate": 4.787401095042513e-05,
      "loss": 3.3304,
      "step": 1451
    },
    {
      "epoch": 0.1323127392017496,
      "grad_norm": 1.2401968240737915,
      "learning_rate": 4.787112189247956e-05,
      "loss": 2.7759,
      "step": 1452
    },
    {
      "epoch": 0.13240386367778384,
      "grad_norm": 2.5161514282226562,
      "learning_rate": 4.786823096015547e-05,
      "loss": 3.0425,
      "step": 1453
    },
    {
      "epoch": 0.1324949881538181,
      "grad_norm": 3.511383533477783,
      "learning_rate": 4.786533815368978e-05,
      "loss": 4.4405,
      "step": 1454
    },
    {
      "epoch": 0.13258611262985237,
      "grad_norm": 2.5629379749298096,
      "learning_rate": 4.786244347331956e-05,
      "loss": 3.0869,
      "step": 1455
    },
    {
      "epoch": 0.13267723710588664,
      "grad_norm": 1.6853193044662476,
      "learning_rate": 4.785954691928206e-05,
      "loss": 3.2892,
      "step": 1456
    },
    {
      "epoch": 0.1327683615819209,
      "grad_norm": 1.5359575748443604,
      "learning_rate": 4.785664849181465e-05,
      "loss": 2.9666,
      "step": 1457
    },
    {
      "epoch": 0.13285948605795517,
      "grad_norm": 2.309920310974121,
      "learning_rate": 4.785374819115487e-05,
      "loss": 3.3415,
      "step": 1458
    },
    {
      "epoch": 0.13295061053398943,
      "grad_norm": 1.5705186128616333,
      "learning_rate": 4.7850846017540404e-05,
      "loss": 2.9575,
      "step": 1459
    },
    {
      "epoch": 0.1330417350100237,
      "grad_norm": 2.871138572692871,
      "learning_rate": 4.784794197120911e-05,
      "loss": 2.8445,
      "step": 1460
    },
    {
      "epoch": 0.13313285948605796,
      "grad_norm": 2.2359492778778076,
      "learning_rate": 4.784503605239898e-05,
      "loss": 3.2162,
      "step": 1461
    },
    {
      "epoch": 0.13322398396209223,
      "grad_norm": 1.5131843090057373,
      "learning_rate": 4.7842128261348164e-05,
      "loss": 3.1909,
      "step": 1462
    },
    {
      "epoch": 0.13331510843812647,
      "grad_norm": 2.033951759338379,
      "learning_rate": 4.783921859829496e-05,
      "loss": 3.2403,
      "step": 1463
    },
    {
      "epoch": 0.13340623291416073,
      "grad_norm": 3.2505292892456055,
      "learning_rate": 4.783630706347785e-05,
      "loss": 3.3799,
      "step": 1464
    },
    {
      "epoch": 0.133497357390195,
      "grad_norm": 2.0687122344970703,
      "learning_rate": 4.783339365713542e-05,
      "loss": 3.1018,
      "step": 1465
    },
    {
      "epoch": 0.13358848186622926,
      "grad_norm": 3.194941282272339,
      "learning_rate": 4.7830478379506446e-05,
      "loss": 3.0541,
      "step": 1466
    },
    {
      "epoch": 0.13367960634226353,
      "grad_norm": 1.9203050136566162,
      "learning_rate": 4.782756123082986e-05,
      "loss": 3.1837,
      "step": 1467
    },
    {
      "epoch": 0.1337707308182978,
      "grad_norm": 1.5487380027770996,
      "learning_rate": 4.78246422113447e-05,
      "loss": 3.048,
      "step": 1468
    },
    {
      "epoch": 0.13386185529433206,
      "grad_norm": 3.313387870788574,
      "learning_rate": 4.7821721321290216e-05,
      "loss": 2.9177,
      "step": 1469
    },
    {
      "epoch": 0.13395297977036633,
      "grad_norm": 1.603618860244751,
      "learning_rate": 4.7818798560905785e-05,
      "loss": 3.0036,
      "step": 1470
    },
    {
      "epoch": 0.1340441042464006,
      "grad_norm": 2.8620636463165283,
      "learning_rate": 4.7815873930430934e-05,
      "loss": 3.0144,
      "step": 1471
    },
    {
      "epoch": 0.13413522872243486,
      "grad_norm": 3.5870542526245117,
      "learning_rate": 4.7812947430105346e-05,
      "loss": 3.2079,
      "step": 1472
    },
    {
      "epoch": 0.1342263531984691,
      "grad_norm": 1.7587014436721802,
      "learning_rate": 4.781001906016887e-05,
      "loss": 2.9849,
      "step": 1473
    },
    {
      "epoch": 0.13431747767450336,
      "grad_norm": 1.599656581878662,
      "learning_rate": 4.780708882086148e-05,
      "loss": 3.0486,
      "step": 1474
    },
    {
      "epoch": 0.13440860215053763,
      "grad_norm": 2.6890735626220703,
      "learning_rate": 4.780415671242334e-05,
      "loss": 3.2422,
      "step": 1475
    },
    {
      "epoch": 0.1344997266265719,
      "grad_norm": 2.605520248413086,
      "learning_rate": 4.780122273509473e-05,
      "loss": 3.0581,
      "step": 1476
    },
    {
      "epoch": 0.13459085110260616,
      "grad_norm": 2.7986021041870117,
      "learning_rate": 4.7798286889116113e-05,
      "loss": 3.2861,
      "step": 1477
    },
    {
      "epoch": 0.13468197557864042,
      "grad_norm": 2.143354892730713,
      "learning_rate": 4.779534917472809e-05,
      "loss": 3.1575,
      "step": 1478
    },
    {
      "epoch": 0.1347731000546747,
      "grad_norm": 3.173980712890625,
      "learning_rate": 4.779240959217141e-05,
      "loss": 2.7888,
      "step": 1479
    },
    {
      "epoch": 0.13486422453070895,
      "grad_norm": 2.8368523120880127,
      "learning_rate": 4.7789468141687e-05,
      "loss": 3.12,
      "step": 1480
    },
    {
      "epoch": 0.13495534900674322,
      "grad_norm": 2.084016799926758,
      "learning_rate": 4.778652482351591e-05,
      "loss": 3.0239,
      "step": 1481
    },
    {
      "epoch": 0.13504647348277748,
      "grad_norm": 2.599902629852295,
      "learning_rate": 4.778357963789936e-05,
      "loss": 3.1395,
      "step": 1482
    },
    {
      "epoch": 0.13513759795881175,
      "grad_norm": 3.741800546646118,
      "learning_rate": 4.778063258507872e-05,
      "loss": 3.1891,
      "step": 1483
    },
    {
      "epoch": 0.135228722434846,
      "grad_norm": 2.012301445007324,
      "learning_rate": 4.777768366529551e-05,
      "loss": 3.3807,
      "step": 1484
    },
    {
      "epoch": 0.13531984691088025,
      "grad_norm": 1.6227391958236694,
      "learning_rate": 4.777473287879142e-05,
      "loss": 3.0725,
      "step": 1485
    },
    {
      "epoch": 0.13541097138691452,
      "grad_norm": 2.414212703704834,
      "learning_rate": 4.777178022580826e-05,
      "loss": 3.3113,
      "step": 1486
    },
    {
      "epoch": 0.13550209586294878,
      "grad_norm": 3.2059073448181152,
      "learning_rate": 4.776882570658802e-05,
      "loss": 3.3099,
      "step": 1487
    },
    {
      "epoch": 0.13559322033898305,
      "grad_norm": 4.668062210083008,
      "learning_rate": 4.7765869321372836e-05,
      "loss": 3.2421,
      "step": 1488
    },
    {
      "epoch": 0.13568434481501732,
      "grad_norm": 1.8324368000030518,
      "learning_rate": 4.776291107040498e-05,
      "loss": 2.9967,
      "step": 1489
    },
    {
      "epoch": 0.13577546929105158,
      "grad_norm": 2.761749744415283,
      "learning_rate": 4.775995095392692e-05,
      "loss": 3.2551,
      "step": 1490
    },
    {
      "epoch": 0.13586659376708585,
      "grad_norm": 2.2648134231567383,
      "learning_rate": 4.775698897218123e-05,
      "loss": 3.1676,
      "step": 1491
    },
    {
      "epoch": 0.1359577182431201,
      "grad_norm": 3.5984230041503906,
      "learning_rate": 4.7754025125410654e-05,
      "loss": 2.5762,
      "step": 1492
    },
    {
      "epoch": 0.13604884271915438,
      "grad_norm": 2.518404245376587,
      "learning_rate": 4.77510594138581e-05,
      "loss": 3.1694,
      "step": 1493
    },
    {
      "epoch": 0.13613996719518862,
      "grad_norm": 2.6424813270568848,
      "learning_rate": 4.7748091837766623e-05,
      "loss": 3.5051,
      "step": 1494
    },
    {
      "epoch": 0.13623109167122288,
      "grad_norm": 1.2104846239089966,
      "learning_rate": 4.7745122397379413e-05,
      "loss": 2.9033,
      "step": 1495
    },
    {
      "epoch": 0.13632221614725715,
      "grad_norm": 2.830470085144043,
      "learning_rate": 4.774215109293984e-05,
      "loss": 2.8473,
      "step": 1496
    },
    {
      "epoch": 0.1364133406232914,
      "grad_norm": 2.8165035247802734,
      "learning_rate": 4.773917792469142e-05,
      "loss": 3.152,
      "step": 1497
    },
    {
      "epoch": 0.13650446509932568,
      "grad_norm": 2.5895888805389404,
      "learning_rate": 4.773620289287778e-05,
      "loss": 3.1265,
      "step": 1498
    },
    {
      "epoch": 0.13659558957535994,
      "grad_norm": 1.6958574056625366,
      "learning_rate": 4.773322599774278e-05,
      "loss": 3.1278,
      "step": 1499
    },
    {
      "epoch": 0.1366867140513942,
      "grad_norm": 1.5739076137542725,
      "learning_rate": 4.773024723953037e-05,
      "loss": 2.9989,
      "step": 1500
    },
    {
      "epoch": 0.13677783852742847,
      "grad_norm": 1.790186882019043,
      "learning_rate": 4.772726661848467e-05,
      "loss": 2.9863,
      "step": 1501
    },
    {
      "epoch": 0.13686896300346274,
      "grad_norm": 1.7760246992111206,
      "learning_rate": 4.7724284134849945e-05,
      "loss": 3.0368,
      "step": 1502
    },
    {
      "epoch": 0.136960087479497,
      "grad_norm": 1.7839252948760986,
      "learning_rate": 4.7721299788870634e-05,
      "loss": 3.0473,
      "step": 1503
    },
    {
      "epoch": 0.13705121195553124,
      "grad_norm": 3.4607291221618652,
      "learning_rate": 4.771831358079132e-05,
      "loss": 3.3073,
      "step": 1504
    },
    {
      "epoch": 0.1371423364315655,
      "grad_norm": 4.04274845123291,
      "learning_rate": 4.771532551085672e-05,
      "loss": 4.4627,
      "step": 1505
    },
    {
      "epoch": 0.13723346090759977,
      "grad_norm": 1.4731826782226562,
      "learning_rate": 4.771233557931172e-05,
      "loss": 2.8455,
      "step": 1506
    },
    {
      "epoch": 0.13732458538363404,
      "grad_norm": 6.251603603363037,
      "learning_rate": 4.770934378640137e-05,
      "loss": 3.3063,
      "step": 1507
    },
    {
      "epoch": 0.1374157098596683,
      "grad_norm": 2.3858461380004883,
      "learning_rate": 4.7706350132370844e-05,
      "loss": 3.1662,
      "step": 1508
    },
    {
      "epoch": 0.13750683433570257,
      "grad_norm": 2.9865639209747314,
      "learning_rate": 4.77033546174655e-05,
      "loss": 3.1456,
      "step": 1509
    },
    {
      "epoch": 0.13759795881173684,
      "grad_norm": 2.4804508686065674,
      "learning_rate": 4.7700357241930815e-05,
      "loss": 3.1369,
      "step": 1510
    },
    {
      "epoch": 0.1376890832877711,
      "grad_norm": 3.5824382305145264,
      "learning_rate": 4.769735800601245e-05,
      "loss": 3.1561,
      "step": 1511
    },
    {
      "epoch": 0.13778020776380537,
      "grad_norm": 2.742952585220337,
      "learning_rate": 4.7694356909956194e-05,
      "loss": 4.2205,
      "step": 1512
    },
    {
      "epoch": 0.13787133223983963,
      "grad_norm": 1.8473784923553467,
      "learning_rate": 4.7691353954008e-05,
      "loss": 2.8503,
      "step": 1513
    },
    {
      "epoch": 0.13796245671587387,
      "grad_norm": 2.6001064777374268,
      "learning_rate": 4.768834913841398e-05,
      "loss": 3.1715,
      "step": 1514
    },
    {
      "epoch": 0.13805358119190814,
      "grad_norm": 2.0480306148529053,
      "learning_rate": 4.768534246342038e-05,
      "loss": 3.1111,
      "step": 1515
    },
    {
      "epoch": 0.1381447056679424,
      "grad_norm": 1.9670919179916382,
      "learning_rate": 4.768233392927361e-05,
      "loss": 2.9742,
      "step": 1516
    },
    {
      "epoch": 0.13823583014397667,
      "grad_norm": 2.6016757488250732,
      "learning_rate": 4.767932353622025e-05,
      "loss": 3.2246,
      "step": 1517
    },
    {
      "epoch": 0.13832695462001093,
      "grad_norm": 1.480478048324585,
      "learning_rate": 4.767631128450699e-05,
      "loss": 2.9532,
      "step": 1518
    },
    {
      "epoch": 0.1384180790960452,
      "grad_norm": 2.2611401081085205,
      "learning_rate": 4.767329717438071e-05,
      "loss": 3.1999,
      "step": 1519
    },
    {
      "epoch": 0.13850920357207946,
      "grad_norm": 1.8727288246154785,
      "learning_rate": 4.7670281206088406e-05,
      "loss": 2.9416,
      "step": 1520
    },
    {
      "epoch": 0.13860032804811373,
      "grad_norm": 3.45841121673584,
      "learning_rate": 4.766726337987728e-05,
      "loss": 3.2848,
      "step": 1521
    },
    {
      "epoch": 0.138691452524148,
      "grad_norm": 2.117701530456543,
      "learning_rate": 4.7664243695994634e-05,
      "loss": 2.9624,
      "step": 1522
    },
    {
      "epoch": 0.13878257700018226,
      "grad_norm": 1.6901532411575317,
      "learning_rate": 4.766122215468795e-05,
      "loss": 2.9421,
      "step": 1523
    },
    {
      "epoch": 0.1388737014762165,
      "grad_norm": 3.0419657230377197,
      "learning_rate": 4.765819875620485e-05,
      "loss": 2.9388,
      "step": 1524
    },
    {
      "epoch": 0.13896482595225076,
      "grad_norm": 2.559110641479492,
      "learning_rate": 4.765517350079313e-05,
      "loss": 3.0909,
      "step": 1525
    },
    {
      "epoch": 0.13905595042828503,
      "grad_norm": 1.8691017627716064,
      "learning_rate": 4.7652146388700705e-05,
      "loss": 3.017,
      "step": 1526
    },
    {
      "epoch": 0.1391470749043193,
      "grad_norm": 2.1920082569122314,
      "learning_rate": 4.764911742017565e-05,
      "loss": 2.8878,
      "step": 1527
    },
    {
      "epoch": 0.13923819938035356,
      "grad_norm": 5.991618633270264,
      "learning_rate": 4.764608659546623e-05,
      "loss": 3.2354,
      "step": 1528
    },
    {
      "epoch": 0.13932932385638783,
      "grad_norm": 1.6653143167495728,
      "learning_rate": 4.764305391482081e-05,
      "loss": 2.9833,
      "step": 1529
    },
    {
      "epoch": 0.1394204483324221,
      "grad_norm": 2.5954642295837402,
      "learning_rate": 4.7640019378487934e-05,
      "loss": 3.1639,
      "step": 1530
    },
    {
      "epoch": 0.13951157280845636,
      "grad_norm": 3.557215929031372,
      "learning_rate": 4.763698298671629e-05,
      "loss": 2.8795,
      "step": 1531
    },
    {
      "epoch": 0.13960269728449062,
      "grad_norm": 1.8863126039505005,
      "learning_rate": 4.7633944739754746e-05,
      "loss": 2.833,
      "step": 1532
    },
    {
      "epoch": 0.1396938217605249,
      "grad_norm": 5.656214714050293,
      "learning_rate": 4.7630904637852275e-05,
      "loss": 3.1804,
      "step": 1533
    },
    {
      "epoch": 0.13978494623655913,
      "grad_norm": 2.910088300704956,
      "learning_rate": 4.7627862681258037e-05,
      "loss": 3.056,
      "step": 1534
    },
    {
      "epoch": 0.1398760707125934,
      "grad_norm": 1.1903241872787476,
      "learning_rate": 4.762481887022132e-05,
      "loss": 2.9107,
      "step": 1535
    },
    {
      "epoch": 0.13996719518862766,
      "grad_norm": 1.3917659521102905,
      "learning_rate": 4.762177320499158e-05,
      "loss": 2.9145,
      "step": 1536
    },
    {
      "epoch": 0.14005831966466192,
      "grad_norm": 2.0048739910125732,
      "learning_rate": 4.7618725685818434e-05,
      "loss": 2.9777,
      "step": 1537
    },
    {
      "epoch": 0.1401494441406962,
      "grad_norm": 3.215196371078491,
      "learning_rate": 4.761567631295163e-05,
      "loss": 2.9824,
      "step": 1538
    },
    {
      "epoch": 0.14024056861673045,
      "grad_norm": 2.187133550643921,
      "learning_rate": 4.761262508664107e-05,
      "loss": 3.1835,
      "step": 1539
    },
    {
      "epoch": 0.14033169309276472,
      "grad_norm": 2.6757946014404297,
      "learning_rate": 4.760957200713682e-05,
      "loss": 3.1799,
      "step": 1540
    },
    {
      "epoch": 0.14042281756879899,
      "grad_norm": 2.7127139568328857,
      "learning_rate": 4.760651707468908e-05,
      "loss": 3.1264,
      "step": 1541
    },
    {
      "epoch": 0.14051394204483325,
      "grad_norm": 2.6738502979278564,
      "learning_rate": 4.760346028954824e-05,
      "loss": 3.2401,
      "step": 1542
    },
    {
      "epoch": 0.14060506652086752,
      "grad_norm": 2.608595371246338,
      "learning_rate": 4.76004016519648e-05,
      "loss": 3.0873,
      "step": 1543
    },
    {
      "epoch": 0.14069619099690175,
      "grad_norm": 2.3539223670959473,
      "learning_rate": 4.7597341162189426e-05,
      "loss": 3.2628,
      "step": 1544
    },
    {
      "epoch": 0.14078731547293602,
      "grad_norm": 1.7645334005355835,
      "learning_rate": 4.7594278820472934e-05,
      "loss": 3.0802,
      "step": 1545
    },
    {
      "epoch": 0.14087843994897029,
      "grad_norm": 2.0945842266082764,
      "learning_rate": 4.759121462706631e-05,
      "loss": 3.3989,
      "step": 1546
    },
    {
      "epoch": 0.14096956442500455,
      "grad_norm": 3.260627031326294,
      "learning_rate": 4.758814858222066e-05,
      "loss": 3.2328,
      "step": 1547
    },
    {
      "epoch": 0.14106068890103882,
      "grad_norm": 3.0855679512023926,
      "learning_rate": 4.7585080686187264e-05,
      "loss": 4.337,
      "step": 1548
    },
    {
      "epoch": 0.14115181337707308,
      "grad_norm": 2.290228843688965,
      "learning_rate": 4.758201093921755e-05,
      "loss": 3.2579,
      "step": 1549
    },
    {
      "epoch": 0.14124293785310735,
      "grad_norm": 2.877847194671631,
      "learning_rate": 4.7578939341563095e-05,
      "loss": 3.0991,
      "step": 1550
    },
    {
      "epoch": 0.1413340623291416,
      "grad_norm": 2.6873252391815186,
      "learning_rate": 4.7575865893475625e-05,
      "loss": 3.2119,
      "step": 1551
    },
    {
      "epoch": 0.14142518680517588,
      "grad_norm": 2.232259750366211,
      "learning_rate": 4.757279059520703e-05,
      "loss": 2.8747,
      "step": 1552
    },
    {
      "epoch": 0.14151631128121014,
      "grad_norm": 3.1098685264587402,
      "learning_rate": 4.756971344700934e-05,
      "loss": 4.3473,
      "step": 1553
    },
    {
      "epoch": 0.14160743575724438,
      "grad_norm": 1.4829670190811157,
      "learning_rate": 4.7566634449134734e-05,
      "loss": 3.079,
      "step": 1554
    },
    {
      "epoch": 0.14169856023327865,
      "grad_norm": 2.051966667175293,
      "learning_rate": 4.7563553601835555e-05,
      "loss": 3.0552,
      "step": 1555
    },
    {
      "epoch": 0.1417896847093129,
      "grad_norm": 2.224562883377075,
      "learning_rate": 4.756047090536428e-05,
      "loss": 3.1404,
      "step": 1556
    },
    {
      "epoch": 0.14188080918534718,
      "grad_norm": 2.442316770553589,
      "learning_rate": 4.7557386359973554e-05,
      "loss": 2.9279,
      "step": 1557
    },
    {
      "epoch": 0.14197193366138144,
      "grad_norm": 2.465324640274048,
      "learning_rate": 4.755429996591618e-05,
      "loss": 2.7246,
      "step": 1558
    },
    {
      "epoch": 0.1420630581374157,
      "grad_norm": 2.204219102859497,
      "learning_rate": 4.755121172344508e-05,
      "loss": 3.0415,
      "step": 1559
    },
    {
      "epoch": 0.14215418261344998,
      "grad_norm": 1.9507112503051758,
      "learning_rate": 4.754812163281335e-05,
      "loss": 3.2588,
      "step": 1560
    },
    {
      "epoch": 0.14224530708948424,
      "grad_norm": 2.620863914489746,
      "learning_rate": 4.7545029694274254e-05,
      "loss": 2.7928,
      "step": 1561
    },
    {
      "epoch": 0.1423364315655185,
      "grad_norm": 2.5902299880981445,
      "learning_rate": 4.754193590808117e-05,
      "loss": 3.3398,
      "step": 1562
    },
    {
      "epoch": 0.14242755604155277,
      "grad_norm": 1.4641609191894531,
      "learning_rate": 4.753884027448765e-05,
      "loss": 2.9775,
      "step": 1563
    },
    {
      "epoch": 0.14251868051758704,
      "grad_norm": 1.9195103645324707,
      "learning_rate": 4.753574279374739e-05,
      "loss": 3.0164,
      "step": 1564
    },
    {
      "epoch": 0.14260980499362128,
      "grad_norm": 2.0863749980926514,
      "learning_rate": 4.7532643466114266e-05,
      "loss": 3.1996,
      "step": 1565
    },
    {
      "epoch": 0.14270092946965554,
      "grad_norm": 2.3902792930603027,
      "learning_rate": 4.752954229184224e-05,
      "loss": 3.1756,
      "step": 1566
    },
    {
      "epoch": 0.1427920539456898,
      "grad_norm": 2.883986711502075,
      "learning_rate": 4.75264392711855e-05,
      "loss": 3.3782,
      "step": 1567
    },
    {
      "epoch": 0.14288317842172407,
      "grad_norm": 1.5548768043518066,
      "learning_rate": 4.752333440439832e-05,
      "loss": 3.0867,
      "step": 1568
    },
    {
      "epoch": 0.14297430289775834,
      "grad_norm": 2.3053369522094727,
      "learning_rate": 4.752022769173519e-05,
      "loss": 3.2028,
      "step": 1569
    },
    {
      "epoch": 0.1430654273737926,
      "grad_norm": 2.11873722076416,
      "learning_rate": 4.751711913345069e-05,
      "loss": 3.4555,
      "step": 1570
    },
    {
      "epoch": 0.14315655184982687,
      "grad_norm": 1.6578961610794067,
      "learning_rate": 4.7514008729799584e-05,
      "loss": 2.8687,
      "step": 1571
    },
    {
      "epoch": 0.14324767632586113,
      "grad_norm": 2.5347859859466553,
      "learning_rate": 4.7510896481036796e-05,
      "loss": 3.3134,
      "step": 1572
    },
    {
      "epoch": 0.1433388008018954,
      "grad_norm": 1.5316507816314697,
      "learning_rate": 4.750778238741737e-05,
      "loss": 3.0429,
      "step": 1573
    },
    {
      "epoch": 0.14342992527792967,
      "grad_norm": 2.9990713596343994,
      "learning_rate": 4.7504666449196534e-05,
      "loss": 3.3818,
      "step": 1574
    },
    {
      "epoch": 0.1435210497539639,
      "grad_norm": 2.222365617752075,
      "learning_rate": 4.750154866662964e-05,
      "loss": 3.1164,
      "step": 1575
    },
    {
      "epoch": 0.14361217422999817,
      "grad_norm": 1.5940253734588623,
      "learning_rate": 4.7498429039972195e-05,
      "loss": 2.9707,
      "step": 1576
    },
    {
      "epoch": 0.14370329870603243,
      "grad_norm": 1.7318164110183716,
      "learning_rate": 4.7495307569479886e-05,
      "loss": 2.9525,
      "step": 1577
    },
    {
      "epoch": 0.1437944231820667,
      "grad_norm": 2.8493340015411377,
      "learning_rate": 4.749218425540851e-05,
      "loss": 2.9688,
      "step": 1578
    },
    {
      "epoch": 0.14388554765810097,
      "grad_norm": 3.3721001148223877,
      "learning_rate": 4.748905909801405e-05,
      "loss": 3.0444,
      "step": 1579
    },
    {
      "epoch": 0.14397667213413523,
      "grad_norm": 3.1313891410827637,
      "learning_rate": 4.748593209755262e-05,
      "loss": 3.0512,
      "step": 1580
    },
    {
      "epoch": 0.1440677966101695,
      "grad_norm": 1.885908842086792,
      "learning_rate": 4.7482803254280484e-05,
      "loss": 2.9611,
      "step": 1581
    },
    {
      "epoch": 0.14415892108620376,
      "grad_norm": 2.9428458213806152,
      "learning_rate": 4.747967256845407e-05,
      "loss": 2.9153,
      "step": 1582
    },
    {
      "epoch": 0.14425004556223803,
      "grad_norm": 4.332671165466309,
      "learning_rate": 4.747654004032995e-05,
      "loss": 3.2974,
      "step": 1583
    },
    {
      "epoch": 0.1443411700382723,
      "grad_norm": 1.3580037355422974,
      "learning_rate": 4.747340567016484e-05,
      "loss": 3.0826,
      "step": 1584
    },
    {
      "epoch": 0.14443229451430653,
      "grad_norm": 9.840303421020508,
      "learning_rate": 4.747026945821562e-05,
      "loss": 2.5629,
      "step": 1585
    },
    {
      "epoch": 0.1445234189903408,
      "grad_norm": 2.672960042953491,
      "learning_rate": 4.7467131404739315e-05,
      "loss": 3.1531,
      "step": 1586
    },
    {
      "epoch": 0.14461454346637506,
      "grad_norm": 2.945892572402954,
      "learning_rate": 4.7463991509993096e-05,
      "loss": 3.3506,
      "step": 1587
    },
    {
      "epoch": 0.14470566794240933,
      "grad_norm": 2.753072738647461,
      "learning_rate": 4.74608497742343e-05,
      "loss": 3.0978,
      "step": 1588
    },
    {
      "epoch": 0.1447967924184436,
      "grad_norm": 2.628932237625122,
      "learning_rate": 4.7457706197720395e-05,
      "loss": 3.2419,
      "step": 1589
    },
    {
      "epoch": 0.14488791689447786,
      "grad_norm": 3.0868217945098877,
      "learning_rate": 4.745456078070901e-05,
      "loss": 3.1019,
      "step": 1590
    },
    {
      "epoch": 0.14497904137051212,
      "grad_norm": 1.7500460147857666,
      "learning_rate": 4.745141352345793e-05,
      "loss": 3.0763,
      "step": 1591
    },
    {
      "epoch": 0.1450701658465464,
      "grad_norm": 2.3832669258117676,
      "learning_rate": 4.744826442622508e-05,
      "loss": 3.0807,
      "step": 1592
    },
    {
      "epoch": 0.14516129032258066,
      "grad_norm": 3.4366202354431152,
      "learning_rate": 4.7445113489268544e-05,
      "loss": 3.0111,
      "step": 1593
    },
    {
      "epoch": 0.14525241479861492,
      "grad_norm": 1.3814283609390259,
      "learning_rate": 4.744196071284655e-05,
      "loss": 2.9914,
      "step": 1594
    },
    {
      "epoch": 0.14534353927464916,
      "grad_norm": 3.4350616931915283,
      "learning_rate": 4.743880609721749e-05,
      "loss": 3.4018,
      "step": 1595
    },
    {
      "epoch": 0.14543466375068342,
      "grad_norm": 3.0712473392486572,
      "learning_rate": 4.7435649642639876e-05,
      "loss": 3.1312,
      "step": 1596
    },
    {
      "epoch": 0.1455257882267177,
      "grad_norm": 2.87288761138916,
      "learning_rate": 4.743249134937242e-05,
      "loss": 3.259,
      "step": 1597
    },
    {
      "epoch": 0.14561691270275195,
      "grad_norm": 3.8843579292297363,
      "learning_rate": 4.742933121767394e-05,
      "loss": 3.4705,
      "step": 1598
    },
    {
      "epoch": 0.14570803717878622,
      "grad_norm": 2.3016998767852783,
      "learning_rate": 4.742616924780342e-05,
      "loss": 3.1254,
      "step": 1599
    },
    {
      "epoch": 0.14579916165482049,
      "grad_norm": 2.080766439437866,
      "learning_rate": 4.742300544002e-05,
      "loss": 3.0068,
      "step": 1600
    },
    {
      "epoch": 0.14589028613085475,
      "grad_norm": 1.403579831123352,
      "learning_rate": 4.741983979458296e-05,
      "loss": 3.0318,
      "step": 1601
    },
    {
      "epoch": 0.14598141060688902,
      "grad_norm": 2.5273921489715576,
      "learning_rate": 4.741667231175175e-05,
      "loss": 3.2275,
      "step": 1602
    },
    {
      "epoch": 0.14607253508292328,
      "grad_norm": 2.89467453956604,
      "learning_rate": 4.741350299178595e-05,
      "loss": 2.9018,
      "step": 1603
    },
    {
      "epoch": 0.14616365955895755,
      "grad_norm": 1.527073621749878,
      "learning_rate": 4.74103318349453e-05,
      "loss": 2.9688,
      "step": 1604
    },
    {
      "epoch": 0.14625478403499179,
      "grad_norm": 1.2628742456436157,
      "learning_rate": 4.7407158841489693e-05,
      "loss": 2.9605,
      "step": 1605
    },
    {
      "epoch": 0.14634590851102605,
      "grad_norm": 1.3628411293029785,
      "learning_rate": 4.740398401167916e-05,
      "loss": 2.8406,
      "step": 1606
    },
    {
      "epoch": 0.14643703298706032,
      "grad_norm": 2.6044845581054688,
      "learning_rate": 4.740080734577389e-05,
      "loss": 2.8398,
      "step": 1607
    },
    {
      "epoch": 0.14652815746309458,
      "grad_norm": 3.561075210571289,
      "learning_rate": 4.7397628844034225e-05,
      "loss": 4.4007,
      "step": 1608
    },
    {
      "epoch": 0.14661928193912885,
      "grad_norm": 1.4533250331878662,
      "learning_rate": 4.739444850672067e-05,
      "loss": 2.9798,
      "step": 1609
    },
    {
      "epoch": 0.1467104064151631,
      "grad_norm": 3.046421527862549,
      "learning_rate": 4.7391266334093845e-05,
      "loss": 3.0307,
      "step": 1610
    },
    {
      "epoch": 0.14680153089119738,
      "grad_norm": 2.7299585342407227,
      "learning_rate": 4.738808232641455e-05,
      "loss": 3.1078,
      "step": 1611
    },
    {
      "epoch": 0.14689265536723164,
      "grad_norm": 1.458348035812378,
      "learning_rate": 4.738489648394373e-05,
      "loss": 2.9425,
      "step": 1612
    },
    {
      "epoch": 0.1469837798432659,
      "grad_norm": 1.5302770137786865,
      "learning_rate": 4.7381708806942474e-05,
      "loss": 3.0449,
      "step": 1613
    },
    {
      "epoch": 0.14707490431930018,
      "grad_norm": 4.053586006164551,
      "learning_rate": 4.737851929567203e-05,
      "loss": 3.5561,
      "step": 1614
    },
    {
      "epoch": 0.1471660287953344,
      "grad_norm": 2.3581931591033936,
      "learning_rate": 4.737532795039378e-05,
      "loss": 2.928,
      "step": 1615
    },
    {
      "epoch": 0.14725715327136868,
      "grad_norm": 1.6155071258544922,
      "learning_rate": 4.737213477136928e-05,
      "loss": 3.1576,
      "step": 1616
    },
    {
      "epoch": 0.14734827774740294,
      "grad_norm": 2.6783015727996826,
      "learning_rate": 4.736893975886022e-05,
      "loss": 3.3114,
      "step": 1617
    },
    {
      "epoch": 0.1474394022234372,
      "grad_norm": 2.132664203643799,
      "learning_rate": 4.7365742913128434e-05,
      "loss": 2.8533,
      "step": 1618
    },
    {
      "epoch": 0.14753052669947148,
      "grad_norm": 2.1703169345855713,
      "learning_rate": 4.736254423443593e-05,
      "loss": 2.973,
      "step": 1619
    },
    {
      "epoch": 0.14762165117550574,
      "grad_norm": 1.7046056985855103,
      "learning_rate": 4.7359343723044844e-05,
      "loss": 3.0936,
      "step": 1620
    },
    {
      "epoch": 0.14771277565154,
      "grad_norm": 2.7974915504455566,
      "learning_rate": 4.7356141379217475e-05,
      "loss": 3.1275,
      "step": 1621
    },
    {
      "epoch": 0.14780390012757427,
      "grad_norm": 2.6626646518707275,
      "learning_rate": 4.735293720321626e-05,
      "loss": 3.1263,
      "step": 1622
    },
    {
      "epoch": 0.14789502460360854,
      "grad_norm": 3.38659405708313,
      "learning_rate": 4.7349731195303805e-05,
      "loss": 2.5051,
      "step": 1623
    },
    {
      "epoch": 0.1479861490796428,
      "grad_norm": 2.316164493560791,
      "learning_rate": 4.734652335574285e-05,
      "loss": 3.0061,
      "step": 1624
    },
    {
      "epoch": 0.14807727355567704,
      "grad_norm": 2.6719372272491455,
      "learning_rate": 4.7343313684796275e-05,
      "loss": 2.7537,
      "step": 1625
    },
    {
      "epoch": 0.1481683980317113,
      "grad_norm": 2.598212480545044,
      "learning_rate": 4.7340102182727155e-05,
      "loss": 2.9145,
      "step": 1626
    },
    {
      "epoch": 0.14825952250774557,
      "grad_norm": 1.4254382848739624,
      "learning_rate": 4.7336888849798664e-05,
      "loss": 2.917,
      "step": 1627
    },
    {
      "epoch": 0.14835064698377984,
      "grad_norm": 2.534688949584961,
      "learning_rate": 4.733367368627415e-05,
      "loss": 2.8936,
      "step": 1628
    },
    {
      "epoch": 0.1484417714598141,
      "grad_norm": 4.316157817840576,
      "learning_rate": 4.7330456692417115e-05,
      "loss": 3.0555,
      "step": 1629
    },
    {
      "epoch": 0.14853289593584837,
      "grad_norm": 2.75527024269104,
      "learning_rate": 4.73272378684912e-05,
      "loss": 4.4197,
      "step": 1630
    },
    {
      "epoch": 0.14862402041188263,
      "grad_norm": 2.4005396366119385,
      "learning_rate": 4.73240172147602e-05,
      "loss": 3.2265,
      "step": 1631
    },
    {
      "epoch": 0.1487151448879169,
      "grad_norm": 2.969036102294922,
      "learning_rate": 4.732079473148806e-05,
      "loss": 3.2398,
      "step": 1632
    },
    {
      "epoch": 0.14880626936395117,
      "grad_norm": 1.4357764720916748,
      "learning_rate": 4.7317570418938884e-05,
      "loss": 2.9779,
      "step": 1633
    },
    {
      "epoch": 0.14889739383998543,
      "grad_norm": 3.58138108253479,
      "learning_rate": 4.73143442773769e-05,
      "loss": 3.0653,
      "step": 1634
    },
    {
      "epoch": 0.14898851831601967,
      "grad_norm": 2.6361777782440186,
      "learning_rate": 4.731111630706652e-05,
      "loss": 2.9738,
      "step": 1635
    },
    {
      "epoch": 0.14907964279205393,
      "grad_norm": 2.812761068344116,
      "learning_rate": 4.730788650827227e-05,
      "loss": 2.9724,
      "step": 1636
    },
    {
      "epoch": 0.1491707672680882,
      "grad_norm": 2.8054981231689453,
      "learning_rate": 4.730465488125887e-05,
      "loss": 3.2216,
      "step": 1637
    },
    {
      "epoch": 0.14926189174412247,
      "grad_norm": 1.5526211261749268,
      "learning_rate": 4.7301421426291135e-05,
      "loss": 2.9845,
      "step": 1638
    },
    {
      "epoch": 0.14935301622015673,
      "grad_norm": 2.076298236846924,
      "learning_rate": 4.729818614363409e-05,
      "loss": 2.8241,
      "step": 1639
    },
    {
      "epoch": 0.149444140696191,
      "grad_norm": 1.7731536626815796,
      "learning_rate": 4.729494903355285e-05,
      "loss": 2.9929,
      "step": 1640
    },
    {
      "epoch": 0.14953526517222526,
      "grad_norm": 1.967139482498169,
      "learning_rate": 4.7291710096312736e-05,
      "loss": 2.9744,
      "step": 1641
    },
    {
      "epoch": 0.14962638964825953,
      "grad_norm": 2.6699776649475098,
      "learning_rate": 4.728846933217918e-05,
      "loss": 3.0459,
      "step": 1642
    },
    {
      "epoch": 0.1497175141242938,
      "grad_norm": 1.3542306423187256,
      "learning_rate": 4.728522674141776e-05,
      "loss": 2.9644,
      "step": 1643
    },
    {
      "epoch": 0.14980863860032806,
      "grad_norm": 2.028887987136841,
      "learning_rate": 4.728198232429424e-05,
      "loss": 2.9015,
      "step": 1644
    },
    {
      "epoch": 0.14989976307636232,
      "grad_norm": 1.5386238098144531,
      "learning_rate": 4.72787360810745e-05,
      "loss": 2.9069,
      "step": 1645
    },
    {
      "epoch": 0.14999088755239656,
      "grad_norm": 2.95621395111084,
      "learning_rate": 4.727548801202461e-05,
      "loss": 3.0468,
      "step": 1646
    },
    {
      "epoch": 0.15008201202843083,
      "grad_norm": 4.1476545333862305,
      "learning_rate": 4.7272238117410715e-05,
      "loss": 2.6694,
      "step": 1647
    },
    {
      "epoch": 0.1501731365044651,
      "grad_norm": 2.2101778984069824,
      "learning_rate": 4.726898639749919e-05,
      "loss": 3.112,
      "step": 1648
    },
    {
      "epoch": 0.15026426098049936,
      "grad_norm": 3.93254017829895,
      "learning_rate": 4.726573285255652e-05,
      "loss": 3.3209,
      "step": 1649
    },
    {
      "epoch": 0.15035538545653362,
      "grad_norm": 1.543944001197815,
      "learning_rate": 4.726247748284935e-05,
      "loss": 2.9816,
      "step": 1650
    },
    {
      "epoch": 0.1504465099325679,
      "grad_norm": 2.223081588745117,
      "learning_rate": 4.725922028864446e-05,
      "loss": 3.0477,
      "step": 1651
    },
    {
      "epoch": 0.15053763440860216,
      "grad_norm": 2.9665400981903076,
      "learning_rate": 4.725596127020879e-05,
      "loss": 3.0741,
      "step": 1652
    },
    {
      "epoch": 0.15062875888463642,
      "grad_norm": 2.24804949760437,
      "learning_rate": 4.7252700427809436e-05,
      "loss": 3.0598,
      "step": 1653
    },
    {
      "epoch": 0.1507198833606707,
      "grad_norm": 2.718592643737793,
      "learning_rate": 4.724943776171364e-05,
      "loss": 3.0261,
      "step": 1654
    },
    {
      "epoch": 0.15081100783670495,
      "grad_norm": 1.653093695640564,
      "learning_rate": 4.7246173272188774e-05,
      "loss": 2.9727,
      "step": 1655
    },
    {
      "epoch": 0.1509021323127392,
      "grad_norm": 2.4713759422302246,
      "learning_rate": 4.72429069595024e-05,
      "loss": 2.9652,
      "step": 1656
    },
    {
      "epoch": 0.15099325678877346,
      "grad_norm": 2.3406810760498047,
      "learning_rate": 4.723963882392218e-05,
      "loss": 2.9158,
      "step": 1657
    },
    {
      "epoch": 0.15108438126480772,
      "grad_norm": 1.7240417003631592,
      "learning_rate": 4.723636886571597e-05,
      "loss": 3.0695,
      "step": 1658
    },
    {
      "epoch": 0.151175505740842,
      "grad_norm": 3.368987798690796,
      "learning_rate": 4.723309708515175e-05,
      "loss": 3.1482,
      "step": 1659
    },
    {
      "epoch": 0.15126663021687625,
      "grad_norm": 1.6549758911132812,
      "learning_rate": 4.722982348249765e-05,
      "loss": 3.017,
      "step": 1660
    },
    {
      "epoch": 0.15135775469291052,
      "grad_norm": 3.695152759552002,
      "learning_rate": 4.722654805802196e-05,
      "loss": 3.2281,
      "step": 1661
    },
    {
      "epoch": 0.15144887916894478,
      "grad_norm": 2.500075578689575,
      "learning_rate": 4.7223270811993116e-05,
      "loss": 2.9514,
      "step": 1662
    },
    {
      "epoch": 0.15154000364497905,
      "grad_norm": 4.324959754943848,
      "learning_rate": 4.721999174467969e-05,
      "loss": 3.2794,
      "step": 1663
    },
    {
      "epoch": 0.15163112812101331,
      "grad_norm": 2.2761075496673584,
      "learning_rate": 4.7216710856350424e-05,
      "loss": 2.7181,
      "step": 1664
    },
    {
      "epoch": 0.15172225259704758,
      "grad_norm": 2.5118227005004883,
      "learning_rate": 4.7213428147274195e-05,
      "loss": 3.2282,
      "step": 1665
    },
    {
      "epoch": 0.15181337707308182,
      "grad_norm": 2.3931479454040527,
      "learning_rate": 4.721014361772005e-05,
      "loss": 2.7632,
      "step": 1666
    },
    {
      "epoch": 0.15190450154911608,
      "grad_norm": 1.8002700805664062,
      "learning_rate": 4.720685726795714e-05,
      "loss": 2.9921,
      "step": 1667
    },
    {
      "epoch": 0.15199562602515035,
      "grad_norm": 1.4867767095565796,
      "learning_rate": 4.720356909825482e-05,
      "loss": 2.991,
      "step": 1668
    },
    {
      "epoch": 0.15208675050118461,
      "grad_norm": 2.3881375789642334,
      "learning_rate": 4.7200279108882554e-05,
      "loss": 3.2776,
      "step": 1669
    },
    {
      "epoch": 0.15217787497721888,
      "grad_norm": 2.2196474075317383,
      "learning_rate": 4.7196987300109974e-05,
      "loss": 3.1953,
      "step": 1670
    },
    {
      "epoch": 0.15226899945325315,
      "grad_norm": 1.6471738815307617,
      "learning_rate": 4.719369367220686e-05,
      "loss": 3.0896,
      "step": 1671
    },
    {
      "epoch": 0.1523601239292874,
      "grad_norm": 2.35978364944458,
      "learning_rate": 4.7190398225443134e-05,
      "loss": 2.9034,
      "step": 1672
    },
    {
      "epoch": 0.15245124840532168,
      "grad_norm": 3.6606178283691406,
      "learning_rate": 4.718710096008887e-05,
      "loss": 3.1773,
      "step": 1673
    },
    {
      "epoch": 0.15254237288135594,
      "grad_norm": 2.787719249725342,
      "learning_rate": 4.7183801876414294e-05,
      "loss": 3.3528,
      "step": 1674
    },
    {
      "epoch": 0.1526334973573902,
      "grad_norm": 4.270319938659668,
      "learning_rate": 4.718050097468978e-05,
      "loss": 2.5025,
      "step": 1675
    },
    {
      "epoch": 0.15272462183342445,
      "grad_norm": 1.192939281463623,
      "learning_rate": 4.717719825518585e-05,
      "loss": 2.9122,
      "step": 1676
    },
    {
      "epoch": 0.1528157463094587,
      "grad_norm": 2.2663092613220215,
      "learning_rate": 4.717389371817316e-05,
      "loss": 3.0213,
      "step": 1677
    },
    {
      "epoch": 0.15290687078549298,
      "grad_norm": 2.0562195777893066,
      "learning_rate": 4.717058736392256e-05,
      "loss": 3.0291,
      "step": 1678
    },
    {
      "epoch": 0.15299799526152724,
      "grad_norm": 1.2530059814453125,
      "learning_rate": 4.716727919270499e-05,
      "loss": 2.8827,
      "step": 1679
    },
    {
      "epoch": 0.1530891197375615,
      "grad_norm": 1.6773467063903809,
      "learning_rate": 4.716396920479158e-05,
      "loss": 2.9988,
      "step": 1680
    },
    {
      "epoch": 0.15318024421359577,
      "grad_norm": 2.1727144718170166,
      "learning_rate": 4.71606574004536e-05,
      "loss": 2.9866,
      "step": 1681
    },
    {
      "epoch": 0.15327136868963004,
      "grad_norm": 2.6730339527130127,
      "learning_rate": 4.715734377996246e-05,
      "loss": 3.1705,
      "step": 1682
    },
    {
      "epoch": 0.1533624931656643,
      "grad_norm": 2.3407809734344482,
      "learning_rate": 4.7154028343589726e-05,
      "loss": 2.997,
      "step": 1683
    },
    {
      "epoch": 0.15345361764169857,
      "grad_norm": 2.6133763790130615,
      "learning_rate": 4.7150711091607114e-05,
      "loss": 3.0567,
      "step": 1684
    },
    {
      "epoch": 0.15354474211773284,
      "grad_norm": 1.2702889442443848,
      "learning_rate": 4.714739202428648e-05,
      "loss": 2.9257,
      "step": 1685
    },
    {
      "epoch": 0.15363586659376707,
      "grad_norm": 2.4354734420776367,
      "learning_rate": 4.714407114189984e-05,
      "loss": 2.9063,
      "step": 1686
    },
    {
      "epoch": 0.15372699106980134,
      "grad_norm": 2.6022043228149414,
      "learning_rate": 4.714074844471934e-05,
      "loss": 3.0259,
      "step": 1687
    },
    {
      "epoch": 0.1538181155458356,
      "grad_norm": 3.257946252822876,
      "learning_rate": 4.713742393301731e-05,
      "loss": 3.0208,
      "step": 1688
    },
    {
      "epoch": 0.15390924002186987,
      "grad_norm": 2.0955233573913574,
      "learning_rate": 4.7134097607066194e-05,
      "loss": 2.9569,
      "step": 1689
    },
    {
      "epoch": 0.15400036449790414,
      "grad_norm": 2.2379086017608643,
      "learning_rate": 4.713076946713859e-05,
      "loss": 3.005,
      "step": 1690
    },
    {
      "epoch": 0.1540914889739384,
      "grad_norm": 2.551835060119629,
      "learning_rate": 4.712743951350727e-05,
      "loss": 2.8125,
      "step": 1691
    },
    {
      "epoch": 0.15418261344997267,
      "grad_norm": 1.4898242950439453,
      "learning_rate": 4.7124107746445126e-05,
      "loss": 2.9545,
      "step": 1692
    },
    {
      "epoch": 0.15427373792600693,
      "grad_norm": 3.3373043537139893,
      "learning_rate": 4.7120774166225215e-05,
      "loss": 2.5542,
      "step": 1693
    },
    {
      "epoch": 0.1543648624020412,
      "grad_norm": 2.2917416095733643,
      "learning_rate": 4.7117438773120725e-05,
      "loss": 3.1463,
      "step": 1694
    },
    {
      "epoch": 0.15445598687807546,
      "grad_norm": 2.7962570190429688,
      "learning_rate": 4.7114101567405016e-05,
      "loss": 2.9269,
      "step": 1695
    },
    {
      "epoch": 0.1545471113541097,
      "grad_norm": 2.6020569801330566,
      "learning_rate": 4.7110762549351586e-05,
      "loss": 3.0507,
      "step": 1696
    },
    {
      "epoch": 0.15463823583014397,
      "grad_norm": 3.0041913986206055,
      "learning_rate": 4.7107421719234066e-05,
      "loss": 3.1105,
      "step": 1697
    },
    {
      "epoch": 0.15472936030617823,
      "grad_norm": 3.319488525390625,
      "learning_rate": 4.710407907732627e-05,
      "loss": 3.1954,
      "step": 1698
    },
    {
      "epoch": 0.1548204847822125,
      "grad_norm": 1.2503721714019775,
      "learning_rate": 4.7100734623902135e-05,
      "loss": 2.8883,
      "step": 1699
    },
    {
      "epoch": 0.15491160925824676,
      "grad_norm": 2.5423226356506348,
      "learning_rate": 4.709738835923575e-05,
      "loss": 3.2076,
      "step": 1700
    },
    {
      "epoch": 0.15500273373428103,
      "grad_norm": 2.350539445877075,
      "learning_rate": 4.7094040283601345e-05,
      "loss": 3.1558,
      "step": 1701
    },
    {
      "epoch": 0.1550938582103153,
      "grad_norm": 2.6543209552764893,
      "learning_rate": 4.709069039727332e-05,
      "loss": 3.2013,
      "step": 1702
    },
    {
      "epoch": 0.15518498268634956,
      "grad_norm": 2.5844523906707764,
      "learning_rate": 4.708733870052621e-05,
      "loss": 4.2824,
      "step": 1703
    },
    {
      "epoch": 0.15527610716238383,
      "grad_norm": 4.499124526977539,
      "learning_rate": 4.708398519363469e-05,
      "loss": 3.0362,
      "step": 1704
    },
    {
      "epoch": 0.1553672316384181,
      "grad_norm": 2.7563819885253906,
      "learning_rate": 4.70806298768736e-05,
      "loss": 3.3136,
      "step": 1705
    },
    {
      "epoch": 0.15545835611445233,
      "grad_norm": 2.212899923324585,
      "learning_rate": 4.707727275051793e-05,
      "loss": 2.9758,
      "step": 1706
    },
    {
      "epoch": 0.1555494805904866,
      "grad_norm": 2.452393054962158,
      "learning_rate": 4.70739138148428e-05,
      "loss": 3.3339,
      "step": 1707
    },
    {
      "epoch": 0.15564060506652086,
      "grad_norm": 3.724048614501953,
      "learning_rate": 4.7070553070123494e-05,
      "loss": 3.231,
      "step": 1708
    },
    {
      "epoch": 0.15573172954255513,
      "grad_norm": 2.489640235900879,
      "learning_rate": 4.706719051663543e-05,
      "loss": 3.2228,
      "step": 1709
    },
    {
      "epoch": 0.1558228540185894,
      "grad_norm": 1.4192156791687012,
      "learning_rate": 4.7063826154654175e-05,
      "loss": 3.0394,
      "step": 1710
    },
    {
      "epoch": 0.15591397849462366,
      "grad_norm": 1.8565047979354858,
      "learning_rate": 4.706045998445548e-05,
      "loss": 2.9931,
      "step": 1711
    },
    {
      "epoch": 0.15600510297065792,
      "grad_norm": 3.727804183959961,
      "learning_rate": 4.70570920063152e-05,
      "loss": 3.3733,
      "step": 1712
    },
    {
      "epoch": 0.1560962274466922,
      "grad_norm": 1.7734785079956055,
      "learning_rate": 4.705372222050934e-05,
      "loss": 3.134,
      "step": 1713
    },
    {
      "epoch": 0.15618735192272645,
      "grad_norm": 1.5476933717727661,
      "learning_rate": 4.705035062731409e-05,
      "loss": 3.0023,
      "step": 1714
    },
    {
      "epoch": 0.15627847639876072,
      "grad_norm": 1.2864508628845215,
      "learning_rate": 4.7046977227005754e-05,
      "loss": 2.832,
      "step": 1715
    },
    {
      "epoch": 0.15636960087479496,
      "grad_norm": 1.4140430688858032,
      "learning_rate": 4.704360201986079e-05,
      "loss": 2.9195,
      "step": 1716
    },
    {
      "epoch": 0.15646072535082922,
      "grad_norm": 1.4808127880096436,
      "learning_rate": 4.704022500615583e-05,
      "loss": 3.0438,
      "step": 1717
    },
    {
      "epoch": 0.1565518498268635,
      "grad_norm": 2.3502492904663086,
      "learning_rate": 4.7036846186167605e-05,
      "loss": 3.0985,
      "step": 1718
    },
    {
      "epoch": 0.15664297430289775,
      "grad_norm": 2.2176742553710938,
      "learning_rate": 4.703346556017305e-05,
      "loss": 3.1193,
      "step": 1719
    },
    {
      "epoch": 0.15673409877893202,
      "grad_norm": 2.9510931968688965,
      "learning_rate": 4.70300831284492e-05,
      "loss": 3.2911,
      "step": 1720
    },
    {
      "epoch": 0.15682522325496628,
      "grad_norm": 2.358860969543457,
      "learning_rate": 4.702669889127328e-05,
      "loss": 3.121,
      "step": 1721
    },
    {
      "epoch": 0.15691634773100055,
      "grad_norm": 3.136566638946533,
      "learning_rate": 4.702331284892262e-05,
      "loss": 3.0761,
      "step": 1722
    },
    {
      "epoch": 0.15700747220703481,
      "grad_norm": 1.5300287008285522,
      "learning_rate": 4.701992500167473e-05,
      "loss": 3.0094,
      "step": 1723
    },
    {
      "epoch": 0.15709859668306908,
      "grad_norm": 3.3138129711151123,
      "learning_rate": 4.701653534980724e-05,
      "loss": 3.2065,
      "step": 1724
    },
    {
      "epoch": 0.15718972115910335,
      "grad_norm": 2.763362407684326,
      "learning_rate": 4.7013143893597984e-05,
      "loss": 3.1027,
      "step": 1725
    },
    {
      "epoch": 0.1572808456351376,
      "grad_norm": 1.5856213569641113,
      "learning_rate": 4.700975063332487e-05,
      "loss": 3.029,
      "step": 1726
    },
    {
      "epoch": 0.15737197011117185,
      "grad_norm": 3.636809825897217,
      "learning_rate": 4.700635556926601e-05,
      "loss": 3.1847,
      "step": 1727
    },
    {
      "epoch": 0.15746309458720611,
      "grad_norm": 3.5086801052093506,
      "learning_rate": 4.7002958701699626e-05,
      "loss": 2.9192,
      "step": 1728
    },
    {
      "epoch": 0.15755421906324038,
      "grad_norm": 2.65805983543396,
      "learning_rate": 4.699956003090412e-05,
      "loss": 3.2344,
      "step": 1729
    },
    {
      "epoch": 0.15764534353927465,
      "grad_norm": 1.6364223957061768,
      "learning_rate": 4.6996159557158015e-05,
      "loss": 2.9734,
      "step": 1730
    },
    {
      "epoch": 0.1577364680153089,
      "grad_norm": 1.9043081998825073,
      "learning_rate": 4.6992757280739994e-05,
      "loss": 3.0671,
      "step": 1731
    },
    {
      "epoch": 0.15782759249134318,
      "grad_norm": 2.699431896209717,
      "learning_rate": 4.698935320192889e-05,
      "loss": 4.0291,
      "step": 1732
    },
    {
      "epoch": 0.15791871696737744,
      "grad_norm": 3.2991626262664795,
      "learning_rate": 4.698594732100369e-05,
      "loss": 2.9972,
      "step": 1733
    },
    {
      "epoch": 0.1580098414434117,
      "grad_norm": 2.3580875396728516,
      "learning_rate": 4.6982539638243506e-05,
      "loss": 2.992,
      "step": 1734
    },
    {
      "epoch": 0.15810096591944597,
      "grad_norm": 1.856777310371399,
      "learning_rate": 4.6979130153927605e-05,
      "loss": 3.1221,
      "step": 1735
    },
    {
      "epoch": 0.15819209039548024,
      "grad_norm": 2.132002592086792,
      "learning_rate": 4.697571886833544e-05,
      "loss": 3.1007,
      "step": 1736
    },
    {
      "epoch": 0.15828321487151448,
      "grad_norm": 1.3877090215682983,
      "learning_rate": 4.697230578174654e-05,
      "loss": 3.0206,
      "step": 1737
    },
    {
      "epoch": 0.15837433934754874,
      "grad_norm": 2.1733388900756836,
      "learning_rate": 4.6968890894440646e-05,
      "loss": 3.1006,
      "step": 1738
    },
    {
      "epoch": 0.158465463823583,
      "grad_norm": 2.9271535873413086,
      "learning_rate": 4.696547420669761e-05,
      "loss": 4.3027,
      "step": 1739
    },
    {
      "epoch": 0.15855658829961727,
      "grad_norm": 2.7141754627227783,
      "learning_rate": 4.696205571879745e-05,
      "loss": 4.056,
      "step": 1740
    },
    {
      "epoch": 0.15864771277565154,
      "grad_norm": 1.7694344520568848,
      "learning_rate": 4.6958635431020315e-05,
      "loss": 2.9097,
      "step": 1741
    },
    {
      "epoch": 0.1587388372516858,
      "grad_norm": 2.743277072906494,
      "learning_rate": 4.695521334364653e-05,
      "loss": 3.0041,
      "step": 1742
    },
    {
      "epoch": 0.15882996172772007,
      "grad_norm": 2.976694107055664,
      "learning_rate": 4.6951789456956524e-05,
      "loss": 3.1339,
      "step": 1743
    },
    {
      "epoch": 0.15892108620375434,
      "grad_norm": 2.281081199645996,
      "learning_rate": 4.6948363771230917e-05,
      "loss": 3.0526,
      "step": 1744
    },
    {
      "epoch": 0.1590122106797886,
      "grad_norm": 2.0678000450134277,
      "learning_rate": 4.694493628675044e-05,
      "loss": 3.2022,
      "step": 1745
    },
    {
      "epoch": 0.15910333515582287,
      "grad_norm": 1.403245210647583,
      "learning_rate": 4.694150700379601e-05,
      "loss": 2.8774,
      "step": 1746
    },
    {
      "epoch": 0.1591944596318571,
      "grad_norm": 2.7492146492004395,
      "learning_rate": 4.693807592264866e-05,
      "loss": 3.3178,
      "step": 1747
    },
    {
      "epoch": 0.15928558410789137,
      "grad_norm": 2.7067978382110596,
      "learning_rate": 4.693464304358957e-05,
      "loss": 3.1067,
      "step": 1748
    },
    {
      "epoch": 0.15937670858392564,
      "grad_norm": 3.026930093765259,
      "learning_rate": 4.693120836690009e-05,
      "loss": 2.6783,
      "step": 1749
    },
    {
      "epoch": 0.1594678330599599,
      "grad_norm": 2.294149398803711,
      "learning_rate": 4.6927771892861715e-05,
      "loss": 3.0602,
      "step": 1750
    },
    {
      "epoch": 0.15955895753599417,
      "grad_norm": 2.4239230155944824,
      "learning_rate": 4.6924333621756055e-05,
      "loss": 3.366,
      "step": 1751
    },
    {
      "epoch": 0.15965008201202843,
      "grad_norm": 1.500225305557251,
      "learning_rate": 4.692089355386491e-05,
      "loss": 3.0087,
      "step": 1752
    },
    {
      "epoch": 0.1597412064880627,
      "grad_norm": 3.9721415042877197,
      "learning_rate": 4.691745168947019e-05,
      "loss": 1.5708,
      "step": 1753
    },
    {
      "epoch": 0.15983233096409696,
      "grad_norm": 2.3244717121124268,
      "learning_rate": 4.6914008028853974e-05,
      "loss": 3.062,
      "step": 1754
    },
    {
      "epoch": 0.15992345544013123,
      "grad_norm": 3.399137258529663,
      "learning_rate": 4.6910562572298496e-05,
      "loss": 3.275,
      "step": 1755
    },
    {
      "epoch": 0.1600145799161655,
      "grad_norm": 1.5590107440948486,
      "learning_rate": 4.690711532008611e-05,
      "loss": 3.0246,
      "step": 1756
    },
    {
      "epoch": 0.16010570439219973,
      "grad_norm": 1.5847901105880737,
      "learning_rate": 4.690366627249934e-05,
      "loss": 3.0687,
      "step": 1757
    },
    {
      "epoch": 0.160196828868234,
      "grad_norm": 3.033726453781128,
      "learning_rate": 4.690021542982084e-05,
      "loss": 3.1466,
      "step": 1758
    },
    {
      "epoch": 0.16028795334426826,
      "grad_norm": 2.4416680335998535,
      "learning_rate": 4.689676279233344e-05,
      "loss": 3.0355,
      "step": 1759
    },
    {
      "epoch": 0.16037907782030253,
      "grad_norm": 3.199636459350586,
      "learning_rate": 4.689330836032007e-05,
      "loss": 2.9404,
      "step": 1760
    },
    {
      "epoch": 0.1604702022963368,
      "grad_norm": 2.1317837238311768,
      "learning_rate": 4.688985213406386e-05,
      "loss": 3.3088,
      "step": 1761
    },
    {
      "epoch": 0.16056132677237106,
      "grad_norm": 2.5305070877075195,
      "learning_rate": 4.6886394113848034e-05,
      "loss": 2.8364,
      "step": 1762
    },
    {
      "epoch": 0.16065245124840533,
      "grad_norm": 1.2650429010391235,
      "learning_rate": 4.6882934299956014e-05,
      "loss": 2.881,
      "step": 1763
    },
    {
      "epoch": 0.1607435757244396,
      "grad_norm": 3.0399343967437744,
      "learning_rate": 4.6879472692671344e-05,
      "loss": 2.9391,
      "step": 1764
    },
    {
      "epoch": 0.16083470020047386,
      "grad_norm": 1.4439702033996582,
      "learning_rate": 4.68760092922777e-05,
      "loss": 2.9775,
      "step": 1765
    },
    {
      "epoch": 0.16092582467650812,
      "grad_norm": 3.708564281463623,
      "learning_rate": 4.6872544099058934e-05,
      "loss": 2.7142,
      "step": 1766
    },
    {
      "epoch": 0.16101694915254236,
      "grad_norm": 2.397183656692505,
      "learning_rate": 4.686907711329903e-05,
      "loss": 3.2577,
      "step": 1767
    },
    {
      "epoch": 0.16110807362857663,
      "grad_norm": 5.58218240737915,
      "learning_rate": 4.686560833528213e-05,
      "loss": 2.4521,
      "step": 1768
    },
    {
      "epoch": 0.1611991981046109,
      "grad_norm": 1.9717990159988403,
      "learning_rate": 4.6862137765292493e-05,
      "loss": 3.0401,
      "step": 1769
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 2.144928455352783,
      "learning_rate": 4.685866540361456e-05,
      "loss": 3.0899,
      "step": 1770
    },
    {
      "epoch": 0.16138144705667942,
      "grad_norm": 2.322648286819458,
      "learning_rate": 4.685519125053289e-05,
      "loss": 3.0279,
      "step": 1771
    },
    {
      "epoch": 0.1614725715327137,
      "grad_norm": 1.8815804719924927,
      "learning_rate": 4.6851715306332235e-05,
      "loss": 3.0445,
      "step": 1772
    },
    {
      "epoch": 0.16156369600874795,
      "grad_norm": 3.7805604934692383,
      "learning_rate": 4.684823757129743e-05,
      "loss": 2.8023,
      "step": 1773
    },
    {
      "epoch": 0.16165482048478222,
      "grad_norm": 3.898134231567383,
      "learning_rate": 4.684475804571351e-05,
      "loss": 3.1171,
      "step": 1774
    },
    {
      "epoch": 0.16174594496081648,
      "grad_norm": 2.4973833560943604,
      "learning_rate": 4.684127672986562e-05,
      "loss": 3.3566,
      "step": 1775
    },
    {
      "epoch": 0.16183706943685075,
      "grad_norm": 2.69533371925354,
      "learning_rate": 4.683779362403908e-05,
      "loss": 3.1604,
      "step": 1776
    },
    {
      "epoch": 0.161928193912885,
      "grad_norm": 1.983508825302124,
      "learning_rate": 4.683430872851934e-05,
      "loss": 2.9452,
      "step": 1777
    },
    {
      "epoch": 0.16201931838891925,
      "grad_norm": 2.2573885917663574,
      "learning_rate": 4.6830822043591994e-05,
      "loss": 3.1903,
      "step": 1778
    },
    {
      "epoch": 0.16211044286495352,
      "grad_norm": 1.7813071012496948,
      "learning_rate": 4.68273335695428e-05,
      "loss": 3.0245,
      "step": 1779
    },
    {
      "epoch": 0.16220156734098778,
      "grad_norm": 2.4912261962890625,
      "learning_rate": 4.682384330665765e-05,
      "loss": 3.1379,
      "step": 1780
    },
    {
      "epoch": 0.16229269181702205,
      "grad_norm": 2.328861713409424,
      "learning_rate": 4.682035125522258e-05,
      "loss": 3.1379,
      "step": 1781
    },
    {
      "epoch": 0.16238381629305632,
      "grad_norm": 1.43288254737854,
      "learning_rate": 4.681685741552379e-05,
      "loss": 2.959,
      "step": 1782
    },
    {
      "epoch": 0.16247494076909058,
      "grad_norm": 3.625856876373291,
      "learning_rate": 4.6813361787847585e-05,
      "loss": 3.088,
      "step": 1783
    },
    {
      "epoch": 0.16256606524512485,
      "grad_norm": 2.959545612335205,
      "learning_rate": 4.680986437248048e-05,
      "loss": 3.1953,
      "step": 1784
    },
    {
      "epoch": 0.1626571897211591,
      "grad_norm": 4.424033164978027,
      "learning_rate": 4.680636516970908e-05,
      "loss": 3.2453,
      "step": 1785
    },
    {
      "epoch": 0.16274831419719338,
      "grad_norm": 2.5210375785827637,
      "learning_rate": 4.680286417982017e-05,
      "loss": 3.127,
      "step": 1786
    },
    {
      "epoch": 0.16283943867322762,
      "grad_norm": 2.69915509223938,
      "learning_rate": 4.679936140310066e-05,
      "loss": 2.9651,
      "step": 1787
    },
    {
      "epoch": 0.16293056314926188,
      "grad_norm": 2.2764296531677246,
      "learning_rate": 4.679585683983763e-05,
      "loss": 2.4394,
      "step": 1788
    },
    {
      "epoch": 0.16302168762529615,
      "grad_norm": 2.4969351291656494,
      "learning_rate": 4.679235049031827e-05,
      "loss": 3.1952,
      "step": 1789
    },
    {
      "epoch": 0.1631128121013304,
      "grad_norm": 2.099569797515869,
      "learning_rate": 4.6788842354829965e-05,
      "loss": 2.9982,
      "step": 1790
    },
    {
      "epoch": 0.16320393657736468,
      "grad_norm": 2.138705015182495,
      "learning_rate": 4.67853324336602e-05,
      "loss": 2.8346,
      "step": 1791
    },
    {
      "epoch": 0.16329506105339894,
      "grad_norm": 3.4511311054229736,
      "learning_rate": 4.6781820727096634e-05,
      "loss": 2.9689,
      "step": 1792
    },
    {
      "epoch": 0.1633861855294332,
      "grad_norm": 1.415783166885376,
      "learning_rate": 4.677830723542708e-05,
      "loss": 2.9163,
      "step": 1793
    },
    {
      "epoch": 0.16347731000546747,
      "grad_norm": 1.5046038627624512,
      "learning_rate": 4.677479195893946e-05,
      "loss": 3.024,
      "step": 1794
    },
    {
      "epoch": 0.16356843448150174,
      "grad_norm": 1.4293829202651978,
      "learning_rate": 4.677127489792188e-05,
      "loss": 3.0218,
      "step": 1795
    },
    {
      "epoch": 0.163659558957536,
      "grad_norm": 2.071714401245117,
      "learning_rate": 4.676775605266256e-05,
      "loss": 3.092,
      "step": 1796
    },
    {
      "epoch": 0.16375068343357024,
      "grad_norm": 2.291851043701172,
      "learning_rate": 4.676423542344991e-05,
      "loss": 2.9331,
      "step": 1797
    },
    {
      "epoch": 0.1638418079096045,
      "grad_norm": 2.606294870376587,
      "learning_rate": 4.676071301057243e-05,
      "loss": 3.2264,
      "step": 1798
    },
    {
      "epoch": 0.16393293238563877,
      "grad_norm": 2.889845848083496,
      "learning_rate": 4.675718881431882e-05,
      "loss": 3.1705,
      "step": 1799
    },
    {
      "epoch": 0.16402405686167304,
      "grad_norm": 3.2519989013671875,
      "learning_rate": 4.675366283497788e-05,
      "loss": 4.5846,
      "step": 1800
    },
    {
      "epoch": 0.1641151813377073,
      "grad_norm": 2.266395092010498,
      "learning_rate": 4.67501350728386e-05,
      "loss": 3.4063,
      "step": 1801
    },
    {
      "epoch": 0.16420630581374157,
      "grad_norm": 2.0074515342712402,
      "learning_rate": 4.674660552819007e-05,
      "loss": 3.1425,
      "step": 1802
    },
    {
      "epoch": 0.16429743028977584,
      "grad_norm": 5.168177604675293,
      "learning_rate": 4.6743074201321577e-05,
      "loss": 3.0502,
      "step": 1803
    },
    {
      "epoch": 0.1643885547658101,
      "grad_norm": 1.329056978225708,
      "learning_rate": 4.673954109252251e-05,
      "loss": 2.9967,
      "step": 1804
    },
    {
      "epoch": 0.16447967924184437,
      "grad_norm": 3.3117220401763916,
      "learning_rate": 4.6736006202082414e-05,
      "loss": 3.1037,
      "step": 1805
    },
    {
      "epoch": 0.16457080371787863,
      "grad_norm": 2.464008331298828,
      "learning_rate": 4.6732469530291e-05,
      "loss": 2.9688,
      "step": 1806
    },
    {
      "epoch": 0.1646619281939129,
      "grad_norm": 2.8803305625915527,
      "learning_rate": 4.672893107743812e-05,
      "loss": 3.1368,
      "step": 1807
    },
    {
      "epoch": 0.16475305266994714,
      "grad_norm": 2.8774044513702393,
      "learning_rate": 4.672539084381375e-05,
      "loss": 3.0782,
      "step": 1808
    },
    {
      "epoch": 0.1648441771459814,
      "grad_norm": 2.2963814735412598,
      "learning_rate": 4.672184882970803e-05,
      "loss": 2.9884,
      "step": 1809
    },
    {
      "epoch": 0.16493530162201567,
      "grad_norm": 2.4225456714630127,
      "learning_rate": 4.671830503541124e-05,
      "loss": 3.3793,
      "step": 1810
    },
    {
      "epoch": 0.16502642609804993,
      "grad_norm": 1.4230417013168335,
      "learning_rate": 4.671475946121381e-05,
      "loss": 3.1628,
      "step": 1811
    },
    {
      "epoch": 0.1651175505740842,
      "grad_norm": 2.5980818271636963,
      "learning_rate": 4.671121210740631e-05,
      "loss": 3.2377,
      "step": 1812
    },
    {
      "epoch": 0.16520867505011846,
      "grad_norm": 1.8780187368392944,
      "learning_rate": 4.6707662974279464e-05,
      "loss": 2.9965,
      "step": 1813
    },
    {
      "epoch": 0.16529979952615273,
      "grad_norm": 1.3507145643234253,
      "learning_rate": 4.6704112062124146e-05,
      "loss": 2.8605,
      "step": 1814
    },
    {
      "epoch": 0.165390924002187,
      "grad_norm": 2.861724376678467,
      "learning_rate": 4.6700559371231345e-05,
      "loss": 2.9891,
      "step": 1815
    },
    {
      "epoch": 0.16548204847822126,
      "grad_norm": 1.8451899290084839,
      "learning_rate": 4.6697004901892244e-05,
      "loss": 3.0085,
      "step": 1816
    },
    {
      "epoch": 0.16557317295425553,
      "grad_norm": 2.6136531829833984,
      "learning_rate": 4.6693448654398126e-05,
      "loss": 3.164,
      "step": 1817
    },
    {
      "epoch": 0.16566429743028976,
      "grad_norm": 2.2334072589874268,
      "learning_rate": 4.668989062904045e-05,
      "loss": 3.0821,
      "step": 1818
    },
    {
      "epoch": 0.16575542190632403,
      "grad_norm": 2.377607583999634,
      "learning_rate": 4.66863308261108e-05,
      "loss": 2.9773,
      "step": 1819
    },
    {
      "epoch": 0.1658465463823583,
      "grad_norm": 2.5205888748168945,
      "learning_rate": 4.6682769245900924e-05,
      "loss": 3.0648,
      "step": 1820
    },
    {
      "epoch": 0.16593767085839256,
      "grad_norm": 1.433559536933899,
      "learning_rate": 4.667920588870271e-05,
      "loss": 2.9843,
      "step": 1821
    },
    {
      "epoch": 0.16602879533442683,
      "grad_norm": 2.934572458267212,
      "learning_rate": 4.667564075480818e-05,
      "loss": 2.8275,
      "step": 1822
    },
    {
      "epoch": 0.1661199198104611,
      "grad_norm": 4.768115997314453,
      "learning_rate": 4.6672073844509524e-05,
      "loss": 2.8542,
      "step": 1823
    },
    {
      "epoch": 0.16621104428649536,
      "grad_norm": 2.2473628520965576,
      "learning_rate": 4.666850515809905e-05,
      "loss": 2.9707,
      "step": 1824
    },
    {
      "epoch": 0.16630216876252962,
      "grad_norm": 2.0815329551696777,
      "learning_rate": 4.6664934695869226e-05,
      "loss": 3.1296,
      "step": 1825
    },
    {
      "epoch": 0.1663932932385639,
      "grad_norm": 2.7739152908325195,
      "learning_rate": 4.666136245811268e-05,
      "loss": 1.463,
      "step": 1826
    },
    {
      "epoch": 0.16648441771459815,
      "grad_norm": 1.61172616481781,
      "learning_rate": 4.6657788445122156e-05,
      "loss": 3.1061,
      "step": 1827
    },
    {
      "epoch": 0.1665755421906324,
      "grad_norm": 4.994141101837158,
      "learning_rate": 4.6654212657190574e-05,
      "loss": 3.2571,
      "step": 1828
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.818001627922058,
      "learning_rate": 4.665063509461097e-05,
      "loss": 3.0249,
      "step": 1829
    },
    {
      "epoch": 0.16675779114270092,
      "grad_norm": 2.999178647994995,
      "learning_rate": 4.664705575767654e-05,
      "loss": 4.4251,
      "step": 1830
    },
    {
      "epoch": 0.1668489156187352,
      "grad_norm": 2.0437114238739014,
      "learning_rate": 4.6643474646680636e-05,
      "loss": 3.1958,
      "step": 1831
    },
    {
      "epoch": 0.16694004009476945,
      "grad_norm": 2.0116827487945557,
      "learning_rate": 4.663989176191673e-05,
      "loss": 2.7428,
      "step": 1832
    },
    {
      "epoch": 0.16703116457080372,
      "grad_norm": 1.6298675537109375,
      "learning_rate": 4.6636307103678464e-05,
      "loss": 2.9703,
      "step": 1833
    },
    {
      "epoch": 0.16712228904683799,
      "grad_norm": 2.848588466644287,
      "learning_rate": 4.663272067225961e-05,
      "loss": 3.0825,
      "step": 1834
    },
    {
      "epoch": 0.16721341352287225,
      "grad_norm": 1.2318345308303833,
      "learning_rate": 4.66291324679541e-05,
      "loss": 2.8645,
      "step": 1835
    },
    {
      "epoch": 0.16730453799890652,
      "grad_norm": 3.15191388130188,
      "learning_rate": 4.6625542491055985e-05,
      "loss": 3.2674,
      "step": 1836
    },
    {
      "epoch": 0.16739566247494078,
      "grad_norm": 1.4416121244430542,
      "learning_rate": 4.662195074185949e-05,
      "loss": 2.9721,
      "step": 1837
    },
    {
      "epoch": 0.16748678695097502,
      "grad_norm": 1.4033936262130737,
      "learning_rate": 4.661835722065896e-05,
      "loss": 2.8413,
      "step": 1838
    },
    {
      "epoch": 0.16757791142700929,
      "grad_norm": 3.393137216567993,
      "learning_rate": 4.661476192774892e-05,
      "loss": 3.255,
      "step": 1839
    },
    {
      "epoch": 0.16766903590304355,
      "grad_norm": 1.4797853231430054,
      "learning_rate": 4.6611164863424e-05,
      "loss": 2.9213,
      "step": 1840
    },
    {
      "epoch": 0.16776016037907782,
      "grad_norm": 1.99228835105896,
      "learning_rate": 4.660756602797899e-05,
      "loss": 3.0292,
      "step": 1841
    },
    {
      "epoch": 0.16785128485511208,
      "grad_norm": 3.2348062992095947,
      "learning_rate": 4.6603965421708845e-05,
      "loss": 3.0822,
      "step": 1842
    },
    {
      "epoch": 0.16794240933114635,
      "grad_norm": 1.782169222831726,
      "learning_rate": 4.660036304490864e-05,
      "loss": 2.9511,
      "step": 1843
    },
    {
      "epoch": 0.1680335338071806,
      "grad_norm": 4.648063659667969,
      "learning_rate": 4.6596758897873605e-05,
      "loss": 3.0001,
      "step": 1844
    },
    {
      "epoch": 0.16812465828321488,
      "grad_norm": 2.3051891326904297,
      "learning_rate": 4.659315298089912e-05,
      "loss": 2.6815,
      "step": 1845
    },
    {
      "epoch": 0.16821578275924914,
      "grad_norm": 3.1102521419525146,
      "learning_rate": 4.6589545294280694e-05,
      "loss": 3.291,
      "step": 1846
    },
    {
      "epoch": 0.1683069072352834,
      "grad_norm": 2.6495754718780518,
      "learning_rate": 4.6585935838313996e-05,
      "loss": 3.3111,
      "step": 1847
    },
    {
      "epoch": 0.16839803171131765,
      "grad_norm": 1.6148706674575806,
      "learning_rate": 4.658232461329484e-05,
      "loss": 3.0352,
      "step": 1848
    },
    {
      "epoch": 0.1684891561873519,
      "grad_norm": 2.024562120437622,
      "learning_rate": 4.657871161951917e-05,
      "loss": 3.2065,
      "step": 1849
    },
    {
      "epoch": 0.16858028066338618,
      "grad_norm": 3.0572400093078613,
      "learning_rate": 4.657509685728309e-05,
      "loss": 2.8895,
      "step": 1850
    },
    {
      "epoch": 0.16867140513942044,
      "grad_norm": 3.482790946960449,
      "learning_rate": 4.657148032688285e-05,
      "loss": 3.1904,
      "step": 1851
    },
    {
      "epoch": 0.1687625296154547,
      "grad_norm": 2.436032772064209,
      "learning_rate": 4.656786202861483e-05,
      "loss": 2.8036,
      "step": 1852
    },
    {
      "epoch": 0.16885365409148897,
      "grad_norm": 3.0358383655548096,
      "learning_rate": 4.6564241962775564e-05,
      "loss": 3.1313,
      "step": 1853
    },
    {
      "epoch": 0.16894477856752324,
      "grad_norm": 2.4795777797698975,
      "learning_rate": 4.656062012966173e-05,
      "loss": 3.2146,
      "step": 1854
    },
    {
      "epoch": 0.1690359030435575,
      "grad_norm": 1.5819923877716064,
      "learning_rate": 4.655699652957016e-05,
      "loss": 3.0148,
      "step": 1855
    },
    {
      "epoch": 0.16912702751959177,
      "grad_norm": 1.9436419010162354,
      "learning_rate": 4.655337116279782e-05,
      "loss": 2.9958,
      "step": 1856
    },
    {
      "epoch": 0.16921815199562604,
      "grad_norm": 2.404379367828369,
      "learning_rate": 4.6549744029641816e-05,
      "loss": 2.6195,
      "step": 1857
    },
    {
      "epoch": 0.16930927647166027,
      "grad_norm": 3.0540213584899902,
      "learning_rate": 4.6546115130399414e-05,
      "loss": 2.9237,
      "step": 1858
    },
    {
      "epoch": 0.16940040094769454,
      "grad_norm": 1.2016074657440186,
      "learning_rate": 4.6542484465368006e-05,
      "loss": 3.0172,
      "step": 1859
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 2.7861342430114746,
      "learning_rate": 4.653885203484515e-05,
      "loss": 4.522,
      "step": 1860
    },
    {
      "epoch": 0.16958264989976307,
      "grad_norm": 2.5136163234710693,
      "learning_rate": 4.6535217839128545e-05,
      "loss": 3.2934,
      "step": 1861
    },
    {
      "epoch": 0.16967377437579734,
      "grad_norm": 1.917813777923584,
      "learning_rate": 4.6531581878516005e-05,
      "loss": 2.982,
      "step": 1862
    },
    {
      "epoch": 0.1697648988518316,
      "grad_norm": 2.3996527194976807,
      "learning_rate": 4.652794415330552e-05,
      "loss": 3.2438,
      "step": 1863
    },
    {
      "epoch": 0.16985602332786587,
      "grad_norm": 2.1027286052703857,
      "learning_rate": 4.652430466379523e-05,
      "loss": 2.9588,
      "step": 1864
    },
    {
      "epoch": 0.16994714780390013,
      "grad_norm": 1.443307876586914,
      "learning_rate": 4.652066341028338e-05,
      "loss": 2.993,
      "step": 1865
    },
    {
      "epoch": 0.1700382722799344,
      "grad_norm": 2.603544235229492,
      "learning_rate": 4.6517020393068414e-05,
      "loss": 3.099,
      "step": 1866
    },
    {
      "epoch": 0.17012939675596866,
      "grad_norm": 2.022552728652954,
      "learning_rate": 4.651337561244887e-05,
      "loss": 3.4147,
      "step": 1867
    },
    {
      "epoch": 0.1702205212320029,
      "grad_norm": 3.2061586380004883,
      "learning_rate": 4.650972906872346e-05,
      "loss": 3.2699,
      "step": 1868
    },
    {
      "epoch": 0.17031164570803717,
      "grad_norm": 1.5013995170593262,
      "learning_rate": 4.650608076219103e-05,
      "loss": 2.9541,
      "step": 1869
    },
    {
      "epoch": 0.17040277018407143,
      "grad_norm": 1.4068304300308228,
      "learning_rate": 4.650243069315058e-05,
      "loss": 2.9621,
      "step": 1870
    },
    {
      "epoch": 0.1704938946601057,
      "grad_norm": 1.5417219400405884,
      "learning_rate": 4.649877886190124e-05,
      "loss": 2.9637,
      "step": 1871
    },
    {
      "epoch": 0.17058501913613996,
      "grad_norm": 1.9698114395141602,
      "learning_rate": 4.649512526874229e-05,
      "loss": 2.9846,
      "step": 1872
    },
    {
      "epoch": 0.17067614361217423,
      "grad_norm": 2.276402473449707,
      "learning_rate": 4.649146991397317e-05,
      "loss": 3.2274,
      "step": 1873
    },
    {
      "epoch": 0.1707672680882085,
      "grad_norm": 1.367923617362976,
      "learning_rate": 4.648781279789344e-05,
      "loss": 2.9497,
      "step": 1874
    },
    {
      "epoch": 0.17085839256424276,
      "grad_norm": 2.30353045463562,
      "learning_rate": 4.648415392080281e-05,
      "loss": 2.8962,
      "step": 1875
    },
    {
      "epoch": 0.17094951704027703,
      "grad_norm": 2.0635387897491455,
      "learning_rate": 4.6480493283001145e-05,
      "loss": 3.2399,
      "step": 1876
    },
    {
      "epoch": 0.1710406415163113,
      "grad_norm": 1.7822022438049316,
      "learning_rate": 4.6476830884788456e-05,
      "loss": 2.6609,
      "step": 1877
    },
    {
      "epoch": 0.17113176599234553,
      "grad_norm": 1.4074290990829468,
      "learning_rate": 4.647316672646488e-05,
      "loss": 3.0442,
      "step": 1878
    },
    {
      "epoch": 0.1712228904683798,
      "grad_norm": 1.7174943685531616,
      "learning_rate": 4.6469500808330724e-05,
      "loss": 3.3071,
      "step": 1879
    },
    {
      "epoch": 0.17131401494441406,
      "grad_norm": 2.6318490505218506,
      "learning_rate": 4.6465833130686405e-05,
      "loss": 3.231,
      "step": 1880
    },
    {
      "epoch": 0.17140513942044833,
      "grad_norm": 2.40649676322937,
      "learning_rate": 4.646216369383252e-05,
      "loss": 3.1019,
      "step": 1881
    },
    {
      "epoch": 0.1714962638964826,
      "grad_norm": 2.1710281372070312,
      "learning_rate": 4.645849249806977e-05,
      "loss": 2.9026,
      "step": 1882
    },
    {
      "epoch": 0.17158738837251686,
      "grad_norm": 3.329578161239624,
      "learning_rate": 4.645481954369906e-05,
      "loss": 4.2214,
      "step": 1883
    },
    {
      "epoch": 0.17167851284855112,
      "grad_norm": 2.663468360900879,
      "learning_rate": 4.6451144831021375e-05,
      "loss": 3.0867,
      "step": 1884
    },
    {
      "epoch": 0.1717696373245854,
      "grad_norm": 3.370527982711792,
      "learning_rate": 4.6447468360337876e-05,
      "loss": 3.0162,
      "step": 1885
    },
    {
      "epoch": 0.17186076180061965,
      "grad_norm": 1.3907276391983032,
      "learning_rate": 4.6443790131949874e-05,
      "loss": 2.9264,
      "step": 1886
    },
    {
      "epoch": 0.17195188627665392,
      "grad_norm": 3.615497350692749,
      "learning_rate": 4.644011014615881e-05,
      "loss": 3.0735,
      "step": 1887
    },
    {
      "epoch": 0.17204301075268819,
      "grad_norm": 1.8801993131637573,
      "learning_rate": 4.643642840326627e-05,
      "loss": 3.1026,
      "step": 1888
    },
    {
      "epoch": 0.17213413522872242,
      "grad_norm": 1.2994283437728882,
      "learning_rate": 4.6432744903573996e-05,
      "loss": 2.9899,
      "step": 1889
    },
    {
      "epoch": 0.1722252597047567,
      "grad_norm": 3.113603115081787,
      "learning_rate": 4.6429059647383867e-05,
      "loss": 2.4245,
      "step": 1890
    },
    {
      "epoch": 0.17231638418079095,
      "grad_norm": 1.4192283153533936,
      "learning_rate": 4.642537263499788e-05,
      "loss": 3.0991,
      "step": 1891
    },
    {
      "epoch": 0.17240750865682522,
      "grad_norm": 1.6561801433563232,
      "learning_rate": 4.642168386671823e-05,
      "loss": 3.0053,
      "step": 1892
    },
    {
      "epoch": 0.17249863313285949,
      "grad_norm": 2.518643617630005,
      "learning_rate": 4.64179933428472e-05,
      "loss": 3.0779,
      "step": 1893
    },
    {
      "epoch": 0.17258975760889375,
      "grad_norm": 1.9463084936141968,
      "learning_rate": 4.641430106368726e-05,
      "loss": 2.9763,
      "step": 1894
    },
    {
      "epoch": 0.17268088208492802,
      "grad_norm": 2.9101171493530273,
      "learning_rate": 4.641060702954101e-05,
      "loss": 3.1603,
      "step": 1895
    },
    {
      "epoch": 0.17277200656096228,
      "grad_norm": 2.2001545429229736,
      "learning_rate": 4.640691124071118e-05,
      "loss": 2.7663,
      "step": 1896
    },
    {
      "epoch": 0.17286313103699655,
      "grad_norm": 2.230363130569458,
      "learning_rate": 4.6403213697500656e-05,
      "loss": 2.4912,
      "step": 1897
    },
    {
      "epoch": 0.1729542555130308,
      "grad_norm": 2.127821445465088,
      "learning_rate": 4.639951440021247e-05,
      "loss": 3.0039,
      "step": 1898
    },
    {
      "epoch": 0.17304537998906505,
      "grad_norm": 2.014512538909912,
      "learning_rate": 4.639581334914979e-05,
      "loss": 3.266,
      "step": 1899
    },
    {
      "epoch": 0.17313650446509932,
      "grad_norm": 2.769667625427246,
      "learning_rate": 4.639211054461593e-05,
      "loss": 3.225,
      "step": 1900
    },
    {
      "epoch": 0.17322762894113358,
      "grad_norm": 1.3063126802444458,
      "learning_rate": 4.6388405986914365e-05,
      "loss": 2.9487,
      "step": 1901
    },
    {
      "epoch": 0.17331875341716785,
      "grad_norm": 2.091466188430786,
      "learning_rate": 4.6384699676348674e-05,
      "loss": 3.1492,
      "step": 1902
    },
    {
      "epoch": 0.1734098778932021,
      "grad_norm": 2.709743022918701,
      "learning_rate": 4.6380991613222625e-05,
      "loss": 3.2338,
      "step": 1903
    },
    {
      "epoch": 0.17350100236923638,
      "grad_norm": 2.078986406326294,
      "learning_rate": 4.637728179784009e-05,
      "loss": 3.4682,
      "step": 1904
    },
    {
      "epoch": 0.17359212684527064,
      "grad_norm": 2.516191005706787,
      "learning_rate": 4.637357023050512e-05,
      "loss": 3.0394,
      "step": 1905
    },
    {
      "epoch": 0.1736832513213049,
      "grad_norm": 3.148676872253418,
      "learning_rate": 4.636985691152188e-05,
      "loss": 3.1488,
      "step": 1906
    },
    {
      "epoch": 0.17377437579733918,
      "grad_norm": 3.0405311584472656,
      "learning_rate": 4.63661418411947e-05,
      "loss": 3.0929,
      "step": 1907
    },
    {
      "epoch": 0.17386550027337344,
      "grad_norm": 1.5074187517166138,
      "learning_rate": 4.6362425019828035e-05,
      "loss": 2.9689,
      "step": 1908
    },
    {
      "epoch": 0.17395662474940768,
      "grad_norm": 1.9838029146194458,
      "learning_rate": 4.635870644772651e-05,
      "loss": 2.8708,
      "step": 1909
    },
    {
      "epoch": 0.17404774922544194,
      "grad_norm": 1.4794012308120728,
      "learning_rate": 4.635498612519486e-05,
      "loss": 3.1818,
      "step": 1910
    },
    {
      "epoch": 0.1741388737014762,
      "grad_norm": 1.3626906871795654,
      "learning_rate": 4.6351264052537984e-05,
      "loss": 2.8804,
      "step": 1911
    },
    {
      "epoch": 0.17422999817751048,
      "grad_norm": 1.694874882698059,
      "learning_rate": 4.6347540230060924e-05,
      "loss": 3.1085,
      "step": 1912
    },
    {
      "epoch": 0.17432112265354474,
      "grad_norm": 3.162107467651367,
      "learning_rate": 4.634381465806886e-05,
      "loss": 3.2248,
      "step": 1913
    },
    {
      "epoch": 0.174412247129579,
      "grad_norm": 1.7732110023498535,
      "learning_rate": 4.6340087336867115e-05,
      "loss": 3.015,
      "step": 1914
    },
    {
      "epoch": 0.17450337160561327,
      "grad_norm": 2.3251988887786865,
      "learning_rate": 4.633635826676116e-05,
      "loss": 2.833,
      "step": 1915
    },
    {
      "epoch": 0.17459449608164754,
      "grad_norm": 2.435624599456787,
      "learning_rate": 4.633262744805661e-05,
      "loss": 3.0755,
      "step": 1916
    },
    {
      "epoch": 0.1746856205576818,
      "grad_norm": 3.7790379524230957,
      "learning_rate": 4.6328894881059216e-05,
      "loss": 3.1371,
      "step": 1917
    },
    {
      "epoch": 0.17477674503371607,
      "grad_norm": 5.323071002960205,
      "learning_rate": 4.6325160566074875e-05,
      "loss": 2.9746,
      "step": 1918
    },
    {
      "epoch": 0.1748678695097503,
      "grad_norm": 3.0934462547302246,
      "learning_rate": 4.632142450340964e-05,
      "loss": 3.2001,
      "step": 1919
    },
    {
      "epoch": 0.17495899398578457,
      "grad_norm": 1.9771567583084106,
      "learning_rate": 4.631768669336968e-05,
      "loss": 3.1897,
      "step": 1920
    },
    {
      "epoch": 0.17505011846181884,
      "grad_norm": 4.835346698760986,
      "learning_rate": 4.631394713626133e-05,
      "loss": 3.0367,
      "step": 1921
    },
    {
      "epoch": 0.1751412429378531,
      "grad_norm": 2.225339889526367,
      "learning_rate": 4.631020583239107e-05,
      "loss": 2.8157,
      "step": 1922
    },
    {
      "epoch": 0.17523236741388737,
      "grad_norm": 3.113792657852173,
      "learning_rate": 4.63064627820655e-05,
      "loss": 2.4321,
      "step": 1923
    },
    {
      "epoch": 0.17532349188992163,
      "grad_norm": 1.4210890531539917,
      "learning_rate": 4.630271798559138e-05,
      "loss": 3.0678,
      "step": 1924
    },
    {
      "epoch": 0.1754146163659559,
      "grad_norm": 1.9921435117721558,
      "learning_rate": 4.629897144327563e-05,
      "loss": 3.067,
      "step": 1925
    },
    {
      "epoch": 0.17550574084199017,
      "grad_norm": 2.643051862716675,
      "learning_rate": 4.6295223155425274e-05,
      "loss": 2.9527,
      "step": 1926
    },
    {
      "epoch": 0.17559686531802443,
      "grad_norm": 1.9968456029891968,
      "learning_rate": 4.6291473122347494e-05,
      "loss": 2.8901,
      "step": 1927
    },
    {
      "epoch": 0.1756879897940587,
      "grad_norm": 2.101381301879883,
      "learning_rate": 4.628772134434964e-05,
      "loss": 2.988,
      "step": 1928
    },
    {
      "epoch": 0.17577911427009293,
      "grad_norm": 3.2429404258728027,
      "learning_rate": 4.628396782173918e-05,
      "loss": 3.1101,
      "step": 1929
    },
    {
      "epoch": 0.1758702387461272,
      "grad_norm": 2.9418232440948486,
      "learning_rate": 4.6280212554823715e-05,
      "loss": 3.2891,
      "step": 1930
    },
    {
      "epoch": 0.17596136322216147,
      "grad_norm": 1.3050503730773926,
      "learning_rate": 4.6276455543911026e-05,
      "loss": 2.9632,
      "step": 1931
    },
    {
      "epoch": 0.17605248769819573,
      "grad_norm": 1.7188957929611206,
      "learning_rate": 4.627269678930899e-05,
      "loss": 3.0123,
      "step": 1932
    },
    {
      "epoch": 0.17614361217423,
      "grad_norm": 3.276196002960205,
      "learning_rate": 4.626893629132567e-05,
      "loss": 3.3355,
      "step": 1933
    },
    {
      "epoch": 0.17623473665026426,
      "grad_norm": 2.130753517150879,
      "learning_rate": 4.6265174050269245e-05,
      "loss": 3.1184,
      "step": 1934
    },
    {
      "epoch": 0.17632586112629853,
      "grad_norm": 3.7487149238586426,
      "learning_rate": 4.626141006644805e-05,
      "loss": 3.2589,
      "step": 1935
    },
    {
      "epoch": 0.1764169856023328,
      "grad_norm": 1.4219084978103638,
      "learning_rate": 4.625764434017056e-05,
      "loss": 2.9983,
      "step": 1936
    },
    {
      "epoch": 0.17650811007836706,
      "grad_norm": 2.6864495277404785,
      "learning_rate": 4.625387687174539e-05,
      "loss": 3.1346,
      "step": 1937
    },
    {
      "epoch": 0.17659923455440132,
      "grad_norm": 1.8427038192749023,
      "learning_rate": 4.62501076614813e-05,
      "loss": 2.7725,
      "step": 1938
    },
    {
      "epoch": 0.17669035903043556,
      "grad_norm": 3.0085763931274414,
      "learning_rate": 4.624633670968718e-05,
      "loss": 4.6207,
      "step": 1939
    },
    {
      "epoch": 0.17678148350646983,
      "grad_norm": 2.097071647644043,
      "learning_rate": 4.6242564016672094e-05,
      "loss": 3.2445,
      "step": 1940
    },
    {
      "epoch": 0.1768726079825041,
      "grad_norm": 2.8107852935791016,
      "learning_rate": 4.6238789582745215e-05,
      "loss": 3.3332,
      "step": 1941
    },
    {
      "epoch": 0.17696373245853836,
      "grad_norm": 1.6054763793945312,
      "learning_rate": 4.623501340821586e-05,
      "loss": 3.0692,
      "step": 1942
    },
    {
      "epoch": 0.17705485693457262,
      "grad_norm": 2.5180320739746094,
      "learning_rate": 4.6231235493393535e-05,
      "loss": 3.1702,
      "step": 1943
    },
    {
      "epoch": 0.1771459814106069,
      "grad_norm": 2.668308734893799,
      "learning_rate": 4.6227455838587827e-05,
      "loss": 4.3692,
      "step": 1944
    },
    {
      "epoch": 0.17723710588664116,
      "grad_norm": 3.261211633682251,
      "learning_rate": 4.6223674444108514e-05,
      "loss": 3.1087,
      "step": 1945
    },
    {
      "epoch": 0.17732823036267542,
      "grad_norm": 2.2094995975494385,
      "learning_rate": 4.621989131026548e-05,
      "loss": 3.2398,
      "step": 1946
    },
    {
      "epoch": 0.1774193548387097,
      "grad_norm": 2.806913375854492,
      "learning_rate": 4.621610643736878e-05,
      "loss": 3.2686,
      "step": 1947
    },
    {
      "epoch": 0.17751047931474395,
      "grad_norm": 3.777822494506836,
      "learning_rate": 4.621231982572858e-05,
      "loss": 2.7723,
      "step": 1948
    },
    {
      "epoch": 0.1776016037907782,
      "grad_norm": 2.3020384311676025,
      "learning_rate": 4.6208531475655236e-05,
      "loss": 2.8918,
      "step": 1949
    },
    {
      "epoch": 0.17769272826681246,
      "grad_norm": 3.0134787559509277,
      "learning_rate": 4.6204741387459196e-05,
      "loss": 3.2594,
      "step": 1950
    },
    {
      "epoch": 0.17778385274284672,
      "grad_norm": 3.5491254329681396,
      "learning_rate": 4.620094956145108e-05,
      "loss": 3.1717,
      "step": 1951
    },
    {
      "epoch": 0.177874977218881,
      "grad_norm": 1.3344693183898926,
      "learning_rate": 4.619715599794164e-05,
      "loss": 3.0054,
      "step": 1952
    },
    {
      "epoch": 0.17796610169491525,
      "grad_norm": 1.2647558450698853,
      "learning_rate": 4.619336069724178e-05,
      "loss": 2.9496,
      "step": 1953
    },
    {
      "epoch": 0.17805722617094952,
      "grad_norm": 1.373024821281433,
      "learning_rate": 4.6189563659662525e-05,
      "loss": 2.9396,
      "step": 1954
    },
    {
      "epoch": 0.17814835064698378,
      "grad_norm": 1.3420569896697998,
      "learning_rate": 4.618576488551508e-05,
      "loss": 2.9063,
      "step": 1955
    },
    {
      "epoch": 0.17823947512301805,
      "grad_norm": 1.575454592704773,
      "learning_rate": 4.618196437511075e-05,
      "loss": 3.0349,
      "step": 1956
    },
    {
      "epoch": 0.17833059959905231,
      "grad_norm": 2.5236902236938477,
      "learning_rate": 4.617816212876102e-05,
      "loss": 2.694,
      "step": 1957
    },
    {
      "epoch": 0.17842172407508658,
      "grad_norm": 2.2877037525177,
      "learning_rate": 4.617435814677748e-05,
      "loss": 3.2323,
      "step": 1958
    },
    {
      "epoch": 0.17851284855112082,
      "grad_norm": 2.5378546714782715,
      "learning_rate": 4.6170552429471905e-05,
      "loss": 2.8907,
      "step": 1959
    },
    {
      "epoch": 0.17860397302715508,
      "grad_norm": 2.347435712814331,
      "learning_rate": 4.6166744977156154e-05,
      "loss": 3.1412,
      "step": 1960
    },
    {
      "epoch": 0.17869509750318935,
      "grad_norm": 1.9487541913986206,
      "learning_rate": 4.616293579014229e-05,
      "loss": 2.858,
      "step": 1961
    },
    {
      "epoch": 0.17878622197922361,
      "grad_norm": 1.4565964937210083,
      "learning_rate": 4.6159124868742485e-05,
      "loss": 2.9924,
      "step": 1962
    },
    {
      "epoch": 0.17887734645525788,
      "grad_norm": 1.4799039363861084,
      "learning_rate": 4.6155312213269053e-05,
      "loss": 2.7628,
      "step": 1963
    },
    {
      "epoch": 0.17896847093129215,
      "grad_norm": 3.3805723190307617,
      "learning_rate": 4.615149782403446e-05,
      "loss": 3.2429,
      "step": 1964
    },
    {
      "epoch": 0.1790595954073264,
      "grad_norm": 2.6682169437408447,
      "learning_rate": 4.614768170135132e-05,
      "loss": 2.9839,
      "step": 1965
    },
    {
      "epoch": 0.17915071988336068,
      "grad_norm": 2.525508403778076,
      "learning_rate": 4.614386384553235e-05,
      "loss": 3.1916,
      "step": 1966
    },
    {
      "epoch": 0.17924184435939494,
      "grad_norm": 2.9044320583343506,
      "learning_rate": 4.614004425689048e-05,
      "loss": 3.3356,
      "step": 1967
    },
    {
      "epoch": 0.1793329688354292,
      "grad_norm": 3.02644419670105,
      "learning_rate": 4.6136222935738704e-05,
      "loss": 2.914,
      "step": 1968
    },
    {
      "epoch": 0.17942409331146347,
      "grad_norm": 2.337035655975342,
      "learning_rate": 4.6132399882390206e-05,
      "loss": 2.9974,
      "step": 1969
    },
    {
      "epoch": 0.1795152177874977,
      "grad_norm": 2.178936243057251,
      "learning_rate": 4.6128575097158314e-05,
      "loss": 3.1322,
      "step": 1970
    },
    {
      "epoch": 0.17960634226353198,
      "grad_norm": 1.6006604433059692,
      "learning_rate": 4.612474858035647e-05,
      "loss": 3.0153,
      "step": 1971
    },
    {
      "epoch": 0.17969746673956624,
      "grad_norm": 2.1648287773132324,
      "learning_rate": 4.612092033229828e-05,
      "loss": 3.2334,
      "step": 1972
    },
    {
      "epoch": 0.1797885912156005,
      "grad_norm": 2.6615753173828125,
      "learning_rate": 4.611709035329747e-05,
      "loss": 2.9051,
      "step": 1973
    },
    {
      "epoch": 0.17987971569163477,
      "grad_norm": 1.9354596138000488,
      "learning_rate": 4.6113258643667936e-05,
      "loss": 3.108,
      "step": 1974
    },
    {
      "epoch": 0.17997084016766904,
      "grad_norm": 1.7395933866500854,
      "learning_rate": 4.610942520372369e-05,
      "loss": 3.0382,
      "step": 1975
    },
    {
      "epoch": 0.1800619646437033,
      "grad_norm": 2.480259895324707,
      "learning_rate": 4.610559003377891e-05,
      "loss": 2.9684,
      "step": 1976
    },
    {
      "epoch": 0.18015308911973757,
      "grad_norm": 1.3295300006866455,
      "learning_rate": 4.61017531341479e-05,
      "loss": 3.0404,
      "step": 1977
    },
    {
      "epoch": 0.18024421359577183,
      "grad_norm": 1.599542498588562,
      "learning_rate": 4.60979145051451e-05,
      "loss": 2.9742,
      "step": 1978
    },
    {
      "epoch": 0.1803353380718061,
      "grad_norm": 1.8933284282684326,
      "learning_rate": 4.609407414708512e-05,
      "loss": 3.0518,
      "step": 1979
    },
    {
      "epoch": 0.18042646254784034,
      "grad_norm": 1.4877815246582031,
      "learning_rate": 4.6090232060282666e-05,
      "loss": 2.9679,
      "step": 1980
    },
    {
      "epoch": 0.1805175870238746,
      "grad_norm": 1.6608158349990845,
      "learning_rate": 4.6086388245052636e-05,
      "loss": 3.0059,
      "step": 1981
    },
    {
      "epoch": 0.18060871149990887,
      "grad_norm": 3.126404047012329,
      "learning_rate": 4.608254270171003e-05,
      "loss": 3.1677,
      "step": 1982
    },
    {
      "epoch": 0.18069983597594313,
      "grad_norm": 2.0683717727661133,
      "learning_rate": 4.6078695430570004e-05,
      "loss": 3.2245,
      "step": 1983
    },
    {
      "epoch": 0.1807909604519774,
      "grad_norm": 2.5405118465423584,
      "learning_rate": 4.607484643194788e-05,
      "loss": 2.286,
      "step": 1984
    },
    {
      "epoch": 0.18088208492801167,
      "grad_norm": 2.7362663745880127,
      "learning_rate": 4.6070995706159075e-05,
      "loss": 3.0712,
      "step": 1985
    },
    {
      "epoch": 0.18097320940404593,
      "grad_norm": 2.33858585357666,
      "learning_rate": 4.606714325351918e-05,
      "loss": 3.1092,
      "step": 1986
    },
    {
      "epoch": 0.1810643338800802,
      "grad_norm": 2.5455336570739746,
      "learning_rate": 4.606328907434392e-05,
      "loss": 3.2204,
      "step": 1987
    },
    {
      "epoch": 0.18115545835611446,
      "grad_norm": 2.442511558532715,
      "learning_rate": 4.605943316894915e-05,
      "loss": 3.2713,
      "step": 1988
    },
    {
      "epoch": 0.18124658283214873,
      "grad_norm": 1.7061405181884766,
      "learning_rate": 4.605557553765089e-05,
      "loss": 2.9598,
      "step": 1989
    },
    {
      "epoch": 0.18133770730818297,
      "grad_norm": 2.4000542163848877,
      "learning_rate": 4.605171618076528e-05,
      "loss": 3.0296,
      "step": 1990
    },
    {
      "epoch": 0.18142883178421723,
      "grad_norm": 2.3328189849853516,
      "learning_rate": 4.6047855098608615e-05,
      "loss": 2.9324,
      "step": 1991
    },
    {
      "epoch": 0.1815199562602515,
      "grad_norm": 2.2985846996307373,
      "learning_rate": 4.604399229149733e-05,
      "loss": 3.152,
      "step": 1992
    },
    {
      "epoch": 0.18161108073628576,
      "grad_norm": 1.8579432964324951,
      "learning_rate": 4.604012775974798e-05,
      "loss": 3.0462,
      "step": 1993
    },
    {
      "epoch": 0.18170220521232003,
      "grad_norm": 1.2653917074203491,
      "learning_rate": 4.6036261503677285e-05,
      "loss": 2.9474,
      "step": 1994
    },
    {
      "epoch": 0.1817933296883543,
      "grad_norm": 1.8129152059555054,
      "learning_rate": 4.6032393523602114e-05,
      "loss": 3.0247,
      "step": 1995
    },
    {
      "epoch": 0.18188445416438856,
      "grad_norm": 2.5666768550872803,
      "learning_rate": 4.602852381983945e-05,
      "loss": 3.043,
      "step": 1996
    },
    {
      "epoch": 0.18197557864042282,
      "grad_norm": 2.3878471851348877,
      "learning_rate": 4.602465239270643e-05,
      "loss": 3.5325,
      "step": 1997
    },
    {
      "epoch": 0.1820667031164571,
      "grad_norm": 2.6700685024261475,
      "learning_rate": 4.602077924252034e-05,
      "loss": 2.6536,
      "step": 1998
    },
    {
      "epoch": 0.18215782759249136,
      "grad_norm": 2.682384967803955,
      "learning_rate": 4.601690436959859e-05,
      "loss": 3.1602,
      "step": 1999
    },
    {
      "epoch": 0.1822489520685256,
      "grad_norm": 2.54840087890625,
      "learning_rate": 4.601302777425875e-05,
      "loss": 3.2196,
      "step": 2000
    },
    {
      "epoch": 0.18234007654455986,
      "grad_norm": 3.4487414360046387,
      "learning_rate": 4.600914945681852e-05,
      "loss": 2.9432,
      "step": 2001
    },
    {
      "epoch": 0.18243120102059412,
      "grad_norm": 2.397068977355957,
      "learning_rate": 4.600526941759575e-05,
      "loss": 2.6896,
      "step": 2002
    },
    {
      "epoch": 0.1825223254966284,
      "grad_norm": 3.136640787124634,
      "learning_rate": 4.600138765690841e-05,
      "loss": 3.1864,
      "step": 2003
    },
    {
      "epoch": 0.18261344997266266,
      "grad_norm": 2.7209324836730957,
      "learning_rate": 4.5997504175074635e-05,
      "loss": 3.2833,
      "step": 2004
    },
    {
      "epoch": 0.18270457444869692,
      "grad_norm": 3.403630256652832,
      "learning_rate": 4.5993618972412684e-05,
      "loss": 3.3523,
      "step": 2005
    },
    {
      "epoch": 0.1827956989247312,
      "grad_norm": 1.2634307146072388,
      "learning_rate": 4.598973204924097e-05,
      "loss": 2.9085,
      "step": 2006
    },
    {
      "epoch": 0.18288682340076545,
      "grad_norm": 6.175005912780762,
      "learning_rate": 4.598584340587805e-05,
      "loss": 3.1583,
      "step": 2007
    },
    {
      "epoch": 0.18297794787679972,
      "grad_norm": 2.7069215774536133,
      "learning_rate": 4.59819530426426e-05,
      "loss": 3.3082,
      "step": 2008
    },
    {
      "epoch": 0.18306907235283398,
      "grad_norm": 2.222499370574951,
      "learning_rate": 4.597806095985346e-05,
      "loss": 2.8776,
      "step": 2009
    },
    {
      "epoch": 0.18316019682886822,
      "grad_norm": 2.880443811416626,
      "learning_rate": 4.5974167157829595e-05,
      "loss": 3.0739,
      "step": 2010
    },
    {
      "epoch": 0.1832513213049025,
      "grad_norm": 2.596893787384033,
      "learning_rate": 4.5970271636890114e-05,
      "loss": 3.215,
      "step": 2011
    },
    {
      "epoch": 0.18334244578093675,
      "grad_norm": 2.9432852268218994,
      "learning_rate": 4.5966374397354285e-05,
      "loss": 2.6714,
      "step": 2012
    },
    {
      "epoch": 0.18343357025697102,
      "grad_norm": 3.321160316467285,
      "learning_rate": 4.5962475439541494e-05,
      "loss": 3.1507,
      "step": 2013
    },
    {
      "epoch": 0.18352469473300528,
      "grad_norm": 2.0445027351379395,
      "learning_rate": 4.595857476377127e-05,
      "loss": 2.9539,
      "step": 2014
    },
    {
      "epoch": 0.18361581920903955,
      "grad_norm": 2.5950844287872314,
      "learning_rate": 4.595467237036329e-05,
      "loss": 3.0004,
      "step": 2015
    },
    {
      "epoch": 0.18370694368507381,
      "grad_norm": 3.709110975265503,
      "learning_rate": 4.5950768259637384e-05,
      "loss": 3.2379,
      "step": 2016
    },
    {
      "epoch": 0.18379806816110808,
      "grad_norm": 2.228250741958618,
      "learning_rate": 4.594686243191349e-05,
      "loss": 3.1437,
      "step": 2017
    },
    {
      "epoch": 0.18388919263714235,
      "grad_norm": 2.0134904384613037,
      "learning_rate": 4.594295488751172e-05,
      "loss": 3.2146,
      "step": 2018
    },
    {
      "epoch": 0.1839803171131766,
      "grad_norm": 2.154862880706787,
      "learning_rate": 4.593904562675232e-05,
      "loss": 3.1902,
      "step": 2019
    },
    {
      "epoch": 0.18407144158921085,
      "grad_norm": 2.953571081161499,
      "learning_rate": 4.593513464995565e-05,
      "loss": 2.7227,
      "step": 2020
    },
    {
      "epoch": 0.18416256606524511,
      "grad_norm": 2.9403328895568848,
      "learning_rate": 4.593122195744223e-05,
      "loss": 3.4374,
      "step": 2021
    },
    {
      "epoch": 0.18425369054127938,
      "grad_norm": 1.9956389665603638,
      "learning_rate": 4.592730754953274e-05,
      "loss": 3.0119,
      "step": 2022
    },
    {
      "epoch": 0.18434481501731365,
      "grad_norm": 1.7445966005325317,
      "learning_rate": 4.5923391426547954e-05,
      "loss": 2.8911,
      "step": 2023
    },
    {
      "epoch": 0.1844359394933479,
      "grad_norm": 2.4210798740386963,
      "learning_rate": 4.591947358880884e-05,
      "loss": 3.3719,
      "step": 2024
    },
    {
      "epoch": 0.18452706396938218,
      "grad_norm": 3.0291802883148193,
      "learning_rate": 4.591555403663647e-05,
      "loss": 2.9503,
      "step": 2025
    },
    {
      "epoch": 0.18461818844541644,
      "grad_norm": 2.123589515686035,
      "learning_rate": 4.591163277035206e-05,
      "loss": 2.8746,
      "step": 2026
    },
    {
      "epoch": 0.1847093129214507,
      "grad_norm": 2.325672149658203,
      "learning_rate": 4.590770979027699e-05,
      "loss": 3.1381,
      "step": 2027
    },
    {
      "epoch": 0.18480043739748497,
      "grad_norm": 2.3994011878967285,
      "learning_rate": 4.5903785096732746e-05,
      "loss": 3.2229,
      "step": 2028
    },
    {
      "epoch": 0.18489156187351924,
      "grad_norm": 2.372422933578491,
      "learning_rate": 4.589985869004098e-05,
      "loss": 3.3611,
      "step": 2029
    },
    {
      "epoch": 0.18498268634955348,
      "grad_norm": 2.531792163848877,
      "learning_rate": 4.589593057052347e-05,
      "loss": 3.3041,
      "step": 2030
    },
    {
      "epoch": 0.18507381082558774,
      "grad_norm": 2.779130458831787,
      "learning_rate": 4.589200073850215e-05,
      "loss": 3.0685,
      "step": 2031
    },
    {
      "epoch": 0.185164935301622,
      "grad_norm": 1.2927250862121582,
      "learning_rate": 4.5888069194299086e-05,
      "loss": 2.9203,
      "step": 2032
    },
    {
      "epoch": 0.18525605977765627,
      "grad_norm": 1.568803310394287,
      "learning_rate": 4.5884135938236474e-05,
      "loss": 3.0554,
      "step": 2033
    },
    {
      "epoch": 0.18534718425369054,
      "grad_norm": 2.5967936515808105,
      "learning_rate": 4.5880200970636665e-05,
      "loss": 2.9178,
      "step": 2034
    },
    {
      "epoch": 0.1854383087297248,
      "grad_norm": 3.115445375442505,
      "learning_rate": 4.5876264291822144e-05,
      "loss": 2.5378,
      "step": 2035
    },
    {
      "epoch": 0.18552943320575907,
      "grad_norm": 1.5003809928894043,
      "learning_rate": 4.587232590211554e-05,
      "loss": 2.916,
      "step": 2036
    },
    {
      "epoch": 0.18562055768179334,
      "grad_norm": 2.3400187492370605,
      "learning_rate": 4.586838580183963e-05,
      "loss": 3.4025,
      "step": 2037
    },
    {
      "epoch": 0.1857116821578276,
      "grad_norm": 1.731300950050354,
      "learning_rate": 4.586444399131729e-05,
      "loss": 2.9039,
      "step": 2038
    },
    {
      "epoch": 0.18580280663386187,
      "grad_norm": 1.213876724243164,
      "learning_rate": 4.586050047087159e-05,
      "loss": 2.9432,
      "step": 2039
    },
    {
      "epoch": 0.1858939311098961,
      "grad_norm": 2.3750312328338623,
      "learning_rate": 4.585655524082572e-05,
      "loss": 2.8809,
      "step": 2040
    },
    {
      "epoch": 0.18598505558593037,
      "grad_norm": 2.145286798477173,
      "learning_rate": 4.5852608301502994e-05,
      "loss": 2.9922,
      "step": 2041
    },
    {
      "epoch": 0.18607618006196464,
      "grad_norm": 1.3066060543060303,
      "learning_rate": 4.584865965322689e-05,
      "loss": 2.8885,
      "step": 2042
    },
    {
      "epoch": 0.1861673045379989,
      "grad_norm": 3.6793642044067383,
      "learning_rate": 4.584470929632101e-05,
      "loss": 3.3173,
      "step": 2043
    },
    {
      "epoch": 0.18625842901403317,
      "grad_norm": 1.1995574235916138,
      "learning_rate": 4.5840757231109096e-05,
      "loss": 2.8449,
      "step": 2044
    },
    {
      "epoch": 0.18634955349006743,
      "grad_norm": 2.265745162963867,
      "learning_rate": 4.583680345791505e-05,
      "loss": 3.0027,
      "step": 2045
    },
    {
      "epoch": 0.1864406779661017,
      "grad_norm": 2.7274153232574463,
      "learning_rate": 4.5832847977062874e-05,
      "loss": 2.9786,
      "step": 2046
    },
    {
      "epoch": 0.18653180244213596,
      "grad_norm": 2.7558224201202393,
      "learning_rate": 4.5828890788876766e-05,
      "loss": 2.8255,
      "step": 2047
    },
    {
      "epoch": 0.18662292691817023,
      "grad_norm": 1.4628591537475586,
      "learning_rate": 4.582493189368101e-05,
      "loss": 3.0028,
      "step": 2048
    },
    {
      "epoch": 0.1867140513942045,
      "grad_norm": 2.768812894821167,
      "learning_rate": 4.5820971291800067e-05,
      "loss": 3.2096,
      "step": 2049
    },
    {
      "epoch": 0.18680517587023876,
      "grad_norm": 2.3017489910125732,
      "learning_rate": 4.581700898355852e-05,
      "loss": 3.1005,
      "step": 2050
    },
    {
      "epoch": 0.186896300346273,
      "grad_norm": 2.9306223392486572,
      "learning_rate": 4.581304496928109e-05,
      "loss": 3.0057,
      "step": 2051
    },
    {
      "epoch": 0.18698742482230726,
      "grad_norm": 2.0629477500915527,
      "learning_rate": 4.580907924929265e-05,
      "loss": 2.6099,
      "step": 2052
    },
    {
      "epoch": 0.18707854929834153,
      "grad_norm": 3.7458243370056152,
      "learning_rate": 4.58051118239182e-05,
      "loss": 3.3813,
      "step": 2053
    },
    {
      "epoch": 0.1871696737743758,
      "grad_norm": 2.1974449157714844,
      "learning_rate": 4.5801142693482895e-05,
      "loss": 2.697,
      "step": 2054
    },
    {
      "epoch": 0.18726079825041006,
      "grad_norm": 3.1911566257476807,
      "learning_rate": 4.5797171858312014e-05,
      "loss": 3.0624,
      "step": 2055
    },
    {
      "epoch": 0.18735192272644433,
      "grad_norm": 1.5425851345062256,
      "learning_rate": 4.579319931873098e-05,
      "loss": 2.8569,
      "step": 2056
    },
    {
      "epoch": 0.1874430472024786,
      "grad_norm": 1.3476756811141968,
      "learning_rate": 4.578922507506538e-05,
      "loss": 2.9883,
      "step": 2057
    },
    {
      "epoch": 0.18753417167851286,
      "grad_norm": 2.4094388484954834,
      "learning_rate": 4.578524912764088e-05,
      "loss": 3.2592,
      "step": 2058
    },
    {
      "epoch": 0.18762529615454712,
      "grad_norm": 1.4279744625091553,
      "learning_rate": 4.578127147678336e-05,
      "loss": 2.9215,
      "step": 2059
    },
    {
      "epoch": 0.1877164206305814,
      "grad_norm": 2.296020984649658,
      "learning_rate": 4.577729212281879e-05,
      "loss": 3.1286,
      "step": 2060
    },
    {
      "epoch": 0.18780754510661563,
      "grad_norm": 2.0938169956207275,
      "learning_rate": 4.57733110660733e-05,
      "loss": 3.1295,
      "step": 2061
    },
    {
      "epoch": 0.1878986695826499,
      "grad_norm": 2.3235247135162354,
      "learning_rate": 4.576932830687314e-05,
      "loss": 3.0549,
      "step": 2062
    },
    {
      "epoch": 0.18798979405868416,
      "grad_norm": 1.3535895347595215,
      "learning_rate": 4.5765343845544715e-05,
      "loss": 2.9731,
      "step": 2063
    },
    {
      "epoch": 0.18808091853471842,
      "grad_norm": 3.0996577739715576,
      "learning_rate": 4.576135768241458e-05,
      "loss": 3.0238,
      "step": 2064
    },
    {
      "epoch": 0.1881720430107527,
      "grad_norm": 2.4257102012634277,
      "learning_rate": 4.5757369817809415e-05,
      "loss": 2.8892,
      "step": 2065
    },
    {
      "epoch": 0.18826316748678695,
      "grad_norm": 1.8863178491592407,
      "learning_rate": 4.575338025205602e-05,
      "loss": 2.9151,
      "step": 2066
    },
    {
      "epoch": 0.18835429196282122,
      "grad_norm": 3.3435513973236084,
      "learning_rate": 4.574938898548139e-05,
      "loss": 4.1283,
      "step": 2067
    },
    {
      "epoch": 0.18844541643885548,
      "grad_norm": 1.7223072052001953,
      "learning_rate": 4.574539601841259e-05,
      "loss": 2.9499,
      "step": 2068
    },
    {
      "epoch": 0.18853654091488975,
      "grad_norm": 2.125389814376831,
      "learning_rate": 4.574140135117689e-05,
      "loss": 3.1987,
      "step": 2069
    },
    {
      "epoch": 0.18862766539092402,
      "grad_norm": 3.0489702224731445,
      "learning_rate": 4.5737404984101646e-05,
      "loss": 2.8953,
      "step": 2070
    },
    {
      "epoch": 0.18871878986695825,
      "grad_norm": 4.152069091796875,
      "learning_rate": 4.573340691751439e-05,
      "loss": 2.5645,
      "step": 2071
    },
    {
      "epoch": 0.18880991434299252,
      "grad_norm": 2.815276622772217,
      "learning_rate": 4.572940715174276e-05,
      "loss": 3.3646,
      "step": 2072
    },
    {
      "epoch": 0.18890103881902678,
      "grad_norm": 3.1981680393218994,
      "learning_rate": 4.5725405687114585e-05,
      "loss": 3.1954,
      "step": 2073
    },
    {
      "epoch": 0.18899216329506105,
      "grad_norm": 1.587654709815979,
      "learning_rate": 4.5721402523957774e-05,
      "loss": 3.028,
      "step": 2074
    },
    {
      "epoch": 0.18908328777109532,
      "grad_norm": 2.3439455032348633,
      "learning_rate": 4.5717397662600416e-05,
      "loss": 3.1105,
      "step": 2075
    },
    {
      "epoch": 0.18917441224712958,
      "grad_norm": 2.171013355255127,
      "learning_rate": 4.5713391103370705e-05,
      "loss": 2.5963,
      "step": 2076
    },
    {
      "epoch": 0.18926553672316385,
      "grad_norm": 1.4065895080566406,
      "learning_rate": 4.570938284659702e-05,
      "loss": 2.8371,
      "step": 2077
    },
    {
      "epoch": 0.1893566611991981,
      "grad_norm": 1.2731765508651733,
      "learning_rate": 4.570537289260785e-05,
      "loss": 2.9662,
      "step": 2078
    },
    {
      "epoch": 0.18944778567523238,
      "grad_norm": 2.258131504058838,
      "learning_rate": 4.57013612417318e-05,
      "loss": 3.3453,
      "step": 2079
    },
    {
      "epoch": 0.18953891015126664,
      "grad_norm": 2.9239134788513184,
      "learning_rate": 4.569734789429767e-05,
      "loss": 3.0962,
      "step": 2080
    },
    {
      "epoch": 0.18963003462730088,
      "grad_norm": 1.1221950054168701,
      "learning_rate": 4.5693332850634354e-05,
      "loss": 2.832,
      "step": 2081
    },
    {
      "epoch": 0.18972115910333515,
      "grad_norm": 1.1654523611068726,
      "learning_rate": 4.5689316111070916e-05,
      "loss": 2.9376,
      "step": 2082
    },
    {
      "epoch": 0.1898122835793694,
      "grad_norm": 2.6919686794281006,
      "learning_rate": 4.568529767593652e-05,
      "loss": 4.4422,
      "step": 2083
    },
    {
      "epoch": 0.18990340805540368,
      "grad_norm": 2.5946178436279297,
      "learning_rate": 4.568127754556051e-05,
      "loss": 3.1004,
      "step": 2084
    },
    {
      "epoch": 0.18999453253143794,
      "grad_norm": 1.5255308151245117,
      "learning_rate": 4.5677255720272347e-05,
      "loss": 3.0114,
      "step": 2085
    },
    {
      "epoch": 0.1900856570074722,
      "grad_norm": 2.6808621883392334,
      "learning_rate": 4.567323220040164e-05,
      "loss": 3.1643,
      "step": 2086
    },
    {
      "epoch": 0.19017678148350647,
      "grad_norm": 1.4799108505249023,
      "learning_rate": 4.5669206986278126e-05,
      "loss": 2.8125,
      "step": 2087
    },
    {
      "epoch": 0.19026790595954074,
      "grad_norm": 2.4696061611175537,
      "learning_rate": 4.566518007823168e-05,
      "loss": 3.2783,
      "step": 2088
    },
    {
      "epoch": 0.190359030435575,
      "grad_norm": 2.746445417404175,
      "learning_rate": 4.5661151476592346e-05,
      "loss": 2.8399,
      "step": 2089
    },
    {
      "epoch": 0.19045015491160927,
      "grad_norm": 2.9999096393585205,
      "learning_rate": 4.5657121181690266e-05,
      "loss": 3.0283,
      "step": 2090
    },
    {
      "epoch": 0.1905412793876435,
      "grad_norm": 1.4273756742477417,
      "learning_rate": 4.5653089193855736e-05,
      "loss": 2.893,
      "step": 2091
    },
    {
      "epoch": 0.19063240386367777,
      "grad_norm": 4.1122145652771,
      "learning_rate": 4.564905551341921e-05,
      "loss": 2.8928,
      "step": 2092
    },
    {
      "epoch": 0.19072352833971204,
      "grad_norm": 2.707650899887085,
      "learning_rate": 4.564502014071125e-05,
      "loss": 3.2524,
      "step": 2093
    },
    {
      "epoch": 0.1908146528157463,
      "grad_norm": 2.301670789718628,
      "learning_rate": 4.564098307606257e-05,
      "loss": 2.7783,
      "step": 2094
    },
    {
      "epoch": 0.19090577729178057,
      "grad_norm": 1.4095778465270996,
      "learning_rate": 4.563694431980403e-05,
      "loss": 2.8485,
      "step": 2095
    },
    {
      "epoch": 0.19099690176781484,
      "grad_norm": 1.4083749055862427,
      "learning_rate": 4.563290387226662e-05,
      "loss": 3.0629,
      "step": 2096
    },
    {
      "epoch": 0.1910880262438491,
      "grad_norm": 2.204667806625366,
      "learning_rate": 4.562886173378147e-05,
      "loss": 2.9999,
      "step": 2097
    },
    {
      "epoch": 0.19117915071988337,
      "grad_norm": 1.3312679529190063,
      "learning_rate": 4.562481790467985e-05,
      "loss": 2.9707,
      "step": 2098
    },
    {
      "epoch": 0.19127027519591763,
      "grad_norm": 3.2946746349334717,
      "learning_rate": 4.562077238529316e-05,
      "loss": 4.23,
      "step": 2099
    },
    {
      "epoch": 0.1913613996719519,
      "grad_norm": 1.7384039163589478,
      "learning_rate": 4.561672517595297e-05,
      "loss": 3.0685,
      "step": 2100
    },
    {
      "epoch": 0.19145252414798614,
      "grad_norm": 1.4533058404922485,
      "learning_rate": 4.561267627699093e-05,
      "loss": 2.8443,
      "step": 2101
    },
    {
      "epoch": 0.1915436486240204,
      "grad_norm": 2.7467310428619385,
      "learning_rate": 4.560862568873889e-05,
      "loss": 4.401,
      "step": 2102
    },
    {
      "epoch": 0.19163477310005467,
      "grad_norm": 2.8676555156707764,
      "learning_rate": 4.560457341152881e-05,
      "loss": 3.08,
      "step": 2103
    },
    {
      "epoch": 0.19172589757608893,
      "grad_norm": 3.2780473232269287,
      "learning_rate": 4.5600519445692776e-05,
      "loss": 3.0176,
      "step": 2104
    },
    {
      "epoch": 0.1918170220521232,
      "grad_norm": 3.354327440261841,
      "learning_rate": 4.559646379156303e-05,
      "loss": 3.6978,
      "step": 2105
    },
    {
      "epoch": 0.19190814652815746,
      "grad_norm": 2.4217288494110107,
      "learning_rate": 4.559240644947196e-05,
      "loss": 3.128,
      "step": 2106
    },
    {
      "epoch": 0.19199927100419173,
      "grad_norm": 4.1892218589782715,
      "learning_rate": 4.558834741975207e-05,
      "loss": 3.1785,
      "step": 2107
    },
    {
      "epoch": 0.192090395480226,
      "grad_norm": 2.2831571102142334,
      "learning_rate": 4.558428670273601e-05,
      "loss": 3.0754,
      "step": 2108
    },
    {
      "epoch": 0.19218151995626026,
      "grad_norm": 2.5602338314056396,
      "learning_rate": 4.558022429875659e-05,
      "loss": 3.3463,
      "step": 2109
    },
    {
      "epoch": 0.19227264443229453,
      "grad_norm": 2.437709093093872,
      "learning_rate": 4.557616020814673e-05,
      "loss": 3.0898,
      "step": 2110
    },
    {
      "epoch": 0.19236376890832876,
      "grad_norm": 2.1364035606384277,
      "learning_rate": 4.557209443123949e-05,
      "loss": 3.2755,
      "step": 2111
    },
    {
      "epoch": 0.19245489338436303,
      "grad_norm": 2.9577412605285645,
      "learning_rate": 4.5568026968368085e-05,
      "loss": 2.6163,
      "step": 2112
    },
    {
      "epoch": 0.1925460178603973,
      "grad_norm": 1.5252705812454224,
      "learning_rate": 4.556395781986586e-05,
      "loss": 2.9584,
      "step": 2113
    },
    {
      "epoch": 0.19263714233643156,
      "grad_norm": 1.9141896963119507,
      "learning_rate": 4.555988698606629e-05,
      "loss": 3.2268,
      "step": 2114
    },
    {
      "epoch": 0.19272826681246583,
      "grad_norm": 2.066657066345215,
      "learning_rate": 4.555581446730301e-05,
      "loss": 2.9732,
      "step": 2115
    },
    {
      "epoch": 0.1928193912885001,
      "grad_norm": 2.2649478912353516,
      "learning_rate": 4.555174026390977e-05,
      "loss": 3.239,
      "step": 2116
    },
    {
      "epoch": 0.19291051576453436,
      "grad_norm": 1.9464354515075684,
      "learning_rate": 4.5547664376220464e-05,
      "loss": 3.0,
      "step": 2117
    },
    {
      "epoch": 0.19300164024056862,
      "grad_norm": 2.357696533203125,
      "learning_rate": 4.554358680456913e-05,
      "loss": 3.2012,
      "step": 2118
    },
    {
      "epoch": 0.1930927647166029,
      "grad_norm": 3.598691940307617,
      "learning_rate": 4.553950754928994e-05,
      "loss": 3.1167,
      "step": 2119
    },
    {
      "epoch": 0.19318388919263715,
      "grad_norm": 2.360828161239624,
      "learning_rate": 4.5535426610717215e-05,
      "loss": 3.15,
      "step": 2120
    },
    {
      "epoch": 0.1932750136686714,
      "grad_norm": 1.884366512298584,
      "learning_rate": 4.553134398918539e-05,
      "loss": 3.1738,
      "step": 2121
    },
    {
      "epoch": 0.19336613814470566,
      "grad_norm": 2.4405581951141357,
      "learning_rate": 4.5527259685029055e-05,
      "loss": 3.0889,
      "step": 2122
    },
    {
      "epoch": 0.19345726262073992,
      "grad_norm": 1.7152312994003296,
      "learning_rate": 4.5523173698582947e-05,
      "loss": 2.9895,
      "step": 2123
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 2.1175060272216797,
      "learning_rate": 4.551908603018191e-05,
      "loss": 3.0523,
      "step": 2124
    },
    {
      "epoch": 0.19363951157280845,
      "grad_norm": 2.213157892227173,
      "learning_rate": 4.551499668016096e-05,
      "loss": 3.2291,
      "step": 2125
    },
    {
      "epoch": 0.19373063604884272,
      "grad_norm": 2.0308704376220703,
      "learning_rate": 4.551090564885523e-05,
      "loss": 2.9796,
      "step": 2126
    },
    {
      "epoch": 0.19382176052487698,
      "grad_norm": 3.078508138656616,
      "learning_rate": 4.550681293659998e-05,
      "loss": 4.9557,
      "step": 2127
    },
    {
      "epoch": 0.19391288500091125,
      "grad_norm": 3.698376178741455,
      "learning_rate": 4.550271854373066e-05,
      "loss": 3.4455,
      "step": 2128
    },
    {
      "epoch": 0.19400400947694552,
      "grad_norm": 2.6224312782287598,
      "learning_rate": 4.5498622470582786e-05,
      "loss": 3.9199,
      "step": 2129
    },
    {
      "epoch": 0.19409513395297978,
      "grad_norm": 2.5228397846221924,
      "learning_rate": 4.5494524717492074e-05,
      "loss": 3.2904,
      "step": 2130
    },
    {
      "epoch": 0.19418625842901405,
      "grad_norm": 1.2899537086486816,
      "learning_rate": 4.549042528479433e-05,
      "loss": 2.9766,
      "step": 2131
    },
    {
      "epoch": 0.19427738290504828,
      "grad_norm": 2.977541446685791,
      "learning_rate": 4.548632417282554e-05,
      "loss": 4.3253,
      "step": 2132
    },
    {
      "epoch": 0.19436850738108255,
      "grad_norm": 2.68886137008667,
      "learning_rate": 4.5482221381921776e-05,
      "loss": 2.8692,
      "step": 2133
    },
    {
      "epoch": 0.19445963185711682,
      "grad_norm": 1.536305546760559,
      "learning_rate": 4.547811691241931e-05,
      "loss": 3.056,
      "step": 2134
    },
    {
      "epoch": 0.19455075633315108,
      "grad_norm": 2.4093570709228516,
      "learning_rate": 4.547401076465449e-05,
      "loss": 2.9792,
      "step": 2135
    },
    {
      "epoch": 0.19464188080918535,
      "grad_norm": 2.991114854812622,
      "learning_rate": 4.546990293896386e-05,
      "loss": 3.2235,
      "step": 2136
    },
    {
      "epoch": 0.1947330052852196,
      "grad_norm": 3.1127562522888184,
      "learning_rate": 4.5465793435684056e-05,
      "loss": 3.223,
      "step": 2137
    },
    {
      "epoch": 0.19482412976125388,
      "grad_norm": 1.3699736595153809,
      "learning_rate": 4.546168225515187e-05,
      "loss": 2.9491,
      "step": 2138
    },
    {
      "epoch": 0.19491525423728814,
      "grad_norm": 2.92671537399292,
      "learning_rate": 4.545756939770423e-05,
      "loss": 4.3738,
      "step": 2139
    },
    {
      "epoch": 0.1950063787133224,
      "grad_norm": 2.226743459701538,
      "learning_rate": 4.5453454863678204e-05,
      "loss": 2.8414,
      "step": 2140
    },
    {
      "epoch": 0.19509750318935667,
      "grad_norm": 1.7715834379196167,
      "learning_rate": 4.544933865341098e-05,
      "loss": 2.9168,
      "step": 2141
    },
    {
      "epoch": 0.1951886276653909,
      "grad_norm": 3.4419286251068115,
      "learning_rate": 4.544522076723993e-05,
      "loss": 3.1534,
      "step": 2142
    },
    {
      "epoch": 0.19527975214142518,
      "grad_norm": 4.409292221069336,
      "learning_rate": 4.544110120550249e-05,
      "loss": 3.2224,
      "step": 2143
    },
    {
      "epoch": 0.19537087661745944,
      "grad_norm": 1.2458796501159668,
      "learning_rate": 4.54369799685363e-05,
      "loss": 3.0672,
      "step": 2144
    },
    {
      "epoch": 0.1954620010934937,
      "grad_norm": 2.2109553813934326,
      "learning_rate": 4.5432857056679114e-05,
      "loss": 3.1748,
      "step": 2145
    },
    {
      "epoch": 0.19555312556952797,
      "grad_norm": 3.46372652053833,
      "learning_rate": 4.5428732470268805e-05,
      "loss": 2.9603,
      "step": 2146
    },
    {
      "epoch": 0.19564425004556224,
      "grad_norm": 3.151329755783081,
      "learning_rate": 4.542460620964341e-05,
      "loss": 4.1138,
      "step": 2147
    },
    {
      "epoch": 0.1957353745215965,
      "grad_norm": 1.7655166387557983,
      "learning_rate": 4.542047827514108e-05,
      "loss": 2.9779,
      "step": 2148
    },
    {
      "epoch": 0.19582649899763077,
      "grad_norm": 3.0415544509887695,
      "learning_rate": 4.5416348667100136e-05,
      "loss": 3.3329,
      "step": 2149
    },
    {
      "epoch": 0.19591762347366504,
      "grad_norm": 1.4054174423217773,
      "learning_rate": 4.5412217385859e-05,
      "loss": 3.0166,
      "step": 2150
    },
    {
      "epoch": 0.1960087479496993,
      "grad_norm": 2.500098705291748,
      "learning_rate": 4.540808443175625e-05,
      "loss": 2.9683,
      "step": 2151
    },
    {
      "epoch": 0.19609987242573354,
      "grad_norm": 2.294620990753174,
      "learning_rate": 4.540394980513061e-05,
      "loss": 4.1639,
      "step": 2152
    },
    {
      "epoch": 0.1961909969017678,
      "grad_norm": 2.088880777359009,
      "learning_rate": 4.5399813506320904e-05,
      "loss": 3.3998,
      "step": 2153
    },
    {
      "epoch": 0.19628212137780207,
      "grad_norm": 2.7382395267486572,
      "learning_rate": 4.539567553566614e-05,
      "loss": 2.9963,
      "step": 2154
    },
    {
      "epoch": 0.19637324585383634,
      "grad_norm": 4.0386433601379395,
      "learning_rate": 4.5391535893505427e-05,
      "loss": 3.1648,
      "step": 2155
    },
    {
      "epoch": 0.1964643703298706,
      "grad_norm": 1.3024513721466064,
      "learning_rate": 4.538739458017803e-05,
      "loss": 2.9437,
      "step": 2156
    },
    {
      "epoch": 0.19655549480590487,
      "grad_norm": 6.040770053863525,
      "learning_rate": 4.5383251596023355e-05,
      "loss": 3.2678,
      "step": 2157
    },
    {
      "epoch": 0.19664661928193913,
      "grad_norm": 2.427100658416748,
      "learning_rate": 4.537910694138092e-05,
      "loss": 2.8039,
      "step": 2158
    },
    {
      "epoch": 0.1967377437579734,
      "grad_norm": 2.61849045753479,
      "learning_rate": 4.5374960616590406e-05,
      "loss": 2.5323,
      "step": 2159
    },
    {
      "epoch": 0.19682886823400766,
      "grad_norm": 2.5621817111968994,
      "learning_rate": 4.537081262199162e-05,
      "loss": 2.8832,
      "step": 2160
    },
    {
      "epoch": 0.19691999271004193,
      "grad_norm": 3.332515239715576,
      "learning_rate": 4.53666629579245e-05,
      "loss": 3.1133,
      "step": 2161
    },
    {
      "epoch": 0.19701111718607617,
      "grad_norm": 2.2672274112701416,
      "learning_rate": 4.536251162472914e-05,
      "loss": 3.0894,
      "step": 2162
    },
    {
      "epoch": 0.19710224166211043,
      "grad_norm": 1.664180874824524,
      "learning_rate": 4.5358358622745746e-05,
      "loss": 3.0358,
      "step": 2163
    },
    {
      "epoch": 0.1971933661381447,
      "grad_norm": 3.5398144721984863,
      "learning_rate": 4.535420395231467e-05,
      "loss": 3.1831,
      "step": 2164
    },
    {
      "epoch": 0.19728449061417896,
      "grad_norm": 3.1233232021331787,
      "learning_rate": 4.535004761377642e-05,
      "loss": 3.0191,
      "step": 2165
    },
    {
      "epoch": 0.19737561509021323,
      "grad_norm": 2.3621573448181152,
      "learning_rate": 4.534588960747161e-05,
      "loss": 3.0258,
      "step": 2166
    },
    {
      "epoch": 0.1974667395662475,
      "grad_norm": 1.4532358646392822,
      "learning_rate": 4.534172993374101e-05,
      "loss": 2.8911,
      "step": 2167
    },
    {
      "epoch": 0.19755786404228176,
      "grad_norm": 2.0871973037719727,
      "learning_rate": 4.533756859292552e-05,
      "loss": 3.0958,
      "step": 2168
    },
    {
      "epoch": 0.19764898851831603,
      "grad_norm": 2.8289287090301514,
      "learning_rate": 4.533340558536618e-05,
      "loss": 3.884,
      "step": 2169
    },
    {
      "epoch": 0.1977401129943503,
      "grad_norm": 1.484332799911499,
      "learning_rate": 4.532924091140417e-05,
      "loss": 2.9941,
      "step": 2170
    },
    {
      "epoch": 0.19783123747038456,
      "grad_norm": 2.4782042503356934,
      "learning_rate": 4.532507457138079e-05,
      "loss": 3.2975,
      "step": 2171
    },
    {
      "epoch": 0.1979223619464188,
      "grad_norm": 2.839928150177002,
      "learning_rate": 4.53209065656375e-05,
      "loss": 3.1493,
      "step": 2172
    },
    {
      "epoch": 0.19801348642245306,
      "grad_norm": 2.041877508163452,
      "learning_rate": 4.531673689451588e-05,
      "loss": 3.306,
      "step": 2173
    },
    {
      "epoch": 0.19810461089848733,
      "grad_norm": 2.4514687061309814,
      "learning_rate": 4.531256555835764e-05,
      "loss": 3.2532,
      "step": 2174
    },
    {
      "epoch": 0.1981957353745216,
      "grad_norm": 3.0531530380249023,
      "learning_rate": 4.530839255750465e-05,
      "loss": 3.0279,
      "step": 2175
    },
    {
      "epoch": 0.19828685985055586,
      "grad_norm": 2.376551866531372,
      "learning_rate": 4.530421789229891e-05,
      "loss": 3.0501,
      "step": 2176
    },
    {
      "epoch": 0.19837798432659012,
      "grad_norm": 2.2981691360473633,
      "learning_rate": 4.530004156308253e-05,
      "loss": 3.3026,
      "step": 2177
    },
    {
      "epoch": 0.1984691088026244,
      "grad_norm": 2.2593307495117188,
      "learning_rate": 4.5295863570197786e-05,
      "loss": 2.9608,
      "step": 2178
    },
    {
      "epoch": 0.19856023327865865,
      "grad_norm": 3.573744297027588,
      "learning_rate": 4.529168391398709e-05,
      "loss": 3.0543,
      "step": 2179
    },
    {
      "epoch": 0.19865135775469292,
      "grad_norm": 3.453265428543091,
      "learning_rate": 4.5287502594792975e-05,
      "loss": 2.6017,
      "step": 2180
    },
    {
      "epoch": 0.19874248223072719,
      "grad_norm": 2.143185615539551,
      "learning_rate": 4.528331961295811e-05,
      "loss": 3.1039,
      "step": 2181
    },
    {
      "epoch": 0.19883360670676142,
      "grad_norm": 2.288445234298706,
      "learning_rate": 4.527913496882531e-05,
      "loss": 3.1229,
      "step": 2182
    },
    {
      "epoch": 0.1989247311827957,
      "grad_norm": 1.655011773109436,
      "learning_rate": 4.527494866273753e-05,
      "loss": 2.8664,
      "step": 2183
    },
    {
      "epoch": 0.19901585565882995,
      "grad_norm": 1.4328827857971191,
      "learning_rate": 4.527076069503785e-05,
      "loss": 3.0061,
      "step": 2184
    },
    {
      "epoch": 0.19910698013486422,
      "grad_norm": 2.815896987915039,
      "learning_rate": 4.526657106606949e-05,
      "loss": 3.1797,
      "step": 2185
    },
    {
      "epoch": 0.19919810461089849,
      "grad_norm": 2.2054543495178223,
      "learning_rate": 4.526237977617581e-05,
      "loss": 3.111,
      "step": 2186
    },
    {
      "epoch": 0.19928922908693275,
      "grad_norm": 3.506070613861084,
      "learning_rate": 4.525818682570029e-05,
      "loss": 3.074,
      "step": 2187
    },
    {
      "epoch": 0.19938035356296702,
      "grad_norm": 1.994211196899414,
      "learning_rate": 4.525399221498657e-05,
      "loss": 3.0744,
      "step": 2188
    },
    {
      "epoch": 0.19947147803900128,
      "grad_norm": 1.78602933883667,
      "learning_rate": 4.524979594437842e-05,
      "loss": 2.6679,
      "step": 2189
    },
    {
      "epoch": 0.19956260251503555,
      "grad_norm": 2.7989487648010254,
      "learning_rate": 4.524559801421973e-05,
      "loss": 2.7106,
      "step": 2190
    },
    {
      "epoch": 0.1996537269910698,
      "grad_norm": 3.2775306701660156,
      "learning_rate": 4.524139842485454e-05,
      "loss": 2.969,
      "step": 2191
    },
    {
      "epoch": 0.19974485146710405,
      "grad_norm": 1.5939009189605713,
      "learning_rate": 4.523719717662703e-05,
      "loss": 3.0324,
      "step": 2192
    },
    {
      "epoch": 0.19983597594313832,
      "grad_norm": 1.711027979850769,
      "learning_rate": 4.52329942698815e-05,
      "loss": 2.9867,
      "step": 2193
    },
    {
      "epoch": 0.19992710041917258,
      "grad_norm": 2.0007896423339844,
      "learning_rate": 4.5228789704962394e-05,
      "loss": 3.2334,
      "step": 2194
    },
    {
      "epoch": 0.20001822489520685,
      "grad_norm": 2.836336612701416,
      "learning_rate": 4.522458348221429e-05,
      "loss": 3.2113,
      "step": 2195
    },
    {
      "epoch": 0.2001093493712411,
      "grad_norm": 2.093379497528076,
      "learning_rate": 4.522037560198192e-05,
      "loss": 3.1519,
      "step": 2196
    },
    {
      "epoch": 0.20020047384727538,
      "grad_norm": 3.2479476928710938,
      "learning_rate": 4.5216166064610124e-05,
      "loss": 2.8446,
      "step": 2197
    },
    {
      "epoch": 0.20029159832330964,
      "grad_norm": 2.2413997650146484,
      "learning_rate": 4.5211954870443895e-05,
      "loss": 3.3056,
      "step": 2198
    },
    {
      "epoch": 0.2003827227993439,
      "grad_norm": 2.7015161514282227,
      "learning_rate": 4.5207742019828356e-05,
      "loss": 2.6716,
      "step": 2199
    },
    {
      "epoch": 0.20047384727537818,
      "grad_norm": 1.2890421152114868,
      "learning_rate": 4.5203527513108764e-05,
      "loss": 2.9608,
      "step": 2200
    },
    {
      "epoch": 0.20056497175141244,
      "grad_norm": 2.004075765609741,
      "learning_rate": 4.519931135063051e-05,
      "loss": 2.8847,
      "step": 2201
    },
    {
      "epoch": 0.20065609622744668,
      "grad_norm": 3.749248743057251,
      "learning_rate": 4.519509353273914e-05,
      "loss": 2.988,
      "step": 2202
    },
    {
      "epoch": 0.20074722070348094,
      "grad_norm": 2.1400833129882812,
      "learning_rate": 4.519087405978031e-05,
      "loss": 2.9994,
      "step": 2203
    },
    {
      "epoch": 0.2008383451795152,
      "grad_norm": 2.165736675262451,
      "learning_rate": 4.518665293209982e-05,
      "loss": 2.9624,
      "step": 2204
    },
    {
      "epoch": 0.20092946965554948,
      "grad_norm": 2.529449224472046,
      "learning_rate": 4.518243015004362e-05,
      "loss": 3.1092,
      "step": 2205
    },
    {
      "epoch": 0.20102059413158374,
      "grad_norm": 1.4174203872680664,
      "learning_rate": 4.5178205713957763e-05,
      "loss": 2.8858,
      "step": 2206
    },
    {
      "epoch": 0.201111718607618,
      "grad_norm": 2.64379620552063,
      "learning_rate": 4.5173979624188486e-05,
      "loss": 3.0946,
      "step": 2207
    },
    {
      "epoch": 0.20120284308365227,
      "grad_norm": 2.4796440601348877,
      "learning_rate": 4.5169751881082115e-05,
      "loss": 3.1116,
      "step": 2208
    },
    {
      "epoch": 0.20129396755968654,
      "grad_norm": 2.458045721054077,
      "learning_rate": 4.516552248498513e-05,
      "loss": 2.4417,
      "step": 2209
    },
    {
      "epoch": 0.2013850920357208,
      "grad_norm": 4.154749393463135,
      "learning_rate": 4.516129143624416e-05,
      "loss": 2.8651,
      "step": 2210
    },
    {
      "epoch": 0.20147621651175507,
      "grad_norm": 3.4186413288116455,
      "learning_rate": 4.5157058735205937e-05,
      "loss": 2.7451,
      "step": 2211
    },
    {
      "epoch": 0.20156734098778933,
      "grad_norm": 1.4489960670471191,
      "learning_rate": 4.515282438221736e-05,
      "loss": 2.9938,
      "step": 2212
    },
    {
      "epoch": 0.20165846546382357,
      "grad_norm": 3.873000383377075,
      "learning_rate": 4.514858837762546e-05,
      "loss": 3.1432,
      "step": 2213
    },
    {
      "epoch": 0.20174958993985784,
      "grad_norm": 1.6123559474945068,
      "learning_rate": 4.5144350721777365e-05,
      "loss": 2.9208,
      "step": 2214
    },
    {
      "epoch": 0.2018407144158921,
      "grad_norm": 2.759320020675659,
      "learning_rate": 4.514011141502039e-05,
      "loss": 3.2234,
      "step": 2215
    },
    {
      "epoch": 0.20193183889192637,
      "grad_norm": 1.4714075326919556,
      "learning_rate": 4.5135870457701965e-05,
      "loss": 2.935,
      "step": 2216
    },
    {
      "epoch": 0.20202296336796063,
      "grad_norm": 2.7092039585113525,
      "learning_rate": 4.513162785016964e-05,
      "loss": 3.5404,
      "step": 2217
    },
    {
      "epoch": 0.2021140878439949,
      "grad_norm": 1.6720075607299805,
      "learning_rate": 4.5127383592771123e-05,
      "loss": 2.909,
      "step": 2218
    },
    {
      "epoch": 0.20220521232002917,
      "grad_norm": 2.9232418537139893,
      "learning_rate": 4.512313768585424e-05,
      "loss": 3.4335,
      "step": 2219
    },
    {
      "epoch": 0.20229633679606343,
      "grad_norm": 2.317652702331543,
      "learning_rate": 4.5118890129766964e-05,
      "loss": 3.1717,
      "step": 2220
    },
    {
      "epoch": 0.2023874612720977,
      "grad_norm": 3.2075698375701904,
      "learning_rate": 4.511464092485741e-05,
      "loss": 2.902,
      "step": 2221
    },
    {
      "epoch": 0.20247858574813196,
      "grad_norm": 2.143725872039795,
      "learning_rate": 4.51103900714738e-05,
      "loss": 3.1302,
      "step": 2222
    },
    {
      "epoch": 0.2025697102241662,
      "grad_norm": 2.4209516048431396,
      "learning_rate": 4.5106137569964506e-05,
      "loss": 3.0288,
      "step": 2223
    },
    {
      "epoch": 0.20266083470020047,
      "grad_norm": 4.132186412811279,
      "learning_rate": 4.5101883420678055e-05,
      "loss": 4.273,
      "step": 2224
    },
    {
      "epoch": 0.20275195917623473,
      "grad_norm": 1.6558290719985962,
      "learning_rate": 4.509762762396308e-05,
      "loss": 2.9985,
      "step": 2225
    },
    {
      "epoch": 0.202843083652269,
      "grad_norm": 1.6124826669692993,
      "learning_rate": 4.509337018016836e-05,
      "loss": 2.9583,
      "step": 2226
    },
    {
      "epoch": 0.20293420812830326,
      "grad_norm": 2.326113224029541,
      "learning_rate": 4.508911108964281e-05,
      "loss": 3.2059,
      "step": 2227
    },
    {
      "epoch": 0.20302533260433753,
      "grad_norm": 2.63885760307312,
      "learning_rate": 4.508485035273549e-05,
      "loss": 3.0897,
      "step": 2228
    },
    {
      "epoch": 0.2031164570803718,
      "grad_norm": 2.462938070297241,
      "learning_rate": 4.508058796979556e-05,
      "loss": 3.0251,
      "step": 2229
    },
    {
      "epoch": 0.20320758155640606,
      "grad_norm": 2.388350248336792,
      "learning_rate": 4.507632394117237e-05,
      "loss": 3.0929,
      "step": 2230
    },
    {
      "epoch": 0.20329870603244032,
      "grad_norm": 2.301842212677002,
      "learning_rate": 4.5072058267215344e-05,
      "loss": 3.0266,
      "step": 2231
    },
    {
      "epoch": 0.2033898305084746,
      "grad_norm": 2.474675416946411,
      "learning_rate": 4.5067790948274094e-05,
      "loss": 2.8062,
      "step": 2232
    },
    {
      "epoch": 0.20348095498450883,
      "grad_norm": 1.9637845754623413,
      "learning_rate": 4.506352198469833e-05,
      "loss": 3.0707,
      "step": 2233
    },
    {
      "epoch": 0.2035720794605431,
      "grad_norm": 2.1387929916381836,
      "learning_rate": 4.505925137683792e-05,
      "loss": 3.2158,
      "step": 2234
    },
    {
      "epoch": 0.20366320393657736,
      "grad_norm": 2.517397880554199,
      "learning_rate": 4.505497912504284e-05,
      "loss": 2.9909,
      "step": 2235
    },
    {
      "epoch": 0.20375432841261162,
      "grad_norm": 1.2456638813018799,
      "learning_rate": 4.5050705229663245e-05,
      "loss": 2.9118,
      "step": 2236
    },
    {
      "epoch": 0.2038454528886459,
      "grad_norm": 1.2078672647476196,
      "learning_rate": 4.504642969104937e-05,
      "loss": 2.8779,
      "step": 2237
    },
    {
      "epoch": 0.20393657736468015,
      "grad_norm": 2.1220755577087402,
      "learning_rate": 4.5042152509551625e-05,
      "loss": 2.9597,
      "step": 2238
    },
    {
      "epoch": 0.20402770184071442,
      "grad_norm": 1.3070915937423706,
      "learning_rate": 4.5037873685520546e-05,
      "loss": 2.8658,
      "step": 2239
    },
    {
      "epoch": 0.20411882631674869,
      "grad_norm": 3.1724870204925537,
      "learning_rate": 4.50335932193068e-05,
      "loss": 3.0935,
      "step": 2240
    },
    {
      "epoch": 0.20420995079278295,
      "grad_norm": 2.707810401916504,
      "learning_rate": 4.5029311111261175e-05,
      "loss": 2.7991,
      "step": 2241
    },
    {
      "epoch": 0.20430107526881722,
      "grad_norm": 1.5350730419158936,
      "learning_rate": 4.502502736173462e-05,
      "loss": 2.8801,
      "step": 2242
    },
    {
      "epoch": 0.20439219974485145,
      "grad_norm": 1.4328793287277222,
      "learning_rate": 4.50207419710782e-05,
      "loss": 2.7279,
      "step": 2243
    },
    {
      "epoch": 0.20448332422088572,
      "grad_norm": 2.164867401123047,
      "learning_rate": 4.501645493964311e-05,
      "loss": 3.0823,
      "step": 2244
    },
    {
      "epoch": 0.20457444869691999,
      "grad_norm": 1.8175523281097412,
      "learning_rate": 4.50121662677807e-05,
      "loss": 2.8453,
      "step": 2245
    },
    {
      "epoch": 0.20466557317295425,
      "grad_norm": 2.747492551803589,
      "learning_rate": 4.500787595584245e-05,
      "loss": 3.1991,
      "step": 2246
    },
    {
      "epoch": 0.20475669764898852,
      "grad_norm": 2.0230212211608887,
      "learning_rate": 4.500358400417995e-05,
      "loss": 3.1672,
      "step": 2247
    },
    {
      "epoch": 0.20484782212502278,
      "grad_norm": 1.1466317176818848,
      "learning_rate": 4.499929041314496e-05,
      "loss": 2.8377,
      "step": 2248
    },
    {
      "epoch": 0.20493894660105705,
      "grad_norm": 2.911933422088623,
      "learning_rate": 4.499499518308935e-05,
      "loss": 3.2225,
      "step": 2249
    },
    {
      "epoch": 0.2050300710770913,
      "grad_norm": 3.3606438636779785,
      "learning_rate": 4.499069831436512e-05,
      "loss": 2.9038,
      "step": 2250
    },
    {
      "epoch": 0.20512119555312558,
      "grad_norm": 2.6701643466949463,
      "learning_rate": 4.498639980732443e-05,
      "loss": 2.7429,
      "step": 2251
    },
    {
      "epoch": 0.20521232002915984,
      "grad_norm": 1.3472394943237305,
      "learning_rate": 4.4982099662319566e-05,
      "loss": 2.9459,
      "step": 2252
    },
    {
      "epoch": 0.20530344450519408,
      "grad_norm": 1.8871601819992065,
      "learning_rate": 4.497779787970292e-05,
      "loss": 3.087,
      "step": 2253
    },
    {
      "epoch": 0.20539456898122835,
      "grad_norm": 2.2974672317504883,
      "learning_rate": 4.497349445982705e-05,
      "loss": 2.9756,
      "step": 2254
    },
    {
      "epoch": 0.2054856934572626,
      "grad_norm": 3.0636179447174072,
      "learning_rate": 4.496918940304464e-05,
      "loss": 3.1726,
      "step": 2255
    },
    {
      "epoch": 0.20557681793329688,
      "grad_norm": 1.4282572269439697,
      "learning_rate": 4.4964882709708515e-05,
      "loss": 3.0755,
      "step": 2256
    },
    {
      "epoch": 0.20566794240933114,
      "grad_norm": 1.5187928676605225,
      "learning_rate": 4.49605743801716e-05,
      "loss": 2.9274,
      "step": 2257
    },
    {
      "epoch": 0.2057590668853654,
      "grad_norm": 3.113553285598755,
      "learning_rate": 4.495626441478701e-05,
      "loss": 3.3544,
      "step": 2258
    },
    {
      "epoch": 0.20585019136139968,
      "grad_norm": 2.18365216255188,
      "learning_rate": 4.4951952813907936e-05,
      "loss": 2.7555,
      "step": 2259
    },
    {
      "epoch": 0.20594131583743394,
      "grad_norm": 3.3173763751983643,
      "learning_rate": 4.4947639577887744e-05,
      "loss": 3.09,
      "step": 2260
    },
    {
      "epoch": 0.2060324403134682,
      "grad_norm": 2.2813990116119385,
      "learning_rate": 4.494332470707993e-05,
      "loss": 3.1145,
      "step": 2261
    },
    {
      "epoch": 0.20612356478950247,
      "grad_norm": 2.0486700534820557,
      "learning_rate": 4.49390082018381e-05,
      "loss": 3.1493,
      "step": 2262
    },
    {
      "epoch": 0.2062146892655367,
      "grad_norm": 2.9934332370758057,
      "learning_rate": 4.493469006251601e-05,
      "loss": 4.1186,
      "step": 2263
    },
    {
      "epoch": 0.20630581374157098,
      "grad_norm": 2.824861764907837,
      "learning_rate": 4.493037028946756e-05,
      "loss": 2.5097,
      "step": 2264
    },
    {
      "epoch": 0.20639693821760524,
      "grad_norm": 3.2467503547668457,
      "learning_rate": 4.492604888304676e-05,
      "loss": 2.9636,
      "step": 2265
    },
    {
      "epoch": 0.2064880626936395,
      "grad_norm": 2.205721378326416,
      "learning_rate": 4.492172584360777e-05,
      "loss": 2.9955,
      "step": 2266
    },
    {
      "epoch": 0.20657918716967377,
      "grad_norm": 1.6623010635375977,
      "learning_rate": 4.491740117150488e-05,
      "loss": 2.9664,
      "step": 2267
    },
    {
      "epoch": 0.20667031164570804,
      "grad_norm": 2.7729270458221436,
      "learning_rate": 4.491307486709252e-05,
      "loss": 3.1816,
      "step": 2268
    },
    {
      "epoch": 0.2067614361217423,
      "grad_norm": 2.163179874420166,
      "learning_rate": 4.490874693072524e-05,
      "loss": 2.9595,
      "step": 2269
    },
    {
      "epoch": 0.20685256059777657,
      "grad_norm": 2.662590742111206,
      "learning_rate": 4.490441736275773e-05,
      "loss": 2.8192,
      "step": 2270
    },
    {
      "epoch": 0.20694368507381083,
      "grad_norm": 5.389655590057373,
      "learning_rate": 4.4900086163544826e-05,
      "loss": 3.1426,
      "step": 2271
    },
    {
      "epoch": 0.2070348095498451,
      "grad_norm": 3.1567933559417725,
      "learning_rate": 4.4895753333441475e-05,
      "loss": 3.4748,
      "step": 2272
    },
    {
      "epoch": 0.20712593402587934,
      "grad_norm": 3.086310386657715,
      "learning_rate": 4.4891418872802784e-05,
      "loss": 2.7779,
      "step": 2273
    },
    {
      "epoch": 0.2072170585019136,
      "grad_norm": 1.493032455444336,
      "learning_rate": 4.488708278198396e-05,
      "loss": 2.9835,
      "step": 2274
    },
    {
      "epoch": 0.20730818297794787,
      "grad_norm": 1.9787254333496094,
      "learning_rate": 4.488274506134037e-05,
      "loss": 2.891,
      "step": 2275
    },
    {
      "epoch": 0.20739930745398213,
      "grad_norm": 2.837303876876831,
      "learning_rate": 4.487840571122751e-05,
      "loss": 2.9514,
      "step": 2276
    },
    {
      "epoch": 0.2074904319300164,
      "grad_norm": 1.8044532537460327,
      "learning_rate": 4.487406473200101e-05,
      "loss": 2.9843,
      "step": 2277
    },
    {
      "epoch": 0.20758155640605067,
      "grad_norm": 1.2358680963516235,
      "learning_rate": 4.486972212401663e-05,
      "loss": 2.9404,
      "step": 2278
    },
    {
      "epoch": 0.20767268088208493,
      "grad_norm": 2.6474313735961914,
      "learning_rate": 4.486537788763026e-05,
      "loss": 3.1568,
      "step": 2279
    },
    {
      "epoch": 0.2077638053581192,
      "grad_norm": 2.4286749362945557,
      "learning_rate": 4.486103202319792e-05,
      "loss": 3.9513,
      "step": 2280
    },
    {
      "epoch": 0.20785492983415346,
      "grad_norm": 1.2719675302505493,
      "learning_rate": 4.4856684531075783e-05,
      "loss": 2.9995,
      "step": 2281
    },
    {
      "epoch": 0.20794605431018773,
      "grad_norm": 2.0894854068756104,
      "learning_rate": 4.485233541162014e-05,
      "loss": 3.1503,
      "step": 2282
    },
    {
      "epoch": 0.20803717878622197,
      "grad_norm": 2.5427334308624268,
      "learning_rate": 4.484798466518741e-05,
      "loss": 3.1696,
      "step": 2283
    },
    {
      "epoch": 0.20812830326225623,
      "grad_norm": 2.574201822280884,
      "learning_rate": 4.4843632292134175e-05,
      "loss": 3.04,
      "step": 2284
    },
    {
      "epoch": 0.2082194277382905,
      "grad_norm": 3.7122645378112793,
      "learning_rate": 4.4839278292817104e-05,
      "loss": 3.4177,
      "step": 2285
    },
    {
      "epoch": 0.20831055221432476,
      "grad_norm": 4.527674674987793,
      "learning_rate": 4.4834922667593036e-05,
      "loss": 3.07,
      "step": 2286
    },
    {
      "epoch": 0.20840167669035903,
      "grad_norm": 2.1080739498138428,
      "learning_rate": 4.483056541681894e-05,
      "loss": 2.8474,
      "step": 2287
    },
    {
      "epoch": 0.2084928011663933,
      "grad_norm": 1.369333028793335,
      "learning_rate": 4.4826206540851886e-05,
      "loss": 3.0055,
      "step": 2288
    },
    {
      "epoch": 0.20858392564242756,
      "grad_norm": 2.6662442684173584,
      "learning_rate": 4.482184604004913e-05,
      "loss": 3.2086,
      "step": 2289
    },
    {
      "epoch": 0.20867505011846182,
      "grad_norm": 3.645087242126465,
      "learning_rate": 4.481748391476801e-05,
      "loss": 2.9404,
      "step": 2290
    },
    {
      "epoch": 0.2087661745944961,
      "grad_norm": 2.58284854888916,
      "learning_rate": 4.4813120165366026e-05,
      "loss": 3.2119,
      "step": 2291
    },
    {
      "epoch": 0.20885729907053036,
      "grad_norm": 1.8313319683074951,
      "learning_rate": 4.4808754792200816e-05,
      "loss": 3.0524,
      "step": 2292
    },
    {
      "epoch": 0.2089484235465646,
      "grad_norm": 3.230278968811035,
      "learning_rate": 4.480438779563012e-05,
      "loss": 2.9928,
      "step": 2293
    },
    {
      "epoch": 0.20903954802259886,
      "grad_norm": 1.9250975847244263,
      "learning_rate": 4.480001917601185e-05,
      "loss": 3.168,
      "step": 2294
    },
    {
      "epoch": 0.20913067249863312,
      "grad_norm": 1.4513882398605347,
      "learning_rate": 4.4795648933704015e-05,
      "loss": 2.976,
      "step": 2295
    },
    {
      "epoch": 0.2092217969746674,
      "grad_norm": 2.85551381111145,
      "learning_rate": 4.479127706906478e-05,
      "loss": 3.014,
      "step": 2296
    },
    {
      "epoch": 0.20931292145070166,
      "grad_norm": 2.1301825046539307,
      "learning_rate": 4.478690358245244e-05,
      "loss": 3.1183,
      "step": 2297
    },
    {
      "epoch": 0.20940404592673592,
      "grad_norm": 2.990741491317749,
      "learning_rate": 4.478252847422541e-05,
      "loss": 3.177,
      "step": 2298
    },
    {
      "epoch": 0.2094951704027702,
      "grad_norm": 2.3298990726470947,
      "learning_rate": 4.477815174474226e-05,
      "loss": 3.3197,
      "step": 2299
    },
    {
      "epoch": 0.20958629487880445,
      "grad_norm": 1.3285237550735474,
      "learning_rate": 4.4773773394361675e-05,
      "loss": 3.0223,
      "step": 2300
    },
    {
      "epoch": 0.20967741935483872,
      "grad_norm": 1.1576237678527832,
      "learning_rate": 4.476939342344246e-05,
      "loss": 2.8808,
      "step": 2301
    },
    {
      "epoch": 0.20976854383087298,
      "grad_norm": 3.019843578338623,
      "learning_rate": 4.4765011832343606e-05,
      "loss": 4.2625,
      "step": 2302
    },
    {
      "epoch": 0.20985966830690725,
      "grad_norm": 2.7779412269592285,
      "learning_rate": 4.4760628621424187e-05,
      "loss": 3.4649,
      "step": 2303
    },
    {
      "epoch": 0.2099507927829415,
      "grad_norm": 1.8121966123580933,
      "learning_rate": 4.4756243791043407e-05,
      "loss": 3.2548,
      "step": 2304
    },
    {
      "epoch": 0.21004191725897575,
      "grad_norm": 2.571568727493286,
      "learning_rate": 4.475185734156064e-05,
      "loss": 3.182,
      "step": 2305
    },
    {
      "epoch": 0.21013304173501002,
      "grad_norm": 2.2391602993011475,
      "learning_rate": 4.4747469273335364e-05,
      "loss": 2.9973,
      "step": 2306
    },
    {
      "epoch": 0.21022416621104428,
      "grad_norm": 1.2395771741867065,
      "learning_rate": 4.47430795867272e-05,
      "loss": 2.8282,
      "step": 2307
    },
    {
      "epoch": 0.21031529068707855,
      "grad_norm": 2.73646879196167,
      "learning_rate": 4.4738688282095907e-05,
      "loss": 3.1172,
      "step": 2308
    },
    {
      "epoch": 0.21040641516311281,
      "grad_norm": 1.5736783742904663,
      "learning_rate": 4.473429535980136e-05,
      "loss": 3.0045,
      "step": 2309
    },
    {
      "epoch": 0.21049753963914708,
      "grad_norm": 1.7237792015075684,
      "learning_rate": 4.4729900820203585e-05,
      "loss": 3.0082,
      "step": 2310
    },
    {
      "epoch": 0.21058866411518135,
      "grad_norm": 2.2953896522521973,
      "learning_rate": 4.472550466366272e-05,
      "loss": 4.1645,
      "step": 2311
    },
    {
      "epoch": 0.2106797885912156,
      "grad_norm": 1.3848017454147339,
      "learning_rate": 4.472110689053906e-05,
      "loss": 2.883,
      "step": 2312
    },
    {
      "epoch": 0.21077091306724988,
      "grad_norm": 1.168258547782898,
      "learning_rate": 4.471670750119301e-05,
      "loss": 2.8631,
      "step": 2313
    },
    {
      "epoch": 0.21086203754328411,
      "grad_norm": 4.12980842590332,
      "learning_rate": 4.471230649598512e-05,
      "loss": 2.8919,
      "step": 2314
    },
    {
      "epoch": 0.21095316201931838,
      "grad_norm": 3.132011890411377,
      "learning_rate": 4.470790387527608e-05,
      "loss": 2.9238,
      "step": 2315
    },
    {
      "epoch": 0.21104428649535265,
      "grad_norm": 1.8719621896743774,
      "learning_rate": 4.470349963942669e-05,
      "loss": 2.8632,
      "step": 2316
    },
    {
      "epoch": 0.2111354109713869,
      "grad_norm": 2.455930709838867,
      "learning_rate": 4.4699093788797895e-05,
      "loss": 3.1226,
      "step": 2317
    },
    {
      "epoch": 0.21122653544742118,
      "grad_norm": 2.128415107727051,
      "learning_rate": 4.469468632375078e-05,
      "loss": 2.5881,
      "step": 2318
    },
    {
      "epoch": 0.21131765992345544,
      "grad_norm": 2.1486282348632812,
      "learning_rate": 4.469027724464654e-05,
      "loss": 3.0125,
      "step": 2319
    },
    {
      "epoch": 0.2114087843994897,
      "grad_norm": 1.8812031745910645,
      "learning_rate": 4.468586655184653e-05,
      "loss": 2.878,
      "step": 2320
    },
    {
      "epoch": 0.21149990887552397,
      "grad_norm": 1.9769287109375,
      "learning_rate": 4.468145424571222e-05,
      "loss": 3.0001,
      "step": 2321
    },
    {
      "epoch": 0.21159103335155824,
      "grad_norm": 1.320775032043457,
      "learning_rate": 4.4677040326605214e-05,
      "loss": 2.9153,
      "step": 2322
    },
    {
      "epoch": 0.2116821578275925,
      "grad_norm": 2.5441370010375977,
      "learning_rate": 4.467262479488725e-05,
      "loss": 3.0172,
      "step": 2323
    },
    {
      "epoch": 0.21177328230362674,
      "grad_norm": 2.2505064010620117,
      "learning_rate": 4.46682076509202e-05,
      "loss": 2.7924,
      "step": 2324
    },
    {
      "epoch": 0.211864406779661,
      "grad_norm": 1.8098341226577759,
      "learning_rate": 4.466378889506607e-05,
      "loss": 2.9512,
      "step": 2325
    },
    {
      "epoch": 0.21195553125569527,
      "grad_norm": 1.8731929063796997,
      "learning_rate": 4.465936852768698e-05,
      "loss": 3.0109,
      "step": 2326
    },
    {
      "epoch": 0.21204665573172954,
      "grad_norm": 2.007005453109741,
      "learning_rate": 4.465494654914521e-05,
      "loss": 3.1843,
      "step": 2327
    },
    {
      "epoch": 0.2121377802077638,
      "grad_norm": 2.4287471771240234,
      "learning_rate": 4.4650522959803155e-05,
      "loss": 2.9751,
      "step": 2328
    },
    {
      "epoch": 0.21222890468379807,
      "grad_norm": 1.5993821620941162,
      "learning_rate": 4.464609776002335e-05,
      "loss": 2.9039,
      "step": 2329
    },
    {
      "epoch": 0.21232002915983234,
      "grad_norm": 2.834073066711426,
      "learning_rate": 4.4641670950168446e-05,
      "loss": 2.9009,
      "step": 2330
    },
    {
      "epoch": 0.2124111536358666,
      "grad_norm": 2.2215964794158936,
      "learning_rate": 4.463724253060125e-05,
      "loss": 3.0011,
      "step": 2331
    },
    {
      "epoch": 0.21250227811190087,
      "grad_norm": 3.293508768081665,
      "learning_rate": 4.4632812501684676e-05,
      "loss": 2.821,
      "step": 2332
    },
    {
      "epoch": 0.21259340258793513,
      "grad_norm": 2.709357976913452,
      "learning_rate": 4.462838086378179e-05,
      "loss": 3.0262,
      "step": 2333
    },
    {
      "epoch": 0.21268452706396937,
      "grad_norm": 2.1736879348754883,
      "learning_rate": 4.462394761725579e-05,
      "loss": 3.0597,
      "step": 2334
    },
    {
      "epoch": 0.21277565154000364,
      "grad_norm": 2.1975202560424805,
      "learning_rate": 4.4619512762469984e-05,
      "loss": 2.7918,
      "step": 2335
    },
    {
      "epoch": 0.2128667760160379,
      "grad_norm": 2.389641761779785,
      "learning_rate": 4.461507629978784e-05,
      "loss": 2.8069,
      "step": 2336
    },
    {
      "epoch": 0.21295790049207217,
      "grad_norm": 3.0154576301574707,
      "learning_rate": 4.461063822957292e-05,
      "loss": 3.0759,
      "step": 2337
    },
    {
      "epoch": 0.21304902496810643,
      "grad_norm": 2.783113956451416,
      "learning_rate": 4.460619855218896e-05,
      "loss": 2.6313,
      "step": 2338
    },
    {
      "epoch": 0.2131401494441407,
      "grad_norm": 2.3460493087768555,
      "learning_rate": 4.460175726799981e-05,
      "loss": 3.037,
      "step": 2339
    },
    {
      "epoch": 0.21323127392017496,
      "grad_norm": 1.4628417491912842,
      "learning_rate": 4.459731437736945e-05,
      "loss": 2.9482,
      "step": 2340
    },
    {
      "epoch": 0.21332239839620923,
      "grad_norm": 2.432687997817993,
      "learning_rate": 4.459286988066198e-05,
      "loss": 3.1152,
      "step": 2341
    },
    {
      "epoch": 0.2134135228722435,
      "grad_norm": 2.1010987758636475,
      "learning_rate": 4.458842377824165e-05,
      "loss": 2.972,
      "step": 2342
    },
    {
      "epoch": 0.21350464734827776,
      "grad_norm": 1.9727678298950195,
      "learning_rate": 4.458397607047284e-05,
      "loss": 2.8277,
      "step": 2343
    },
    {
      "epoch": 0.213595771824312,
      "grad_norm": 2.567218780517578,
      "learning_rate": 4.4579526757720065e-05,
      "loss": 2.9196,
      "step": 2344
    },
    {
      "epoch": 0.21368689630034626,
      "grad_norm": 3.42586612701416,
      "learning_rate": 4.4575075840347944e-05,
      "loss": 2.7328,
      "step": 2345
    },
    {
      "epoch": 0.21377802077638053,
      "grad_norm": 1.4443062543869019,
      "learning_rate": 4.457062331872126e-05,
      "loss": 2.9716,
      "step": 2346
    },
    {
      "epoch": 0.2138691452524148,
      "grad_norm": 1.795553207397461,
      "learning_rate": 4.456616919320491e-05,
      "loss": 3.1126,
      "step": 2347
    },
    {
      "epoch": 0.21396026972844906,
      "grad_norm": 2.1759533882141113,
      "learning_rate": 4.456171346416394e-05,
      "loss": 3.0079,
      "step": 2348
    },
    {
      "epoch": 0.21405139420448333,
      "grad_norm": 2.5547616481781006,
      "learning_rate": 4.4557256131963485e-05,
      "loss": 3.1405,
      "step": 2349
    },
    {
      "epoch": 0.2141425186805176,
      "grad_norm": 1.178818702697754,
      "learning_rate": 4.455279719696887e-05,
      "loss": 2.9067,
      "step": 2350
    },
    {
      "epoch": 0.21423364315655186,
      "grad_norm": 2.3029165267944336,
      "learning_rate": 4.454833665954551e-05,
      "loss": 3.0012,
      "step": 2351
    },
    {
      "epoch": 0.21432476763258612,
      "grad_norm": 4.126332759857178,
      "learning_rate": 4.4543874520058966e-05,
      "loss": 2.9246,
      "step": 2352
    },
    {
      "epoch": 0.2144158921086204,
      "grad_norm": 2.4644579887390137,
      "learning_rate": 4.4539410778874927e-05,
      "loss": 3.0327,
      "step": 2353
    },
    {
      "epoch": 0.21450701658465463,
      "grad_norm": 1.391380786895752,
      "learning_rate": 4.453494543635921e-05,
      "loss": 2.9039,
      "step": 2354
    },
    {
      "epoch": 0.2145981410606889,
      "grad_norm": 1.797414779663086,
      "learning_rate": 4.4530478492877775e-05,
      "loss": 3.04,
      "step": 2355
    },
    {
      "epoch": 0.21468926553672316,
      "grad_norm": 1.274500846862793,
      "learning_rate": 4.4526009948796703e-05,
      "loss": 2.836,
      "step": 2356
    },
    {
      "epoch": 0.21478039001275742,
      "grad_norm": 1.2052398920059204,
      "learning_rate": 4.4521539804482204e-05,
      "loss": 2.7891,
      "step": 2357
    },
    {
      "epoch": 0.2148715144887917,
      "grad_norm": 2.0563175678253174,
      "learning_rate": 4.4517068060300635e-05,
      "loss": 3.1009,
      "step": 2358
    },
    {
      "epoch": 0.21496263896482595,
      "grad_norm": 3.0770535469055176,
      "learning_rate": 4.4512594716618464e-05,
      "loss": 3.0318,
      "step": 2359
    },
    {
      "epoch": 0.21505376344086022,
      "grad_norm": 2.2249608039855957,
      "learning_rate": 4.45081197738023e-05,
      "loss": 3.3546,
      "step": 2360
    },
    {
      "epoch": 0.21514488791689448,
      "grad_norm": 1.3697552680969238,
      "learning_rate": 4.450364323221888e-05,
      "loss": 2.9065,
      "step": 2361
    },
    {
      "epoch": 0.21523601239292875,
      "grad_norm": 2.6643457412719727,
      "learning_rate": 4.4499165092235085e-05,
      "loss": 3.1334,
      "step": 2362
    },
    {
      "epoch": 0.21532713686896301,
      "grad_norm": 2.8277461528778076,
      "learning_rate": 4.44946853542179e-05,
      "loss": 2.8224,
      "step": 2363
    },
    {
      "epoch": 0.21541826134499725,
      "grad_norm": 1.200529932975769,
      "learning_rate": 4.449020401853448e-05,
      "loss": 2.6934,
      "step": 2364
    },
    {
      "epoch": 0.21550938582103152,
      "grad_norm": 2.4886698722839355,
      "learning_rate": 4.448572108555206e-05,
      "loss": 3.1776,
      "step": 2365
    },
    {
      "epoch": 0.21560051029706578,
      "grad_norm": 1.397831678390503,
      "learning_rate": 4.448123655563805e-05,
      "loss": 2.9164,
      "step": 2366
    },
    {
      "epoch": 0.21569163477310005,
      "grad_norm": 1.8870694637298584,
      "learning_rate": 4.4476750429159974e-05,
      "loss": 2.9102,
      "step": 2367
    },
    {
      "epoch": 0.21578275924913431,
      "grad_norm": 2.809436559677124,
      "learning_rate": 4.447226270648549e-05,
      "loss": 3.2628,
      "step": 2368
    },
    {
      "epoch": 0.21587388372516858,
      "grad_norm": 1.847671627998352,
      "learning_rate": 4.446777338798238e-05,
      "loss": 2.8204,
      "step": 2369
    },
    {
      "epoch": 0.21596500820120285,
      "grad_norm": 3.1055705547332764,
      "learning_rate": 4.4463282474018566e-05,
      "loss": 3.9135,
      "step": 2370
    },
    {
      "epoch": 0.2160561326772371,
      "grad_norm": 2.851072072982788,
      "learning_rate": 4.445878996496209e-05,
      "loss": 2.8355,
      "step": 2371
    },
    {
      "epoch": 0.21614725715327138,
      "grad_norm": 1.5610626935958862,
      "learning_rate": 4.445429586118113e-05,
      "loss": 2.9915,
      "step": 2372
    },
    {
      "epoch": 0.21623838162930564,
      "grad_norm": 2.853407144546509,
      "learning_rate": 4.4449800163044e-05,
      "loss": 3.2007,
      "step": 2373
    },
    {
      "epoch": 0.21632950610533988,
      "grad_norm": 2.201547145843506,
      "learning_rate": 4.444530287091914e-05,
      "loss": 3.0788,
      "step": 2374
    },
    {
      "epoch": 0.21642063058137415,
      "grad_norm": 2.071911334991455,
      "learning_rate": 4.444080398517512e-05,
      "loss": 3.1313,
      "step": 2375
    },
    {
      "epoch": 0.2165117550574084,
      "grad_norm": 1.3935573101043701,
      "learning_rate": 4.443630350618064e-05,
      "loss": 3.0167,
      "step": 2376
    },
    {
      "epoch": 0.21660287953344268,
      "grad_norm": 2.38028621673584,
      "learning_rate": 4.443180143430454e-05,
      "loss": 3.116,
      "step": 2377
    },
    {
      "epoch": 0.21669400400947694,
      "grad_norm": 2.42590594291687,
      "learning_rate": 4.4427297769915774e-05,
      "loss": 3.193,
      "step": 2378
    },
    {
      "epoch": 0.2167851284855112,
      "grad_norm": 2.9374406337738037,
      "learning_rate": 4.442279251338344e-05,
      "loss": 3.133,
      "step": 2379
    },
    {
      "epoch": 0.21687625296154547,
      "grad_norm": 3.1376900672912598,
      "learning_rate": 4.4418285665076745e-05,
      "loss": 4.1633,
      "step": 2380
    },
    {
      "epoch": 0.21696737743757974,
      "grad_norm": 2.542917490005493,
      "learning_rate": 4.4413777225365064e-05,
      "loss": 3.1028,
      "step": 2381
    },
    {
      "epoch": 0.217058501913614,
      "grad_norm": 2.9858739376068115,
      "learning_rate": 4.440926719461788e-05,
      "loss": 2.6611,
      "step": 2382
    },
    {
      "epoch": 0.21714962638964827,
      "grad_norm": 2.852911949157715,
      "learning_rate": 4.4404755573204796e-05,
      "loss": 2.9755,
      "step": 2383
    },
    {
      "epoch": 0.21724075086568254,
      "grad_norm": 2.381763219833374,
      "learning_rate": 4.440024236149556e-05,
      "loss": 2.7611,
      "step": 2384
    },
    {
      "epoch": 0.21733187534171677,
      "grad_norm": 1.8644306659698486,
      "learning_rate": 4.439572755986005e-05,
      "loss": 3.0091,
      "step": 2385
    },
    {
      "epoch": 0.21742299981775104,
      "grad_norm": 1.7574383020401,
      "learning_rate": 4.439121116866828e-05,
      "loss": 3.0304,
      "step": 2386
    },
    {
      "epoch": 0.2175141242937853,
      "grad_norm": 1.2105717658996582,
      "learning_rate": 4.4386693188290376e-05,
      "loss": 2.8737,
      "step": 2387
    },
    {
      "epoch": 0.21760524876981957,
      "grad_norm": 3.208273410797119,
      "learning_rate": 4.43821736190966e-05,
      "loss": 4.3837,
      "step": 2388
    },
    {
      "epoch": 0.21769637324585384,
      "grad_norm": 2.6459734439849854,
      "learning_rate": 4.437765246145736e-05,
      "loss": 3.0164,
      "step": 2389
    },
    {
      "epoch": 0.2177874977218881,
      "grad_norm": 2.7467236518859863,
      "learning_rate": 4.4373129715743176e-05,
      "loss": 2.985,
      "step": 2390
    },
    {
      "epoch": 0.21787862219792237,
      "grad_norm": 2.3378372192382812,
      "learning_rate": 4.43686053823247e-05,
      "loss": 2.9114,
      "step": 2391
    },
    {
      "epoch": 0.21796974667395663,
      "grad_norm": 2.857571840286255,
      "learning_rate": 4.4364079461572736e-05,
      "loss": 2.9127,
      "step": 2392
    },
    {
      "epoch": 0.2180608711499909,
      "grad_norm": 2.1076879501342773,
      "learning_rate": 4.435955195385819e-05,
      "loss": 3.2161,
      "step": 2393
    },
    {
      "epoch": 0.21815199562602516,
      "grad_norm": 2.024644136428833,
      "learning_rate": 4.4355022859552106e-05,
      "loss": 2.6958,
      "step": 2394
    },
    {
      "epoch": 0.2182431201020594,
      "grad_norm": 2.0251107215881348,
      "learning_rate": 4.435049217902566e-05,
      "loss": 3.0752,
      "step": 2395
    },
    {
      "epoch": 0.21833424457809367,
      "grad_norm": 3.4016671180725098,
      "learning_rate": 4.434595991265017e-05,
      "loss": 3.0427,
      "step": 2396
    },
    {
      "epoch": 0.21842536905412793,
      "grad_norm": 1.9261730909347534,
      "learning_rate": 4.4341426060797064e-05,
      "loss": 2.8586,
      "step": 2397
    },
    {
      "epoch": 0.2185164935301622,
      "grad_norm": 2.5371387004852295,
      "learning_rate": 4.4336890623837914e-05,
      "loss": 2.9971,
      "step": 2398
    },
    {
      "epoch": 0.21860761800619646,
      "grad_norm": 2.2000441551208496,
      "learning_rate": 4.433235360214441e-05,
      "loss": 3.0667,
      "step": 2399
    },
    {
      "epoch": 0.21869874248223073,
      "grad_norm": 2.305135726928711,
      "learning_rate": 4.432781499608839e-05,
      "loss": 3.1966,
      "step": 2400
    },
    {
      "epoch": 0.218789866958265,
      "grad_norm": 2.110811471939087,
      "learning_rate": 4.4323274806041796e-05,
      "loss": 3.3502,
      "step": 2401
    },
    {
      "epoch": 0.21888099143429926,
      "grad_norm": 2.1586108207702637,
      "learning_rate": 4.4318733032376726e-05,
      "loss": 2.9409,
      "step": 2402
    },
    {
      "epoch": 0.21897211591033353,
      "grad_norm": 3.0279078483581543,
      "learning_rate": 4.4314189675465394e-05,
      "loss": 3.0895,
      "step": 2403
    },
    {
      "epoch": 0.2190632403863678,
      "grad_norm": 2.163571834564209,
      "learning_rate": 4.4309644735680147e-05,
      "loss": 3.4089,
      "step": 2404
    },
    {
      "epoch": 0.21915436486240203,
      "grad_norm": 2.204313039779663,
      "learning_rate": 4.4305098213393456e-05,
      "loss": 3.0158,
      "step": 2405
    },
    {
      "epoch": 0.2192454893384363,
      "grad_norm": 3.005061388015747,
      "learning_rate": 4.430055010897793e-05,
      "loss": 4.1169,
      "step": 2406
    },
    {
      "epoch": 0.21933661381447056,
      "grad_norm": 2.5847089290618896,
      "learning_rate": 4.4296000422806305e-05,
      "loss": 3.0994,
      "step": 2407
    },
    {
      "epoch": 0.21942773829050483,
      "grad_norm": 1.8464537858963013,
      "learning_rate": 4.429144915525144e-05,
      "loss": 3.1682,
      "step": 2408
    },
    {
      "epoch": 0.2195188627665391,
      "grad_norm": 2.6065828800201416,
      "learning_rate": 4.428689630668633e-05,
      "loss": 2.9299,
      "step": 2409
    },
    {
      "epoch": 0.21960998724257336,
      "grad_norm": 1.1873589754104614,
      "learning_rate": 4.428234187748411e-05,
      "loss": 2.9251,
      "step": 2410
    },
    {
      "epoch": 0.21970111171860762,
      "grad_norm": 4.586496829986572,
      "learning_rate": 4.4277785868018016e-05,
      "loss": 2.8373,
      "step": 2411
    },
    {
      "epoch": 0.2197922361946419,
      "grad_norm": 1.9164390563964844,
      "learning_rate": 4.427322827866145e-05,
      "loss": 3.1087,
      "step": 2412
    },
    {
      "epoch": 0.21988336067067615,
      "grad_norm": 2.6399269104003906,
      "learning_rate": 4.4268669109787905e-05,
      "loss": 3.2971,
      "step": 2413
    },
    {
      "epoch": 0.21997448514671042,
      "grad_norm": 3.314610242843628,
      "learning_rate": 4.426410836177104e-05,
      "loss": 3.2885,
      "step": 2414
    },
    {
      "epoch": 0.22006560962274466,
      "grad_norm": 2.759746551513672,
      "learning_rate": 4.425954603498461e-05,
      "loss": 3.0251,
      "step": 2415
    },
    {
      "epoch": 0.22015673409877892,
      "grad_norm": 2.5431792736053467,
      "learning_rate": 4.4254982129802534e-05,
      "loss": 3.1836,
      "step": 2416
    },
    {
      "epoch": 0.2202478585748132,
      "grad_norm": 1.676533818244934,
      "learning_rate": 4.425041664659883e-05,
      "loss": 2.941,
      "step": 2417
    },
    {
      "epoch": 0.22033898305084745,
      "grad_norm": 2.6223435401916504,
      "learning_rate": 4.4245849585747654e-05,
      "loss": 2.7822,
      "step": 2418
    },
    {
      "epoch": 0.22043010752688172,
      "grad_norm": 1.95738685131073,
      "learning_rate": 4.424128094762331e-05,
      "loss": 2.886,
      "step": 2419
    },
    {
      "epoch": 0.22052123200291598,
      "grad_norm": 1.7090452909469604,
      "learning_rate": 4.42367107326002e-05,
      "loss": 2.9412,
      "step": 2420
    },
    {
      "epoch": 0.22061235647895025,
      "grad_norm": 1.4197723865509033,
      "learning_rate": 4.423213894105288e-05,
      "loss": 2.9158,
      "step": 2421
    },
    {
      "epoch": 0.22070348095498452,
      "grad_norm": 2.263172149658203,
      "learning_rate": 4.422756557335602e-05,
      "loss": 3.1257,
      "step": 2422
    },
    {
      "epoch": 0.22079460543101878,
      "grad_norm": 1.331345558166504,
      "learning_rate": 4.422299062988443e-05,
      "loss": 3.0775,
      "step": 2423
    },
    {
      "epoch": 0.22088572990705305,
      "grad_norm": 1.4074492454528809,
      "learning_rate": 4.4218414111013055e-05,
      "loss": 2.942,
      "step": 2424
    },
    {
      "epoch": 0.22097685438308728,
      "grad_norm": 2.6391265392303467,
      "learning_rate": 4.421383601711693e-05,
      "loss": 2.8554,
      "step": 2425
    },
    {
      "epoch": 0.22106797885912155,
      "grad_norm": 3.6921215057373047,
      "learning_rate": 4.420925634857127e-05,
      "loss": 2.5869,
      "step": 2426
    },
    {
      "epoch": 0.22115910333515582,
      "grad_norm": 2.513110399246216,
      "learning_rate": 4.42046751057514e-05,
      "loss": 3.1551,
      "step": 2427
    },
    {
      "epoch": 0.22125022781119008,
      "grad_norm": 1.3886610269546509,
      "learning_rate": 4.4200092289032756e-05,
      "loss": 2.8215,
      "step": 2428
    },
    {
      "epoch": 0.22134135228722435,
      "grad_norm": 4.480032920837402,
      "learning_rate": 4.419550789879093e-05,
      "loss": 2.6537,
      "step": 2429
    },
    {
      "epoch": 0.2214324767632586,
      "grad_norm": 2.4046430587768555,
      "learning_rate": 4.419092193540162e-05,
      "loss": 3.1313,
      "step": 2430
    },
    {
      "epoch": 0.22152360123929288,
      "grad_norm": 3.0777454376220703,
      "learning_rate": 4.418633439924067e-05,
      "loss": 2.7984,
      "step": 2431
    },
    {
      "epoch": 0.22161472571532714,
      "grad_norm": 2.149569034576416,
      "learning_rate": 4.418174529068405e-05,
      "loss": 2.9988,
      "step": 2432
    },
    {
      "epoch": 0.2217058501913614,
      "grad_norm": 2.744971513748169,
      "learning_rate": 4.417715461010785e-05,
      "loss": 3.0486,
      "step": 2433
    },
    {
      "epoch": 0.22179697466739567,
      "grad_norm": 2.3862879276275635,
      "learning_rate": 4.41725623578883e-05,
      "loss": 3.3084,
      "step": 2434
    },
    {
      "epoch": 0.2218880991434299,
      "grad_norm": 2.3604602813720703,
      "learning_rate": 4.416796853440175e-05,
      "loss": 3.1959,
      "step": 2435
    },
    {
      "epoch": 0.22197922361946418,
      "grad_norm": 2.7427408695220947,
      "learning_rate": 4.4163373140024674e-05,
      "loss": 4.3374,
      "step": 2436
    },
    {
      "epoch": 0.22207034809549844,
      "grad_norm": 2.3761186599731445,
      "learning_rate": 4.41587761751337e-05,
      "loss": 2.9658,
      "step": 2437
    },
    {
      "epoch": 0.2221614725715327,
      "grad_norm": 1.355025291442871,
      "learning_rate": 4.4154177640105545e-05,
      "loss": 2.9463,
      "step": 2438
    },
    {
      "epoch": 0.22225259704756697,
      "grad_norm": 3.140056848526001,
      "learning_rate": 4.4149577535317096e-05,
      "loss": 3.158,
      "step": 2439
    },
    {
      "epoch": 0.22234372152360124,
      "grad_norm": 1.1856499910354614,
      "learning_rate": 4.4144975861145346e-05,
      "loss": 2.7849,
      "step": 2440
    },
    {
      "epoch": 0.2224348459996355,
      "grad_norm": 1.4156066179275513,
      "learning_rate": 4.414037261796741e-05,
      "loss": 2.9174,
      "step": 2441
    },
    {
      "epoch": 0.22252597047566977,
      "grad_norm": 1.2073270082473755,
      "learning_rate": 4.4135767806160546e-05,
      "loss": 2.7631,
      "step": 2442
    },
    {
      "epoch": 0.22261709495170404,
      "grad_norm": 1.4197841882705688,
      "learning_rate": 4.4131161426102155e-05,
      "loss": 2.9482,
      "step": 2443
    },
    {
      "epoch": 0.2227082194277383,
      "grad_norm": 2.0889065265655518,
      "learning_rate": 4.412655347816972e-05,
      "loss": 2.9923,
      "step": 2444
    },
    {
      "epoch": 0.22279934390377254,
      "grad_norm": 3.3835675716400146,
      "learning_rate": 4.41219439627409e-05,
      "loss": 3.2683,
      "step": 2445
    },
    {
      "epoch": 0.2228904683798068,
      "grad_norm": 2.081669807434082,
      "learning_rate": 4.411733288019345e-05,
      "loss": 3.0096,
      "step": 2446
    },
    {
      "epoch": 0.22298159285584107,
      "grad_norm": 2.3944973945617676,
      "learning_rate": 4.411272023090528e-05,
      "loss": 3.0477,
      "step": 2447
    },
    {
      "epoch": 0.22307271733187534,
      "grad_norm": 1.937587022781372,
      "learning_rate": 4.41081060152544e-05,
      "loss": 2.9916,
      "step": 2448
    },
    {
      "epoch": 0.2231638418079096,
      "grad_norm": 5.579928398132324,
      "learning_rate": 4.410349023361898e-05,
      "loss": 3.1081,
      "step": 2449
    },
    {
      "epoch": 0.22325496628394387,
      "grad_norm": 3.2476885318756104,
      "learning_rate": 4.4098872886377284e-05,
      "loss": 3.4062,
      "step": 2450
    },
    {
      "epoch": 0.22334609075997813,
      "grad_norm": 2.948455572128296,
      "learning_rate": 4.409425397390773e-05,
      "loss": 3.2524,
      "step": 2451
    },
    {
      "epoch": 0.2234372152360124,
      "grad_norm": 1.2981525659561157,
      "learning_rate": 4.4089633496588856e-05,
      "loss": 2.9227,
      "step": 2452
    },
    {
      "epoch": 0.22352833971204666,
      "grad_norm": 1.7288588285446167,
      "learning_rate": 4.408501145479933e-05,
      "loss": 2.8851,
      "step": 2453
    },
    {
      "epoch": 0.22361946418808093,
      "grad_norm": 1.3016902208328247,
      "learning_rate": 4.4080387848917945e-05,
      "loss": 3.0049,
      "step": 2454
    },
    {
      "epoch": 0.22371058866411517,
      "grad_norm": 1.7864162921905518,
      "learning_rate": 4.407576267932362e-05,
      "loss": 2.92,
      "step": 2455
    },
    {
      "epoch": 0.22380171314014943,
      "grad_norm": 1.4807178974151611,
      "learning_rate": 4.4071135946395416e-05,
      "loss": 2.9496,
      "step": 2456
    },
    {
      "epoch": 0.2238928376161837,
      "grad_norm": 1.7580772638320923,
      "learning_rate": 4.40665076505125e-05,
      "loss": 3.0996,
      "step": 2457
    },
    {
      "epoch": 0.22398396209221796,
      "grad_norm": 2.9470770359039307,
      "learning_rate": 4.406187779205418e-05,
      "loss": 3.0615,
      "step": 2458
    },
    {
      "epoch": 0.22407508656825223,
      "grad_norm": 1.4089727401733398,
      "learning_rate": 4.405724637139991e-05,
      "loss": 3.01,
      "step": 2459
    },
    {
      "epoch": 0.2241662110442865,
      "grad_norm": 1.3313802480697632,
      "learning_rate": 4.4052613388929226e-05,
      "loss": 2.9336,
      "step": 2460
    },
    {
      "epoch": 0.22425733552032076,
      "grad_norm": 1.5454027652740479,
      "learning_rate": 4.4047978845021836e-05,
      "loss": 3.0308,
      "step": 2461
    },
    {
      "epoch": 0.22434845999635503,
      "grad_norm": 1.3297046422958374,
      "learning_rate": 4.404334274005755e-05,
      "loss": 2.9001,
      "step": 2462
    },
    {
      "epoch": 0.2244395844723893,
      "grad_norm": 2.0395898818969727,
      "learning_rate": 4.4038705074416326e-05,
      "loss": 3.1146,
      "step": 2463
    },
    {
      "epoch": 0.22453070894842356,
      "grad_norm": 2.500328540802002,
      "learning_rate": 4.403406584847823e-05,
      "loss": 2.5952,
      "step": 2464
    },
    {
      "epoch": 0.22462183342445782,
      "grad_norm": 2.769273519515991,
      "learning_rate": 4.4029425062623474e-05,
      "loss": 3.2194,
      "step": 2465
    },
    {
      "epoch": 0.22471295790049206,
      "grad_norm": 3.4899208545684814,
      "learning_rate": 4.402478271723237e-05,
      "loss": 2.865,
      "step": 2466
    },
    {
      "epoch": 0.22480408237652633,
      "grad_norm": 2.499091625213623,
      "learning_rate": 4.40201388126854e-05,
      "loss": 2.2543,
      "step": 2467
    },
    {
      "epoch": 0.2248952068525606,
      "grad_norm": 2.8100392818450928,
      "learning_rate": 4.401549334936314e-05,
      "loss": 2.9594,
      "step": 2468
    },
    {
      "epoch": 0.22498633132859486,
      "grad_norm": 2.1237385272979736,
      "learning_rate": 4.401084632764629e-05,
      "loss": 3.0023,
      "step": 2469
    },
    {
      "epoch": 0.22507745580462912,
      "grad_norm": 3.010639190673828,
      "learning_rate": 4.4006197747915724e-05,
      "loss": 2.5678,
      "step": 2470
    },
    {
      "epoch": 0.2251685802806634,
      "grad_norm": 2.5722105503082275,
      "learning_rate": 4.400154761055237e-05,
      "loss": 2.4344,
      "step": 2471
    },
    {
      "epoch": 0.22525970475669765,
      "grad_norm": 1.8636105060577393,
      "learning_rate": 4.399689591593736e-05,
      "loss": 2.9566,
      "step": 2472
    },
    {
      "epoch": 0.22535082923273192,
      "grad_norm": 2.3771228790283203,
      "learning_rate": 4.399224266445191e-05,
      "loss": 3.1314,
      "step": 2473
    },
    {
      "epoch": 0.22544195370876619,
      "grad_norm": 2.78779935836792,
      "learning_rate": 4.398758785647736e-05,
      "loss": 3.0463,
      "step": 2474
    },
    {
      "epoch": 0.22553307818480045,
      "grad_norm": 2.024609088897705,
      "learning_rate": 4.398293149239521e-05,
      "loss": 3.0492,
      "step": 2475
    },
    {
      "epoch": 0.2256242026608347,
      "grad_norm": 2.4805586338043213,
      "learning_rate": 4.397827357258705e-05,
      "loss": 3.1153,
      "step": 2476
    },
    {
      "epoch": 0.22571532713686895,
      "grad_norm": 1.689842700958252,
      "learning_rate": 4.3973614097434613e-05,
      "loss": 2.6776,
      "step": 2477
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 1.5537314414978027,
      "learning_rate": 4.3968953067319777e-05,
      "loss": 2.9198,
      "step": 2478
    },
    {
      "epoch": 0.22589757608893749,
      "grad_norm": 3.156337261199951,
      "learning_rate": 4.396429048262452e-05,
      "loss": 3.212,
      "step": 2479
    },
    {
      "epoch": 0.22598870056497175,
      "grad_norm": 1.334908127784729,
      "learning_rate": 4.395962634373097e-05,
      "loss": 2.8467,
      "step": 2480
    },
    {
      "epoch": 0.22607982504100602,
      "grad_norm": 1.5555346012115479,
      "learning_rate": 4.395496065102135e-05,
      "loss": 2.9074,
      "step": 2481
    },
    {
      "epoch": 0.22617094951704028,
      "grad_norm": 2.0773093700408936,
      "learning_rate": 4.395029340487805e-05,
      "loss": 3.1432,
      "step": 2482
    },
    {
      "epoch": 0.22626207399307455,
      "grad_norm": 1.996966004371643,
      "learning_rate": 4.3945624605683574e-05,
      "loss": 3.1752,
      "step": 2483
    },
    {
      "epoch": 0.2263531984691088,
      "grad_norm": 2.3059380054473877,
      "learning_rate": 4.394095425382053e-05,
      "loss": 2.1763,
      "step": 2484
    },
    {
      "epoch": 0.22644432294514308,
      "grad_norm": 2.776571750640869,
      "learning_rate": 4.39362823496717e-05,
      "loss": 3.1487,
      "step": 2485
    },
    {
      "epoch": 0.22653544742117732,
      "grad_norm": 2.398050308227539,
      "learning_rate": 4.393160889361993e-05,
      "loss": 3.0926,
      "step": 2486
    },
    {
      "epoch": 0.22662657189721158,
      "grad_norm": 2.7774007320404053,
      "learning_rate": 4.392693388604825e-05,
      "loss": 3.143,
      "step": 2487
    },
    {
      "epoch": 0.22671769637324585,
      "grad_norm": 2.048381805419922,
      "learning_rate": 4.3922257327339786e-05,
      "loss": 3.0868,
      "step": 2488
    },
    {
      "epoch": 0.2268088208492801,
      "grad_norm": 3.1677088737487793,
      "learning_rate": 4.391757921787781e-05,
      "loss": 2.9315,
      "step": 2489
    },
    {
      "epoch": 0.22689994532531438,
      "grad_norm": 3.364952325820923,
      "learning_rate": 4.39128995580457e-05,
      "loss": 3.3264,
      "step": 2490
    },
    {
      "epoch": 0.22699106980134864,
      "grad_norm": 2.719878911972046,
      "learning_rate": 4.3908218348226985e-05,
      "loss": 2.8463,
      "step": 2491
    },
    {
      "epoch": 0.2270821942773829,
      "grad_norm": 2.468329906463623,
      "learning_rate": 4.39035355888053e-05,
      "loss": 3.2196,
      "step": 2492
    },
    {
      "epoch": 0.22717331875341717,
      "grad_norm": 2.0813400745391846,
      "learning_rate": 4.389885128016441e-05,
      "loss": 2.9979,
      "step": 2493
    },
    {
      "epoch": 0.22726444322945144,
      "grad_norm": 1.9805845022201538,
      "learning_rate": 4.3894165422688226e-05,
      "loss": 2.9,
      "step": 2494
    },
    {
      "epoch": 0.2273555677054857,
      "grad_norm": 1.562680959701538,
      "learning_rate": 4.388947801676076e-05,
      "loss": 2.9893,
      "step": 2495
    },
    {
      "epoch": 0.22744669218151994,
      "grad_norm": 3.1208388805389404,
      "learning_rate": 4.3884789062766185e-05,
      "loss": 4.5275,
      "step": 2496
    },
    {
      "epoch": 0.2275378166575542,
      "grad_norm": 1.5521373748779297,
      "learning_rate": 4.3880098561088754e-05,
      "loss": 2.9243,
      "step": 2497
    },
    {
      "epoch": 0.22762894113358847,
      "grad_norm": 2.3564810752868652,
      "learning_rate": 4.3875406512112894e-05,
      "loss": 3.1441,
      "step": 2498
    },
    {
      "epoch": 0.22772006560962274,
      "grad_norm": 2.579385757446289,
      "learning_rate": 4.387071291622311e-05,
      "loss": 2.8191,
      "step": 2499
    },
    {
      "epoch": 0.227811190085657,
      "grad_norm": 2.6531479358673096,
      "learning_rate": 4.3866017773804086e-05,
      "loss": 4.1967,
      "step": 2500
    },
    {
      "epoch": 0.22790231456169127,
      "grad_norm": 2.459132432937622,
      "learning_rate": 4.3861321085240595e-05,
      "loss": 3.1576,
      "step": 2501
    },
    {
      "epoch": 0.22799343903772554,
      "grad_norm": 2.3128418922424316,
      "learning_rate": 4.385662285091755e-05,
      "loss": 3.2254,
      "step": 2502
    },
    {
      "epoch": 0.2280845635137598,
      "grad_norm": 1.4728243350982666,
      "learning_rate": 4.385192307121999e-05,
      "loss": 3.0099,
      "step": 2503
    },
    {
      "epoch": 0.22817568798979407,
      "grad_norm": 2.124790668487549,
      "learning_rate": 4.3847221746533084e-05,
      "loss": 3.2911,
      "step": 2504
    },
    {
      "epoch": 0.22826681246582833,
      "grad_norm": 2.188908100128174,
      "learning_rate": 4.3842518877242115e-05,
      "loss": 3.0384,
      "step": 2505
    },
    {
      "epoch": 0.22835793694186257,
      "grad_norm": 1.266202449798584,
      "learning_rate": 4.383781446373252e-05,
      "loss": 2.8427,
      "step": 2506
    },
    {
      "epoch": 0.22844906141789684,
      "grad_norm": 2.004429578781128,
      "learning_rate": 4.383310850638983e-05,
      "loss": 2.8481,
      "step": 2507
    },
    {
      "epoch": 0.2285401858939311,
      "grad_norm": 2.7742865085601807,
      "learning_rate": 4.382840100559971e-05,
      "loss": 3.0441,
      "step": 2508
    },
    {
      "epoch": 0.22863131036996537,
      "grad_norm": 1.5699822902679443,
      "learning_rate": 4.382369196174798e-05,
      "loss": 2.8923,
      "step": 2509
    },
    {
      "epoch": 0.22872243484599963,
      "grad_norm": 1.3616158962249756,
      "learning_rate": 4.381898137522055e-05,
      "loss": 2.8921,
      "step": 2510
    },
    {
      "epoch": 0.2288135593220339,
      "grad_norm": 2.080249309539795,
      "learning_rate": 4.381426924640346e-05,
      "loss": 3.4529,
      "step": 2511
    },
    {
      "epoch": 0.22890468379806816,
      "grad_norm": 2.4456989765167236,
      "learning_rate": 4.380955557568291e-05,
      "loss": 3.1643,
      "step": 2512
    },
    {
      "epoch": 0.22899580827410243,
      "grad_norm": 2.9588747024536133,
      "learning_rate": 4.38048403634452e-05,
      "loss": 3.0233,
      "step": 2513
    },
    {
      "epoch": 0.2290869327501367,
      "grad_norm": 1.9728714227676392,
      "learning_rate": 4.380012361007675e-05,
      "loss": 3.0044,
      "step": 2514
    },
    {
      "epoch": 0.22917805722617096,
      "grad_norm": 2.1903185844421387,
      "learning_rate": 4.379540531596413e-05,
      "loss": 3.0556,
      "step": 2515
    },
    {
      "epoch": 0.2292691817022052,
      "grad_norm": 3.2391867637634277,
      "learning_rate": 4.379068548149401e-05,
      "loss": 3.0722,
      "step": 2516
    },
    {
      "epoch": 0.22936030617823946,
      "grad_norm": 1.926454782485962,
      "learning_rate": 4.378596410705319e-05,
      "loss": 3.1592,
      "step": 2517
    },
    {
      "epoch": 0.22945143065427373,
      "grad_norm": 2.696833610534668,
      "learning_rate": 4.378124119302863e-05,
      "loss": 3.1412,
      "step": 2518
    },
    {
      "epoch": 0.229542555130308,
      "grad_norm": 1.610856294631958,
      "learning_rate": 4.3776516739807386e-05,
      "loss": 2.976,
      "step": 2519
    },
    {
      "epoch": 0.22963367960634226,
      "grad_norm": 3.204596757888794,
      "learning_rate": 4.377179074777663e-05,
      "loss": 3.1426,
      "step": 2520
    },
    {
      "epoch": 0.22972480408237653,
      "grad_norm": 2.105928659439087,
      "learning_rate": 4.376706321732369e-05,
      "loss": 2.9453,
      "step": 2521
    },
    {
      "epoch": 0.2298159285584108,
      "grad_norm": 2.6947638988494873,
      "learning_rate": 4.376233414883601e-05,
      "loss": 4.0467,
      "step": 2522
    },
    {
      "epoch": 0.22990705303444506,
      "grad_norm": 3.7606661319732666,
      "learning_rate": 4.375760354270113e-05,
      "loss": 3.4666,
      "step": 2523
    },
    {
      "epoch": 0.22999817751047932,
      "grad_norm": 1.171001672744751,
      "learning_rate": 4.375287139930676e-05,
      "loss": 2.8478,
      "step": 2524
    },
    {
      "epoch": 0.2300893019865136,
      "grad_norm": 1.8320801258087158,
      "learning_rate": 4.374813771904073e-05,
      "loss": 3.0189,
      "step": 2525
    },
    {
      "epoch": 0.23018042646254783,
      "grad_norm": 2.7848849296569824,
      "learning_rate": 4.3743402502290955e-05,
      "loss": 2.9479,
      "step": 2526
    },
    {
      "epoch": 0.2302715509385821,
      "grad_norm": 2.760819673538208,
      "learning_rate": 4.373866574944553e-05,
      "loss": 3.0715,
      "step": 2527
    },
    {
      "epoch": 0.23036267541461636,
      "grad_norm": 1.3240574598312378,
      "learning_rate": 4.3733927460892635e-05,
      "loss": 2.7905,
      "step": 2528
    },
    {
      "epoch": 0.23045379989065062,
      "grad_norm": 2.0322422981262207,
      "learning_rate": 4.3729187637020594e-05,
      "loss": 3.0867,
      "step": 2529
    },
    {
      "epoch": 0.2305449243666849,
      "grad_norm": 2.0741896629333496,
      "learning_rate": 4.372444627821787e-05,
      "loss": 3.121,
      "step": 2530
    },
    {
      "epoch": 0.23063604884271915,
      "grad_norm": 2.029799222946167,
      "learning_rate": 4.371970338487301e-05,
      "loss": 3.1044,
      "step": 2531
    },
    {
      "epoch": 0.23072717331875342,
      "grad_norm": 2.5058116912841797,
      "learning_rate": 4.371495895737473e-05,
      "loss": 3.1886,
      "step": 2532
    },
    {
      "epoch": 0.23081829779478769,
      "grad_norm": 1.280074119567871,
      "learning_rate": 4.371021299611186e-05,
      "loss": 2.8257,
      "step": 2533
    },
    {
      "epoch": 0.23090942227082195,
      "grad_norm": 2.3696508407592773,
      "learning_rate": 4.370546550147332e-05,
      "loss": 3.3795,
      "step": 2534
    },
    {
      "epoch": 0.23100054674685622,
      "grad_norm": 2.6990113258361816,
      "learning_rate": 4.370071647384822e-05,
      "loss": 3.2594,
      "step": 2535
    },
    {
      "epoch": 0.23109167122289045,
      "grad_norm": 2.986725330352783,
      "learning_rate": 4.369596591362575e-05,
      "loss": 3.0206,
      "step": 2536
    },
    {
      "epoch": 0.23118279569892472,
      "grad_norm": 2.558835506439209,
      "learning_rate": 4.369121382119523e-05,
      "loss": 3.2142,
      "step": 2537
    },
    {
      "epoch": 0.23127392017495899,
      "grad_norm": 1.83037269115448,
      "learning_rate": 4.368646019694612e-05,
      "loss": 2.9043,
      "step": 2538
    },
    {
      "epoch": 0.23136504465099325,
      "grad_norm": 2.7770633697509766,
      "learning_rate": 4.3681705041267987e-05,
      "loss": 3.2446,
      "step": 2539
    },
    {
      "epoch": 0.23145616912702752,
      "grad_norm": 1.2130329608917236,
      "learning_rate": 4.3676948354550556e-05,
      "loss": 2.8928,
      "step": 2540
    },
    {
      "epoch": 0.23154729360306178,
      "grad_norm": 2.6979403495788574,
      "learning_rate": 4.367219013718364e-05,
      "loss": 3.1149,
      "step": 2541
    },
    {
      "epoch": 0.23163841807909605,
      "grad_norm": 2.009228229522705,
      "learning_rate": 4.36674303895572e-05,
      "loss": 3.0308,
      "step": 2542
    },
    {
      "epoch": 0.2317295425551303,
      "grad_norm": 2.1447806358337402,
      "learning_rate": 4.366266911206131e-05,
      "loss": 3.0686,
      "step": 2543
    },
    {
      "epoch": 0.23182066703116458,
      "grad_norm": 1.2554717063903809,
      "learning_rate": 4.3657906305086174e-05,
      "loss": 2.8642,
      "step": 2544
    },
    {
      "epoch": 0.23191179150719884,
      "grad_norm": 2.336080312728882,
      "learning_rate": 4.365314196902214e-05,
      "loss": 3.0662,
      "step": 2545
    },
    {
      "epoch": 0.2320029159832331,
      "grad_norm": 2.766204833984375,
      "learning_rate": 4.364837610425964e-05,
      "loss": 3.128,
      "step": 2546
    },
    {
      "epoch": 0.23209404045926735,
      "grad_norm": 2.7924094200134277,
      "learning_rate": 4.364360871118928e-05,
      "loss": 2.9541,
      "step": 2547
    },
    {
      "epoch": 0.2321851649353016,
      "grad_norm": 3.102461576461792,
      "learning_rate": 4.3638839790201746e-05,
      "loss": 3.0788,
      "step": 2548
    },
    {
      "epoch": 0.23227628941133588,
      "grad_norm": 1.4518617391586304,
      "learning_rate": 4.363406934168788e-05,
      "loss": 3.0123,
      "step": 2549
    },
    {
      "epoch": 0.23236741388737014,
      "grad_norm": 1.4064468145370483,
      "learning_rate": 4.362929736603864e-05,
      "loss": 2.9199,
      "step": 2550
    },
    {
      "epoch": 0.2324585383634044,
      "grad_norm": 2.4712982177734375,
      "learning_rate": 4.36245238636451e-05,
      "loss": 3.158,
      "step": 2551
    },
    {
      "epoch": 0.23254966283943868,
      "grad_norm": 1.942933440208435,
      "learning_rate": 4.3619748834898465e-05,
      "loss": 3.3275,
      "step": 2552
    },
    {
      "epoch": 0.23264078731547294,
      "grad_norm": 1.3232603073120117,
      "learning_rate": 4.361497228019009e-05,
      "loss": 2.9237,
      "step": 2553
    },
    {
      "epoch": 0.2327319117915072,
      "grad_norm": 2.4996190071105957,
      "learning_rate": 4.3610194199911405e-05,
      "loss": 2.8913,
      "step": 2554
    },
    {
      "epoch": 0.23282303626754147,
      "grad_norm": 2.3999860286712646,
      "learning_rate": 4.360541459445402e-05,
      "loss": 2.8331,
      "step": 2555
    },
    {
      "epoch": 0.23291416074357574,
      "grad_norm": 1.3918232917785645,
      "learning_rate": 4.360063346420961e-05,
      "loss": 2.9153,
      "step": 2556
    },
    {
      "epoch": 0.23300528521960998,
      "grad_norm": 2.5752077102661133,
      "learning_rate": 4.359585080957004e-05,
      "loss": 4.3064,
      "step": 2557
    },
    {
      "epoch": 0.23309640969564424,
      "grad_norm": 2.4698143005371094,
      "learning_rate": 4.3591066630927247e-05,
      "loss": 2.97,
      "step": 2558
    },
    {
      "epoch": 0.2331875341716785,
      "grad_norm": 1.9726072549819946,
      "learning_rate": 4.3586280928673315e-05,
      "loss": 3.074,
      "step": 2559
    },
    {
      "epoch": 0.23327865864771277,
      "grad_norm": 1.301616907119751,
      "learning_rate": 4.3581493703200455e-05,
      "loss": 2.8688,
      "step": 2560
    },
    {
      "epoch": 0.23336978312374704,
      "grad_norm": 2.362919807434082,
      "learning_rate": 4.3576704954901e-05,
      "loss": 2.7284,
      "step": 2561
    },
    {
      "epoch": 0.2334609075997813,
      "grad_norm": 1.1444134712219238,
      "learning_rate": 4.3571914684167404e-05,
      "loss": 2.7997,
      "step": 2562
    },
    {
      "epoch": 0.23355203207581557,
      "grad_norm": 2.143993854522705,
      "learning_rate": 4.356712289139225e-05,
      "loss": 2.9981,
      "step": 2563
    },
    {
      "epoch": 0.23364315655184983,
      "grad_norm": 1.9644395112991333,
      "learning_rate": 4.356232957696825e-05,
      "loss": 3.1688,
      "step": 2564
    },
    {
      "epoch": 0.2337342810278841,
      "grad_norm": 3.3716814517974854,
      "learning_rate": 4.355753474128822e-05,
      "loss": 3.0318,
      "step": 2565
    },
    {
      "epoch": 0.23382540550391837,
      "grad_norm": 1.5076642036437988,
      "learning_rate": 4.355273838474514e-05,
      "loss": 2.8963,
      "step": 2566
    },
    {
      "epoch": 0.2339165299799526,
      "grad_norm": 2.6749184131622314,
      "learning_rate": 4.354794050773206e-05,
      "loss": 3.2116,
      "step": 2567
    },
    {
      "epoch": 0.23400765445598687,
      "grad_norm": 2.86080265045166,
      "learning_rate": 4.354314111064221e-05,
      "loss": 3.1095,
      "step": 2568
    },
    {
      "epoch": 0.23409877893202113,
      "grad_norm": 3.4828879833221436,
      "learning_rate": 4.35383401938689e-05,
      "loss": 2.8742,
      "step": 2569
    },
    {
      "epoch": 0.2341899034080554,
      "grad_norm": 1.4495912790298462,
      "learning_rate": 4.3533537757805596e-05,
      "loss": 2.8566,
      "step": 2570
    },
    {
      "epoch": 0.23428102788408967,
      "grad_norm": 1.2647649049758911,
      "learning_rate": 4.352873380284588e-05,
      "loss": 2.9014,
      "step": 2571
    },
    {
      "epoch": 0.23437215236012393,
      "grad_norm": 2.3468852043151855,
      "learning_rate": 4.352392832938345e-05,
      "loss": 3.1465,
      "step": 2572
    },
    {
      "epoch": 0.2344632768361582,
      "grad_norm": 1.3636425733566284,
      "learning_rate": 4.351912133781213e-05,
      "loss": 2.8097,
      "step": 2573
    },
    {
      "epoch": 0.23455440131219246,
      "grad_norm": 1.589536428451538,
      "learning_rate": 4.351431282852587e-05,
      "loss": 2.9583,
      "step": 2574
    },
    {
      "epoch": 0.23464552578822673,
      "grad_norm": 2.9726548194885254,
      "learning_rate": 4.350950280191875e-05,
      "loss": 2.9902,
      "step": 2575
    },
    {
      "epoch": 0.234736650264261,
      "grad_norm": 2.051861524581909,
      "learning_rate": 4.350469125838498e-05,
      "loss": 3.0867,
      "step": 2576
    },
    {
      "epoch": 0.23482777474029523,
      "grad_norm": 2.660329818725586,
      "learning_rate": 4.349987819831887e-05,
      "loss": 4.1739,
      "step": 2577
    },
    {
      "epoch": 0.2349188992163295,
      "grad_norm": 4.011890888214111,
      "learning_rate": 4.349506362211488e-05,
      "loss": 2.9524,
      "step": 2578
    },
    {
      "epoch": 0.23501002369236376,
      "grad_norm": 1.4626097679138184,
      "learning_rate": 4.349024753016758e-05,
      "loss": 2.8506,
      "step": 2579
    },
    {
      "epoch": 0.23510114816839803,
      "grad_norm": 1.8341776132583618,
      "learning_rate": 4.3485429922871664e-05,
      "loss": 2.9603,
      "step": 2580
    },
    {
      "epoch": 0.2351922726444323,
      "grad_norm": 2.7456650733947754,
      "learning_rate": 4.348061080062196e-05,
      "loss": 3.0549,
      "step": 2581
    },
    {
      "epoch": 0.23528339712046656,
      "grad_norm": 2.3558387756347656,
      "learning_rate": 4.34757901638134e-05,
      "loss": 2.8003,
      "step": 2582
    },
    {
      "epoch": 0.23537452159650082,
      "grad_norm": 2.8020052909851074,
      "learning_rate": 4.347096801284107e-05,
      "loss": 4.1171,
      "step": 2583
    },
    {
      "epoch": 0.2354656460725351,
      "grad_norm": 2.9437286853790283,
      "learning_rate": 4.346614434810017e-05,
      "loss": 3.1727,
      "step": 2584
    },
    {
      "epoch": 0.23555677054856936,
      "grad_norm": 1.7535585165023804,
      "learning_rate": 4.3461319169985996e-05,
      "loss": 3.0314,
      "step": 2585
    },
    {
      "epoch": 0.23564789502460362,
      "grad_norm": 1.960329532623291,
      "learning_rate": 4.345649247889401e-05,
      "loss": 2.9734,
      "step": 2586
    },
    {
      "epoch": 0.23573901950063786,
      "grad_norm": 1.2548421621322632,
      "learning_rate": 4.345166427521976e-05,
      "loss": 2.9173,
      "step": 2587
    },
    {
      "epoch": 0.23583014397667212,
      "grad_norm": 2.1360952854156494,
      "learning_rate": 4.3446834559358954e-05,
      "loss": 2.7578,
      "step": 2588
    },
    {
      "epoch": 0.2359212684527064,
      "grad_norm": 1.4238603115081787,
      "learning_rate": 4.34420033317074e-05,
      "loss": 2.9153,
      "step": 2589
    },
    {
      "epoch": 0.23601239292874066,
      "grad_norm": 1.389762043952942,
      "learning_rate": 4.3437170592661034e-05,
      "loss": 2.8114,
      "step": 2590
    },
    {
      "epoch": 0.23610351740477492,
      "grad_norm": 2.0954384803771973,
      "learning_rate": 4.343233634261592e-05,
      "loss": 2.91,
      "step": 2591
    },
    {
      "epoch": 0.2361946418808092,
      "grad_norm": 2.0700836181640625,
      "learning_rate": 4.342750058196824e-05,
      "loss": 3.1276,
      "step": 2592
    },
    {
      "epoch": 0.23628576635684345,
      "grad_norm": 2.870807647705078,
      "learning_rate": 4.3422663311114306e-05,
      "loss": 3.0627,
      "step": 2593
    },
    {
      "epoch": 0.23637689083287772,
      "grad_norm": 2.354541301727295,
      "learning_rate": 4.341782453045055e-05,
      "loss": 2.8899,
      "step": 2594
    },
    {
      "epoch": 0.23646801530891198,
      "grad_norm": 4.662764549255371,
      "learning_rate": 4.341298424037353e-05,
      "loss": 3.1298,
      "step": 2595
    },
    {
      "epoch": 0.23655913978494625,
      "grad_norm": 2.4562151432037354,
      "learning_rate": 4.340814244127993e-05,
      "loss": 3.2805,
      "step": 2596
    },
    {
      "epoch": 0.2366502642609805,
      "grad_norm": 1.2373110055923462,
      "learning_rate": 4.340329913356656e-05,
      "loss": 2.8407,
      "step": 2597
    },
    {
      "epoch": 0.23674138873701475,
      "grad_norm": 1.2382012605667114,
      "learning_rate": 4.339845431763033e-05,
      "loss": 2.8318,
      "step": 2598
    },
    {
      "epoch": 0.23683251321304902,
      "grad_norm": 1.8288928270339966,
      "learning_rate": 4.3393607993868314e-05,
      "loss": 2.9884,
      "step": 2599
    },
    {
      "epoch": 0.23692363768908328,
      "grad_norm": 1.2728618383407593,
      "learning_rate": 4.338876016267767e-05,
      "loss": 2.8936,
      "step": 2600
    },
    {
      "epoch": 0.23701476216511755,
      "grad_norm": 2.1473453044891357,
      "learning_rate": 4.33839108244557e-05,
      "loss": 3.0044,
      "step": 2601
    },
    {
      "epoch": 0.23710588664115181,
      "grad_norm": 2.278900146484375,
      "learning_rate": 4.3379059979599825e-05,
      "loss": 3.2137,
      "step": 2602
    },
    {
      "epoch": 0.23719701111718608,
      "grad_norm": 2.9042911529541016,
      "learning_rate": 4.33742076285076e-05,
      "loss": 3.1474,
      "step": 2603
    },
    {
      "epoch": 0.23728813559322035,
      "grad_norm": 1.1429674625396729,
      "learning_rate": 4.336935377157668e-05,
      "loss": 2.8799,
      "step": 2604
    },
    {
      "epoch": 0.2373792600692546,
      "grad_norm": 2.1435470581054688,
      "learning_rate": 4.336449840920488e-05,
      "loss": 3.382,
      "step": 2605
    },
    {
      "epoch": 0.23747038454528888,
      "grad_norm": 2.265254020690918,
      "learning_rate": 4.33596415417901e-05,
      "loss": 3.0675,
      "step": 2606
    },
    {
      "epoch": 0.23756150902132311,
      "grad_norm": 1.8749951124191284,
      "learning_rate": 4.3354783169730373e-05,
      "loss": 2.871,
      "step": 2607
    },
    {
      "epoch": 0.23765263349735738,
      "grad_norm": 2.2031915187835693,
      "learning_rate": 4.3349923293423874e-05,
      "loss": 3.1805,
      "step": 2608
    },
    {
      "epoch": 0.23774375797339165,
      "grad_norm": 1.8375120162963867,
      "learning_rate": 4.334506191326889e-05,
      "loss": 2.9928,
      "step": 2609
    },
    {
      "epoch": 0.2378348824494259,
      "grad_norm": 2.706744909286499,
      "learning_rate": 4.334019902966383e-05,
      "loss": 3.026,
      "step": 2610
    },
    {
      "epoch": 0.23792600692546018,
      "grad_norm": 3.2778031826019287,
      "learning_rate": 4.333533464300721e-05,
      "loss": 3.2118,
      "step": 2611
    },
    {
      "epoch": 0.23801713140149444,
      "grad_norm": 1.4383528232574463,
      "learning_rate": 4.33304687536977e-05,
      "loss": 3.0019,
      "step": 2612
    },
    {
      "epoch": 0.2381082558775287,
      "grad_norm": 1.7730919122695923,
      "learning_rate": 4.33256013621341e-05,
      "loss": 3.1097,
      "step": 2613
    },
    {
      "epoch": 0.23819938035356297,
      "grad_norm": 2.6617166996002197,
      "learning_rate": 4.3320732468715264e-05,
      "loss": 2.7856,
      "step": 2614
    },
    {
      "epoch": 0.23829050482959724,
      "grad_norm": 3.221576452255249,
      "learning_rate": 4.3315862073840245e-05,
      "loss": 2.5993,
      "step": 2615
    },
    {
      "epoch": 0.2383816293056315,
      "grad_norm": 3.054969072341919,
      "learning_rate": 4.33109901779082e-05,
      "loss": 3.2204,
      "step": 2616
    },
    {
      "epoch": 0.23847275378166574,
      "grad_norm": 2.455723285675049,
      "learning_rate": 4.3306116781318375e-05,
      "loss": 4.1203,
      "step": 2617
    },
    {
      "epoch": 0.2385638782577,
      "grad_norm": 1.201890230178833,
      "learning_rate": 4.330124188447018e-05,
      "loss": 2.8881,
      "step": 2618
    },
    {
      "epoch": 0.23865500273373427,
      "grad_norm": 2.9991354942321777,
      "learning_rate": 4.329636548776313e-05,
      "loss": 3.3366,
      "step": 2619
    },
    {
      "epoch": 0.23874612720976854,
      "grad_norm": 1.1390374898910522,
      "learning_rate": 4.329148759159687e-05,
      "loss": 2.8381,
      "step": 2620
    },
    {
      "epoch": 0.2388372516858028,
      "grad_norm": 1.117658257484436,
      "learning_rate": 4.3286608196371147e-05,
      "loss": 2.7479,
      "step": 2621
    },
    {
      "epoch": 0.23892837616183707,
      "grad_norm": 2.1374876499176025,
      "learning_rate": 4.3281727302485863e-05,
      "loss": 3.1697,
      "step": 2622
    },
    {
      "epoch": 0.23901950063787133,
      "grad_norm": 2.8092637062072754,
      "learning_rate": 4.3276844910341016e-05,
      "loss": 3.117,
      "step": 2623
    },
    {
      "epoch": 0.2391106251139056,
      "grad_norm": 1.9980497360229492,
      "learning_rate": 4.3271961020336734e-05,
      "loss": 3.0234,
      "step": 2624
    },
    {
      "epoch": 0.23920174958993987,
      "grad_norm": 1.4312304258346558,
      "learning_rate": 4.3267075632873294e-05,
      "loss": 2.9348,
      "step": 2625
    },
    {
      "epoch": 0.23929287406597413,
      "grad_norm": 1.4598610401153564,
      "learning_rate": 4.326218874835105e-05,
      "loss": 2.9028,
      "step": 2626
    },
    {
      "epoch": 0.2393839985420084,
      "grad_norm": 1.7749381065368652,
      "learning_rate": 4.3257300367170504e-05,
      "loss": 2.9809,
      "step": 2627
    },
    {
      "epoch": 0.23947512301804263,
      "grad_norm": 3.0945212841033936,
      "learning_rate": 4.325241048973229e-05,
      "loss": 2.9258,
      "step": 2628
    },
    {
      "epoch": 0.2395662474940769,
      "grad_norm": 2.2946019172668457,
      "learning_rate": 4.324751911643714e-05,
      "loss": 2.8593,
      "step": 2629
    },
    {
      "epoch": 0.23965737197011117,
      "grad_norm": 2.288707733154297,
      "learning_rate": 4.324262624768593e-05,
      "loss": 3.0989,
      "step": 2630
    },
    {
      "epoch": 0.23974849644614543,
      "grad_norm": 2.7445297241210938,
      "learning_rate": 4.3237731883879645e-05,
      "loss": 3.1106,
      "step": 2631
    },
    {
      "epoch": 0.2398396209221797,
      "grad_norm": 5.5309062004089355,
      "learning_rate": 4.32328360254194e-05,
      "loss": 2.522,
      "step": 2632
    },
    {
      "epoch": 0.23993074539821396,
      "grad_norm": 1.9457108974456787,
      "learning_rate": 4.322793867270642e-05,
      "loss": 2.8576,
      "step": 2633
    },
    {
      "epoch": 0.24002186987424823,
      "grad_norm": 3.383957862854004,
      "learning_rate": 4.322303982614209e-05,
      "loss": 4.3753,
      "step": 2634
    },
    {
      "epoch": 0.2401129943502825,
      "grad_norm": 2.4001412391662598,
      "learning_rate": 4.3218139486127854e-05,
      "loss": 3.8667,
      "step": 2635
    },
    {
      "epoch": 0.24020411882631676,
      "grad_norm": 1.446460485458374,
      "learning_rate": 4.321323765306533e-05,
      "loss": 2.9932,
      "step": 2636
    },
    {
      "epoch": 0.24029524330235102,
      "grad_norm": 1.8919917345046997,
      "learning_rate": 4.320833432735625e-05,
      "loss": 2.8576,
      "step": 2637
    },
    {
      "epoch": 0.24038636777838526,
      "grad_norm": 1.7042949199676514,
      "learning_rate": 4.3203429509402456e-05,
      "loss": 3.0769,
      "step": 2638
    },
    {
      "epoch": 0.24047749225441953,
      "grad_norm": 2.2334442138671875,
      "learning_rate": 4.319852319960591e-05,
      "loss": 3.1429,
      "step": 2639
    },
    {
      "epoch": 0.2405686167304538,
      "grad_norm": 1.8812450170516968,
      "learning_rate": 4.3193615398368716e-05,
      "loss": 2.9002,
      "step": 2640
    },
    {
      "epoch": 0.24065974120648806,
      "grad_norm": 1.2729361057281494,
      "learning_rate": 4.318870610609307e-05,
      "loss": 2.9281,
      "step": 2641
    },
    {
      "epoch": 0.24075086568252232,
      "grad_norm": 3.1798903942108154,
      "learning_rate": 4.318379532318132e-05,
      "loss": 3.1768,
      "step": 2642
    },
    {
      "epoch": 0.2408419901585566,
      "grad_norm": 1.703678011894226,
      "learning_rate": 4.317888305003593e-05,
      "loss": 2.9493,
      "step": 2643
    },
    {
      "epoch": 0.24093311463459086,
      "grad_norm": 1.3905398845672607,
      "learning_rate": 4.317396928705947e-05,
      "loss": 2.9198,
      "step": 2644
    },
    {
      "epoch": 0.24102423911062512,
      "grad_norm": 1.3515852689743042,
      "learning_rate": 4.3169054034654646e-05,
      "loss": 2.9235,
      "step": 2645
    },
    {
      "epoch": 0.2411153635866594,
      "grad_norm": 2.200063705444336,
      "learning_rate": 4.316413729322427e-05,
      "loss": 2.8466,
      "step": 2646
    },
    {
      "epoch": 0.24120648806269365,
      "grad_norm": 2.3201496601104736,
      "learning_rate": 4.315921906317131e-05,
      "loss": 2.9707,
      "step": 2647
    },
    {
      "epoch": 0.2412976125387279,
      "grad_norm": 2.286952257156372,
      "learning_rate": 4.315429934489882e-05,
      "loss": 2.8815,
      "step": 2648
    },
    {
      "epoch": 0.24138873701476216,
      "grad_norm": 2.6374669075012207,
      "learning_rate": 4.3149378138809995e-05,
      "loss": 3.2079,
      "step": 2649
    },
    {
      "epoch": 0.24147986149079642,
      "grad_norm": 2.1839795112609863,
      "learning_rate": 4.314445544530815e-05,
      "loss": 2.9206,
      "step": 2650
    },
    {
      "epoch": 0.2415709859668307,
      "grad_norm": 2.7234604358673096,
      "learning_rate": 4.313953126479671e-05,
      "loss": 2.6192,
      "step": 2651
    },
    {
      "epoch": 0.24166211044286495,
      "grad_norm": 1.4357060194015503,
      "learning_rate": 4.313460559767923e-05,
      "loss": 2.8557,
      "step": 2652
    },
    {
      "epoch": 0.24175323491889922,
      "grad_norm": 1.4005088806152344,
      "learning_rate": 4.312967844435941e-05,
      "loss": 2.8065,
      "step": 2653
    },
    {
      "epoch": 0.24184435939493348,
      "grad_norm": 1.9790220260620117,
      "learning_rate": 4.3124749805241024e-05,
      "loss": 3.0875,
      "step": 2654
    },
    {
      "epoch": 0.24193548387096775,
      "grad_norm": 2.435584306716919,
      "learning_rate": 4.3119819680728e-05,
      "loss": 3.2637,
      "step": 2655
    },
    {
      "epoch": 0.24202660834700201,
      "grad_norm": 2.065972089767456,
      "learning_rate": 4.311488807122439e-05,
      "loss": 3.0034,
      "step": 2656
    },
    {
      "epoch": 0.24211773282303628,
      "grad_norm": 2.4165637493133545,
      "learning_rate": 4.310995497713435e-05,
      "loss": 2.9274,
      "step": 2657
    },
    {
      "epoch": 0.24220885729907052,
      "grad_norm": 2.2093489170074463,
      "learning_rate": 4.310502039886217e-05,
      "loss": 2.4305,
      "step": 2658
    },
    {
      "epoch": 0.24229998177510478,
      "grad_norm": 8.53961181640625,
      "learning_rate": 4.3100084336812264e-05,
      "loss": 2.9748,
      "step": 2659
    },
    {
      "epoch": 0.24239110625113905,
      "grad_norm": 2.2558650970458984,
      "learning_rate": 4.309514679138914e-05,
      "loss": 3.1578,
      "step": 2660
    },
    {
      "epoch": 0.24248223072717331,
      "grad_norm": 2.6612629890441895,
      "learning_rate": 4.309020776299748e-05,
      "loss": 3.0638,
      "step": 2661
    },
    {
      "epoch": 0.24257335520320758,
      "grad_norm": 2.4825222492218018,
      "learning_rate": 4.308526725204204e-05,
      "loss": 3.0676,
      "step": 2662
    },
    {
      "epoch": 0.24266447967924185,
      "grad_norm": 2.4086720943450928,
      "learning_rate": 4.308032525892771e-05,
      "loss": 3.1848,
      "step": 2663
    },
    {
      "epoch": 0.2427556041552761,
      "grad_norm": 1.9967304468154907,
      "learning_rate": 4.307538178405951e-05,
      "loss": 2.9488,
      "step": 2664
    },
    {
      "epoch": 0.24284672863131038,
      "grad_norm": 3.1566150188446045,
      "learning_rate": 4.307043682784258e-05,
      "loss": 3.491,
      "step": 2665
    },
    {
      "epoch": 0.24293785310734464,
      "grad_norm": 3.432265520095825,
      "learning_rate": 4.3065490390682186e-05,
      "loss": 2.7307,
      "step": 2666
    },
    {
      "epoch": 0.2430289775833789,
      "grad_norm": 3.0809664726257324,
      "learning_rate": 4.3060542472983686e-05,
      "loss": 2.7025,
      "step": 2667
    },
    {
      "epoch": 0.24312010205941315,
      "grad_norm": 1.7178789377212524,
      "learning_rate": 4.305559307515261e-05,
      "loss": 3.0144,
      "step": 2668
    },
    {
      "epoch": 0.2432112265354474,
      "grad_norm": 2.1862175464630127,
      "learning_rate": 4.305064219759455e-05,
      "loss": 3.0457,
      "step": 2669
    },
    {
      "epoch": 0.24330235101148168,
      "grad_norm": 2.2935891151428223,
      "learning_rate": 4.304568984071528e-05,
      "loss": 3.044,
      "step": 2670
    },
    {
      "epoch": 0.24339347548751594,
      "grad_norm": 1.7227686643600464,
      "learning_rate": 4.304073600492063e-05,
      "loss": 2.7858,
      "step": 2671
    },
    {
      "epoch": 0.2434845999635502,
      "grad_norm": 1.5395338535308838,
      "learning_rate": 4.303578069061662e-05,
      "loss": 3.0303,
      "step": 2672
    },
    {
      "epoch": 0.24357572443958447,
      "grad_norm": 3.271428108215332,
      "learning_rate": 4.303082389820935e-05,
      "loss": 2.8974,
      "step": 2673
    },
    {
      "epoch": 0.24366684891561874,
      "grad_norm": 1.7013260126113892,
      "learning_rate": 4.302586562810504e-05,
      "loss": 2.9225,
      "step": 2674
    },
    {
      "epoch": 0.243757973391653,
      "grad_norm": 1.4562128782272339,
      "learning_rate": 4.3020905880710036e-05,
      "loss": 2.9254,
      "step": 2675
    },
    {
      "epoch": 0.24384909786768727,
      "grad_norm": 2.133718490600586,
      "learning_rate": 4.3015944656430825e-05,
      "loss": 2.9733,
      "step": 2676
    },
    {
      "epoch": 0.24394022234372154,
      "grad_norm": 2.119091033935547,
      "learning_rate": 4.3010981955673976e-05,
      "loss": 3.0041,
      "step": 2677
    },
    {
      "epoch": 0.24403134681975577,
      "grad_norm": 3.0426650047302246,
      "learning_rate": 4.3006017778846234e-05,
      "loss": 3.0179,
      "step": 2678
    },
    {
      "epoch": 0.24412247129579004,
      "grad_norm": 1.8831664323806763,
      "learning_rate": 4.3001052126354405e-05,
      "loss": 3.0924,
      "step": 2679
    },
    {
      "epoch": 0.2442135957718243,
      "grad_norm": 2.0983803272247314,
      "learning_rate": 4.2996084998605444e-05,
      "loss": 3.0782,
      "step": 2680
    },
    {
      "epoch": 0.24430472024785857,
      "grad_norm": 1.5185456275939941,
      "learning_rate": 4.299111639600645e-05,
      "loss": 2.7408,
      "step": 2681
    },
    {
      "epoch": 0.24439584472389284,
      "grad_norm": 2.630352735519409,
      "learning_rate": 4.29861463189646e-05,
      "loss": 3.0526,
      "step": 2682
    },
    {
      "epoch": 0.2444869691999271,
      "grad_norm": 1.59076726436615,
      "learning_rate": 4.298117476788721e-05,
      "loss": 2.968,
      "step": 2683
    },
    {
      "epoch": 0.24457809367596137,
      "grad_norm": 2.509619951248169,
      "learning_rate": 4.2976201743181734e-05,
      "loss": 3.0801,
      "step": 2684
    },
    {
      "epoch": 0.24466921815199563,
      "grad_norm": 1.9787074327468872,
      "learning_rate": 4.297122724525572e-05,
      "loss": 3.097,
      "step": 2685
    },
    {
      "epoch": 0.2447603426280299,
      "grad_norm": 2.503199338912964,
      "learning_rate": 4.296625127451685e-05,
      "loss": 4.1969,
      "step": 2686
    },
    {
      "epoch": 0.24485146710406416,
      "grad_norm": 2.560425043106079,
      "learning_rate": 4.296127383137292e-05,
      "loss": 2.828,
      "step": 2687
    },
    {
      "epoch": 0.2449425915800984,
      "grad_norm": 2.3042261600494385,
      "learning_rate": 4.295629491623186e-05,
      "loss": 3.3887,
      "step": 2688
    },
    {
      "epoch": 0.24503371605613267,
      "grad_norm": 1.7289148569107056,
      "learning_rate": 4.29513145295017e-05,
      "loss": 2.9782,
      "step": 2689
    },
    {
      "epoch": 0.24512484053216693,
      "grad_norm": 1.9465707540512085,
      "learning_rate": 4.294633267159062e-05,
      "loss": 3.0959,
      "step": 2690
    },
    {
      "epoch": 0.2452159650082012,
      "grad_norm": 2.121170997619629,
      "learning_rate": 4.294134934290688e-05,
      "loss": 3.0911,
      "step": 2691
    },
    {
      "epoch": 0.24530708948423546,
      "grad_norm": 2.2854654788970947,
      "learning_rate": 4.2936364543858896e-05,
      "loss": 3.0146,
      "step": 2692
    },
    {
      "epoch": 0.24539821396026973,
      "grad_norm": 1.832336187362671,
      "learning_rate": 4.293137827485519e-05,
      "loss": 2.9321,
      "step": 2693
    },
    {
      "epoch": 0.245489338436304,
      "grad_norm": 1.311331033706665,
      "learning_rate": 4.2926390536304415e-05,
      "loss": 2.8879,
      "step": 2694
    },
    {
      "epoch": 0.24558046291233826,
      "grad_norm": 1.351783037185669,
      "learning_rate": 4.292140132861533e-05,
      "loss": 2.9211,
      "step": 2695
    },
    {
      "epoch": 0.24567158738837253,
      "grad_norm": 1.2155320644378662,
      "learning_rate": 4.291641065219681e-05,
      "loss": 2.7691,
      "step": 2696
    },
    {
      "epoch": 0.2457627118644068,
      "grad_norm": 2.298143148422241,
      "learning_rate": 4.291141850745788e-05,
      "loss": 2.7203,
      "step": 2697
    },
    {
      "epoch": 0.24585383634044103,
      "grad_norm": 2.173449754714966,
      "learning_rate": 4.290642489480765e-05,
      "loss": 2.5221,
      "step": 2698
    },
    {
      "epoch": 0.2459449608164753,
      "grad_norm": 2.076207399368286,
      "learning_rate": 4.290142981465537e-05,
      "loss": 3.2915,
      "step": 2699
    },
    {
      "epoch": 0.24603608529250956,
      "grad_norm": 1.3766671419143677,
      "learning_rate": 4.289643326741041e-05,
      "loss": 2.906,
      "step": 2700
    },
    {
      "epoch": 0.24612720976854383,
      "grad_norm": 1.801680326461792,
      "learning_rate": 4.2891435253482256e-05,
      "loss": 2.8031,
      "step": 2701
    },
    {
      "epoch": 0.2462183342445781,
      "grad_norm": 2.4005794525146484,
      "learning_rate": 4.288643577328051e-05,
      "loss": 3.1263,
      "step": 2702
    },
    {
      "epoch": 0.24630945872061236,
      "grad_norm": 2.0800743103027344,
      "learning_rate": 4.28814348272149e-05,
      "loss": 3.0734,
      "step": 2703
    },
    {
      "epoch": 0.24640058319664662,
      "grad_norm": 1.9415937662124634,
      "learning_rate": 4.287643241569528e-05,
      "loss": 3.1275,
      "step": 2704
    },
    {
      "epoch": 0.2464917076726809,
      "grad_norm": 1.3378267288208008,
      "learning_rate": 4.2871428539131606e-05,
      "loss": 2.7699,
      "step": 2705
    },
    {
      "epoch": 0.24658283214871515,
      "grad_norm": 1.343563437461853,
      "learning_rate": 4.2866423197933985e-05,
      "loss": 2.8825,
      "step": 2706
    },
    {
      "epoch": 0.24667395662474942,
      "grad_norm": 1.729950189590454,
      "learning_rate": 4.286141639251261e-05,
      "loss": 2.9122,
      "step": 2707
    },
    {
      "epoch": 0.24676508110078368,
      "grad_norm": 1.2192318439483643,
      "learning_rate": 4.2856408123277806e-05,
      "loss": 2.8577,
      "step": 2708
    },
    {
      "epoch": 0.24685620557681792,
      "grad_norm": 2.4212095737457275,
      "learning_rate": 4.2851398390640024e-05,
      "loss": 2.6407,
      "step": 2709
    },
    {
      "epoch": 0.2469473300528522,
      "grad_norm": 2.7352535724639893,
      "learning_rate": 4.284638719500983e-05,
      "loss": 2.8367,
      "step": 2710
    },
    {
      "epoch": 0.24703845452888645,
      "grad_norm": 1.5762912034988403,
      "learning_rate": 4.2841374536797916e-05,
      "loss": 3.0508,
      "step": 2711
    },
    {
      "epoch": 0.24712957900492072,
      "grad_norm": 3.1649091243743896,
      "learning_rate": 4.283636041641509e-05,
      "loss": 3.0962,
      "step": 2712
    },
    {
      "epoch": 0.24722070348095498,
      "grad_norm": 2.29734206199646,
      "learning_rate": 4.283134483427227e-05,
      "loss": 3.3864,
      "step": 2713
    },
    {
      "epoch": 0.24731182795698925,
      "grad_norm": 1.7747163772583008,
      "learning_rate": 4.282632779078051e-05,
      "loss": 3.104,
      "step": 2714
    },
    {
      "epoch": 0.24740295243302352,
      "grad_norm": 2.981482744216919,
      "learning_rate": 4.2821309286350974e-05,
      "loss": 3.9566,
      "step": 2715
    },
    {
      "epoch": 0.24749407690905778,
      "grad_norm": 1.2930676937103271,
      "learning_rate": 4.281628932139495e-05,
      "loss": 3.0148,
      "step": 2716
    },
    {
      "epoch": 0.24758520138509205,
      "grad_norm": 2.1940977573394775,
      "learning_rate": 4.2811267896323845e-05,
      "loss": 3.2,
      "step": 2717
    },
    {
      "epoch": 0.2476763258611263,
      "grad_norm": 2.1138763427734375,
      "learning_rate": 4.280624501154918e-05,
      "loss": 2.4389,
      "step": 2718
    },
    {
      "epoch": 0.24776745033716055,
      "grad_norm": 2.007523775100708,
      "learning_rate": 4.28012206674826e-05,
      "loss": 3.097,
      "step": 2719
    },
    {
      "epoch": 0.24785857481319482,
      "grad_norm": 1.5791462659835815,
      "learning_rate": 4.279619486453588e-05,
      "loss": 2.9226,
      "step": 2720
    },
    {
      "epoch": 0.24794969928922908,
      "grad_norm": 1.772363305091858,
      "learning_rate": 4.279116760312089e-05,
      "loss": 3.1598,
      "step": 2721
    },
    {
      "epoch": 0.24804082376526335,
      "grad_norm": 1.4500844478607178,
      "learning_rate": 4.278613888364964e-05,
      "loss": 2.9163,
      "step": 2722
    },
    {
      "epoch": 0.2481319482412976,
      "grad_norm": 1.393494725227356,
      "learning_rate": 4.278110870653425e-05,
      "loss": 3.066,
      "step": 2723
    },
    {
      "epoch": 0.24822307271733188,
      "grad_norm": 2.8598203659057617,
      "learning_rate": 4.277607707218697e-05,
      "loss": 3.2181,
      "step": 2724
    },
    {
      "epoch": 0.24831419719336614,
      "grad_norm": 2.300021171569824,
      "learning_rate": 4.277104398102016e-05,
      "loss": 2.9812,
      "step": 2725
    },
    {
      "epoch": 0.2484053216694004,
      "grad_norm": 2.6022722721099854,
      "learning_rate": 4.27660094334463e-05,
      "loss": 3.1024,
      "step": 2726
    },
    {
      "epoch": 0.24849644614543467,
      "grad_norm": 2.1195859909057617,
      "learning_rate": 4.2760973429877995e-05,
      "loss": 3.0905,
      "step": 2727
    },
    {
      "epoch": 0.24858757062146894,
      "grad_norm": 2.173823595046997,
      "learning_rate": 4.275593597072796e-05,
      "loss": 3.0143,
      "step": 2728
    },
    {
      "epoch": 0.24867869509750318,
      "grad_norm": 1.9971680641174316,
      "learning_rate": 4.2750897056409036e-05,
      "loss": 3.1087,
      "step": 2729
    },
    {
      "epoch": 0.24876981957353744,
      "grad_norm": 2.5114338397979736,
      "learning_rate": 4.274585668733418e-05,
      "loss": 3.1582,
      "step": 2730
    },
    {
      "epoch": 0.2488609440495717,
      "grad_norm": 2.2681045532226562,
      "learning_rate": 4.2740814863916476e-05,
      "loss": 2.9436,
      "step": 2731
    },
    {
      "epoch": 0.24895206852560597,
      "grad_norm": 2.3320324420928955,
      "learning_rate": 4.2735771586569116e-05,
      "loss": 2.8523,
      "step": 2732
    },
    {
      "epoch": 0.24904319300164024,
      "grad_norm": 1.6849809885025024,
      "learning_rate": 4.273072685570542e-05,
      "loss": 3.1959,
      "step": 2733
    },
    {
      "epoch": 0.2491343174776745,
      "grad_norm": 1.4265391826629639,
      "learning_rate": 4.272568067173882e-05,
      "loss": 2.9005,
      "step": 2734
    },
    {
      "epoch": 0.24922544195370877,
      "grad_norm": 2.4876840114593506,
      "learning_rate": 4.272063303508287e-05,
      "loss": 3.042,
      "step": 2735
    },
    {
      "epoch": 0.24931656642974304,
      "grad_norm": 2.4077513217926025,
      "learning_rate": 4.271558394615125e-05,
      "loss": 3.0062,
      "step": 2736
    },
    {
      "epoch": 0.2494076909057773,
      "grad_norm": 1.4401582479476929,
      "learning_rate": 4.271053340535774e-05,
      "loss": 3.0109,
      "step": 2737
    },
    {
      "epoch": 0.24949881538181157,
      "grad_norm": 2.2177035808563232,
      "learning_rate": 4.2705481413116265e-05,
      "loss": 2.7662,
      "step": 2738
    },
    {
      "epoch": 0.2495899398578458,
      "grad_norm": 1.1132807731628418,
      "learning_rate": 4.270042796984086e-05,
      "loss": 2.8683,
      "step": 2739
    },
    {
      "epoch": 0.24968106433388007,
      "grad_norm": 3.3413302898406982,
      "learning_rate": 4.269537307594566e-05,
      "loss": 3.3031,
      "step": 2740
    },
    {
      "epoch": 0.24977218880991434,
      "grad_norm": 3.5505168437957764,
      "learning_rate": 4.2690316731844934e-05,
      "loss": 4.2319,
      "step": 2741
    },
    {
      "epoch": 0.2498633132859486,
      "grad_norm": 1.3017137050628662,
      "learning_rate": 4.268525893795308e-05,
      "loss": 2.9058,
      "step": 2742
    },
    {
      "epoch": 0.24995443776198287,
      "grad_norm": 2.826502561569214,
      "learning_rate": 4.2680199694684594e-05,
      "loss": 4.2936,
      "step": 2743
    },
    {
      "epoch": 0.2500455622380171,
      "grad_norm": 2.0487308502197266,
      "learning_rate": 4.267513900245412e-05,
      "loss": 3.1855,
      "step": 2744
    },
    {
      "epoch": 0.25013668671405137,
      "grad_norm": 1.4321306943893433,
      "learning_rate": 4.2670076861676376e-05,
      "loss": 2.8652,
      "step": 2745
    },
    {
      "epoch": 0.25022781119008564,
      "grad_norm": 2.311816692352295,
      "learning_rate": 4.266501327276623e-05,
      "loss": 2.4795,
      "step": 2746
    },
    {
      "epoch": 0.2503189356661199,
      "grad_norm": 2.813767433166504,
      "learning_rate": 4.2659948236138684e-05,
      "loss": 3.1148,
      "step": 2747
    },
    {
      "epoch": 0.25041006014215417,
      "grad_norm": 2.127753257751465,
      "learning_rate": 4.265488175220881e-05,
      "loss": 3.0461,
      "step": 2748
    },
    {
      "epoch": 0.25050118461818843,
      "grad_norm": 1.8213751316070557,
      "learning_rate": 4.264981382139184e-05,
      "loss": 2.5706,
      "step": 2749
    },
    {
      "epoch": 0.2505923090942227,
      "grad_norm": 2.5473597049713135,
      "learning_rate": 4.264474444410311e-05,
      "loss": 2.6984,
      "step": 2750
    },
    {
      "epoch": 0.25068343357025696,
      "grad_norm": 1.9716181755065918,
      "learning_rate": 4.263967362075808e-05,
      "loss": 3.0983,
      "step": 2751
    },
    {
      "epoch": 0.25077455804629123,
      "grad_norm": 2.9187302589416504,
      "learning_rate": 4.263460135177231e-05,
      "loss": 2.881,
      "step": 2752
    },
    {
      "epoch": 0.2508656825223255,
      "grad_norm": 2.36059308052063,
      "learning_rate": 4.2629527637561506e-05,
      "loss": 3.0413,
      "step": 2753
    },
    {
      "epoch": 0.25095680699835976,
      "grad_norm": 2.2453083992004395,
      "learning_rate": 4.2624452478541476e-05,
      "loss": 3.1863,
      "step": 2754
    },
    {
      "epoch": 0.251047931474394,
      "grad_norm": 1.7847957611083984,
      "learning_rate": 4.2619375875128155e-05,
      "loss": 2.8671,
      "step": 2755
    },
    {
      "epoch": 0.2511390559504283,
      "grad_norm": 5.0593156814575195,
      "learning_rate": 4.261429782773757e-05,
      "loss": 3.2455,
      "step": 2756
    },
    {
      "epoch": 0.25123018042646256,
      "grad_norm": 1.4512591361999512,
      "learning_rate": 4.2609218336785905e-05,
      "loss": 2.9477,
      "step": 2757
    },
    {
      "epoch": 0.2513213049024968,
      "grad_norm": 2.3737614154815674,
      "learning_rate": 4.260413740268944e-05,
      "loss": 3.133,
      "step": 2758
    },
    {
      "epoch": 0.2514124293785311,
      "grad_norm": 2.759577751159668,
      "learning_rate": 4.259905502586458e-05,
      "loss": 3.0107,
      "step": 2759
    },
    {
      "epoch": 0.25150355385456535,
      "grad_norm": 1.5885424613952637,
      "learning_rate": 4.259397120672784e-05,
      "loss": 2.9339,
      "step": 2760
    },
    {
      "epoch": 0.2515946783305996,
      "grad_norm": 2.113294839859009,
      "learning_rate": 4.2588885945695854e-05,
      "loss": 2.6084,
      "step": 2761
    },
    {
      "epoch": 0.2516858028066339,
      "grad_norm": 3.36124587059021,
      "learning_rate": 4.2583799243185396e-05,
      "loss": 3.2577,
      "step": 2762
    },
    {
      "epoch": 0.25177692728266815,
      "grad_norm": 2.2398228645324707,
      "learning_rate": 4.257871109961333e-05,
      "loss": 3.0683,
      "step": 2763
    },
    {
      "epoch": 0.25186805175870236,
      "grad_norm": 1.4189859628677368,
      "learning_rate": 4.257362151539666e-05,
      "loss": 2.9033,
      "step": 2764
    },
    {
      "epoch": 0.2519591762347366,
      "grad_norm": 1.2338752746582031,
      "learning_rate": 4.256853049095247e-05,
      "loss": 2.8821,
      "step": 2765
    },
    {
      "epoch": 0.2520503007107709,
      "grad_norm": 1.3947806358337402,
      "learning_rate": 4.256343802669802e-05,
      "loss": 2.9032,
      "step": 2766
    },
    {
      "epoch": 0.25214142518680516,
      "grad_norm": 2.2239599227905273,
      "learning_rate": 4.255834412305065e-05,
      "loss": 3.0917,
      "step": 2767
    },
    {
      "epoch": 0.2522325496628394,
      "grad_norm": 1.1595191955566406,
      "learning_rate": 4.2553248780427815e-05,
      "loss": 2.9436,
      "step": 2768
    },
    {
      "epoch": 0.2523236741388737,
      "grad_norm": 1.0117340087890625,
      "learning_rate": 4.2548151999247105e-05,
      "loss": 2.7456,
      "step": 2769
    },
    {
      "epoch": 0.25241479861490795,
      "grad_norm": 1.8710023164749146,
      "learning_rate": 4.254305377992622e-05,
      "loss": 3.145,
      "step": 2770
    },
    {
      "epoch": 0.2525059230909422,
      "grad_norm": 2.228285789489746,
      "learning_rate": 4.253795412288299e-05,
      "loss": 2.895,
      "step": 2771
    },
    {
      "epoch": 0.2525970475669765,
      "grad_norm": 2.5610108375549316,
      "learning_rate": 4.253285302853533e-05,
      "loss": 2.9391,
      "step": 2772
    },
    {
      "epoch": 0.25268817204301075,
      "grad_norm": 2.729546546936035,
      "learning_rate": 4.2527750497301323e-05,
      "loss": 2.6163,
      "step": 2773
    },
    {
      "epoch": 0.252779296519045,
      "grad_norm": 1.9866018295288086,
      "learning_rate": 4.252264652959911e-05,
      "loss": 3.0333,
      "step": 2774
    },
    {
      "epoch": 0.2528704209950793,
      "grad_norm": 1.9674807786941528,
      "learning_rate": 4.2517541125847005e-05,
      "loss": 3.2293,
      "step": 2775
    },
    {
      "epoch": 0.25296154547111355,
      "grad_norm": 2.267971992492676,
      "learning_rate": 4.251243428646341e-05,
      "loss": 3.2709,
      "step": 2776
    },
    {
      "epoch": 0.2530526699471478,
      "grad_norm": 2.127988576889038,
      "learning_rate": 4.250732601186685e-05,
      "loss": 3.0816,
      "step": 2777
    },
    {
      "epoch": 0.2531437944231821,
      "grad_norm": 2.273996353149414,
      "learning_rate": 4.250221630247595e-05,
      "loss": 2.5128,
      "step": 2778
    },
    {
      "epoch": 0.25323491889921634,
      "grad_norm": 2.3379149436950684,
      "learning_rate": 4.24971051587095e-05,
      "loss": 3.0336,
      "step": 2779
    },
    {
      "epoch": 0.2533260433752506,
      "grad_norm": 2.801602840423584,
      "learning_rate": 4.249199258098637e-05,
      "loss": 3.1966,
      "step": 2780
    },
    {
      "epoch": 0.2534171678512849,
      "grad_norm": 4.067786693572998,
      "learning_rate": 4.248687856972555e-05,
      "loss": 3.0002,
      "step": 2781
    },
    {
      "epoch": 0.25350829232731914,
      "grad_norm": 1.291828989982605,
      "learning_rate": 4.248176312534615e-05,
      "loss": 2.9576,
      "step": 2782
    },
    {
      "epoch": 0.2535994168033534,
      "grad_norm": 2.59358811378479,
      "learning_rate": 4.24766462482674e-05,
      "loss": 2.9876,
      "step": 2783
    },
    {
      "epoch": 0.2536905412793876,
      "grad_norm": 2.164653778076172,
      "learning_rate": 4.2471527938908675e-05,
      "loss": 3.1204,
      "step": 2784
    },
    {
      "epoch": 0.2537816657554219,
      "grad_norm": 1.1405891180038452,
      "learning_rate": 4.24664081976894e-05,
      "loss": 2.7754,
      "step": 2785
    },
    {
      "epoch": 0.25387279023145615,
      "grad_norm": 1.9741638898849487,
      "learning_rate": 4.246128702502919e-05,
      "loss": 3.1346,
      "step": 2786
    },
    {
      "epoch": 0.2539639147074904,
      "grad_norm": 2.037536859512329,
      "learning_rate": 4.245616442134772e-05,
      "loss": 3.1485,
      "step": 2787
    },
    {
      "epoch": 0.2540550391835247,
      "grad_norm": 2.2942094802856445,
      "learning_rate": 4.245104038706483e-05,
      "loss": 3.0839,
      "step": 2788
    },
    {
      "epoch": 0.25414616365955894,
      "grad_norm": 2.56900691986084,
      "learning_rate": 4.244591492260045e-05,
      "loss": 3.0433,
      "step": 2789
    },
    {
      "epoch": 0.2542372881355932,
      "grad_norm": 2.761805295944214,
      "learning_rate": 4.2440788028374624e-05,
      "loss": 2.4214,
      "step": 2790
    },
    {
      "epoch": 0.2543284126116275,
      "grad_norm": 2.236682891845703,
      "learning_rate": 4.2435659704807515e-05,
      "loss": 3.2713,
      "step": 2791
    },
    {
      "epoch": 0.25441953708766174,
      "grad_norm": 1.6430342197418213,
      "learning_rate": 4.243052995231943e-05,
      "loss": 2.8876,
      "step": 2792
    },
    {
      "epoch": 0.254510661563696,
      "grad_norm": 1.7797034978866577,
      "learning_rate": 4.242539877133076e-05,
      "loss": 3.0051,
      "step": 2793
    },
    {
      "epoch": 0.25460178603973027,
      "grad_norm": 1.2564404010772705,
      "learning_rate": 4.242026616226202e-05,
      "loss": 2.9564,
      "step": 2794
    },
    {
      "epoch": 0.25469291051576454,
      "grad_norm": 2.3697896003723145,
      "learning_rate": 4.241513212553386e-05,
      "loss": 2.7262,
      "step": 2795
    },
    {
      "epoch": 0.2547840349917988,
      "grad_norm": 1.4234386682510376,
      "learning_rate": 4.240999666156703e-05,
      "loss": 2.9647,
      "step": 2796
    },
    {
      "epoch": 0.25487515946783307,
      "grad_norm": 3.489351511001587,
      "learning_rate": 4.2404859770782397e-05,
      "loss": 2.7326,
      "step": 2797
    },
    {
      "epoch": 0.25496628394386733,
      "grad_norm": 2.659078598022461,
      "learning_rate": 4.239972145360095e-05,
      "loss": 2.811,
      "step": 2798
    },
    {
      "epoch": 0.2550574084199016,
      "grad_norm": 2.561401128768921,
      "learning_rate": 4.2394581710443806e-05,
      "loss": 4.4293,
      "step": 2799
    },
    {
      "epoch": 0.25514853289593586,
      "grad_norm": 1.787708044052124,
      "learning_rate": 4.2389440541732173e-05,
      "loss": 2.899,
      "step": 2800
    },
    {
      "epoch": 0.25523965737197013,
      "grad_norm": 1.4970817565917969,
      "learning_rate": 4.23842979478874e-05,
      "loss": 2.9044,
      "step": 2801
    },
    {
      "epoch": 0.2553307818480044,
      "grad_norm": 1.1706899404525757,
      "learning_rate": 4.2379153929330926e-05,
      "loss": 2.8384,
      "step": 2802
    },
    {
      "epoch": 0.25542190632403866,
      "grad_norm": 1.1908305883407593,
      "learning_rate": 4.237400848648435e-05,
      "loss": 2.8336,
      "step": 2803
    },
    {
      "epoch": 0.2555130308000729,
      "grad_norm": 2.9013776779174805,
      "learning_rate": 4.236886161976934e-05,
      "loss": 3.036,
      "step": 2804
    },
    {
      "epoch": 0.25560415527610714,
      "grad_norm": 2.4012398719787598,
      "learning_rate": 4.236371332960771e-05,
      "loss": 3.0429,
      "step": 2805
    },
    {
      "epoch": 0.2556952797521414,
      "grad_norm": 2.5297155380249023,
      "learning_rate": 4.235856361642138e-05,
      "loss": 2.5262,
      "step": 2806
    },
    {
      "epoch": 0.25578640422817567,
      "grad_norm": 2.3616104125976562,
      "learning_rate": 4.235341248063239e-05,
      "loss": 2.958,
      "step": 2807
    },
    {
      "epoch": 0.25587752870420993,
      "grad_norm": 2.602342367172241,
      "learning_rate": 4.23482599226629e-05,
      "loss": 2.8596,
      "step": 2808
    },
    {
      "epoch": 0.2559686531802442,
      "grad_norm": 1.4756160974502563,
      "learning_rate": 4.234310594293518e-05,
      "loss": 2.9267,
      "step": 2809
    },
    {
      "epoch": 0.25605977765627846,
      "grad_norm": 1.2815459966659546,
      "learning_rate": 4.233795054187162e-05,
      "loss": 2.8097,
      "step": 2810
    },
    {
      "epoch": 0.25615090213231273,
      "grad_norm": 1.8592561483383179,
      "learning_rate": 4.233279371989472e-05,
      "loss": 3.2458,
      "step": 2811
    },
    {
      "epoch": 0.256242026608347,
      "grad_norm": 2.3240275382995605,
      "learning_rate": 4.2327635477427105e-05,
      "loss": 3.0347,
      "step": 2812
    },
    {
      "epoch": 0.25633315108438126,
      "grad_norm": 2.4634056091308594,
      "learning_rate": 4.2322475814891515e-05,
      "loss": 3.1207,
      "step": 2813
    },
    {
      "epoch": 0.2564242755604155,
      "grad_norm": 2.606762170791626,
      "learning_rate": 4.231731473271081e-05,
      "loss": 2.8514,
      "step": 2814
    },
    {
      "epoch": 0.2565154000364498,
      "grad_norm": 2.147658348083496,
      "learning_rate": 4.231215223130795e-05,
      "loss": 3.0548,
      "step": 2815
    },
    {
      "epoch": 0.25660652451248406,
      "grad_norm": 3.1043879985809326,
      "learning_rate": 4.2306988311106013e-05,
      "loss": 3.0865,
      "step": 2816
    },
    {
      "epoch": 0.2566976489885183,
      "grad_norm": 3.8914921283721924,
      "learning_rate": 4.230182297252823e-05,
      "loss": 3.0735,
      "step": 2817
    },
    {
      "epoch": 0.2567887734645526,
      "grad_norm": 1.857240080833435,
      "learning_rate": 4.229665621599791e-05,
      "loss": 3.0808,
      "step": 2818
    },
    {
      "epoch": 0.25687989794058685,
      "grad_norm": 2.3716938495635986,
      "learning_rate": 4.229148804193848e-05,
      "loss": 3.0159,
      "step": 2819
    },
    {
      "epoch": 0.2569710224166211,
      "grad_norm": 1.4991905689239502,
      "learning_rate": 4.22863184507735e-05,
      "loss": 2.925,
      "step": 2820
    },
    {
      "epoch": 0.2570621468926554,
      "grad_norm": 2.2310595512390137,
      "learning_rate": 4.228114744292664e-05,
      "loss": 2.8327,
      "step": 2821
    },
    {
      "epoch": 0.25715327136868965,
      "grad_norm": 3.5891966819763184,
      "learning_rate": 4.227597501882168e-05,
      "loss": 2.9454,
      "step": 2822
    },
    {
      "epoch": 0.2572443958447239,
      "grad_norm": 3.6419284343719482,
      "learning_rate": 4.2270801178882514e-05,
      "loss": 3.1813,
      "step": 2823
    },
    {
      "epoch": 0.2573355203207582,
      "grad_norm": 1.979240894317627,
      "learning_rate": 4.226562592353318e-05,
      "loss": 2.8899,
      "step": 2824
    },
    {
      "epoch": 0.2574266447967924,
      "grad_norm": 1.8315885066986084,
      "learning_rate": 4.2260449253197786e-05,
      "loss": 3.0892,
      "step": 2825
    },
    {
      "epoch": 0.25751776927282666,
      "grad_norm": 2.027369976043701,
      "learning_rate": 4.2255271168300604e-05,
      "loss": 2.9498,
      "step": 2826
    },
    {
      "epoch": 0.2576088937488609,
      "grad_norm": 1.354431390762329,
      "learning_rate": 4.225009166926597e-05,
      "loss": 2.9251,
      "step": 2827
    },
    {
      "epoch": 0.2577000182248952,
      "grad_norm": 2.05568265914917,
      "learning_rate": 4.224491075651839e-05,
      "loss": 3.0467,
      "step": 2828
    },
    {
      "epoch": 0.25779114270092945,
      "grad_norm": 2.29907488822937,
      "learning_rate": 4.223972843048245e-05,
      "loss": 2.8689,
      "step": 2829
    },
    {
      "epoch": 0.2578822671769637,
      "grad_norm": 1.9548076391220093,
      "learning_rate": 4.2234544691582856e-05,
      "loss": 2.9526,
      "step": 2830
    },
    {
      "epoch": 0.257973391652998,
      "grad_norm": 3.6491405963897705,
      "learning_rate": 4.222935954024445e-05,
      "loss": 3.1412,
      "step": 2831
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 2.1254284381866455,
      "learning_rate": 4.222417297689217e-05,
      "loss": 3.1294,
      "step": 2832
    },
    {
      "epoch": 0.2581556406050665,
      "grad_norm": 2.349571943283081,
      "learning_rate": 4.2218985001951065e-05,
      "loss": 3.2577,
      "step": 2833
    },
    {
      "epoch": 0.2582467650811008,
      "grad_norm": 1.220754861831665,
      "learning_rate": 4.221379561584632e-05,
      "loss": 2.9234,
      "step": 2834
    },
    {
      "epoch": 0.25833788955713505,
      "grad_norm": 2.453462600708008,
      "learning_rate": 4.220860481900323e-05,
      "loss": 2.9296,
      "step": 2835
    },
    {
      "epoch": 0.2584290140331693,
      "grad_norm": 2.142008066177368,
      "learning_rate": 4.220341261184719e-05,
      "loss": 3.0596,
      "step": 2836
    },
    {
      "epoch": 0.2585201385092036,
      "grad_norm": 1.4604429006576538,
      "learning_rate": 4.219821899480373e-05,
      "loss": 2.9582,
      "step": 2837
    },
    {
      "epoch": 0.25861126298523784,
      "grad_norm": 3.274620771408081,
      "learning_rate": 4.219302396829848e-05,
      "loss": 2.8971,
      "step": 2838
    },
    {
      "epoch": 0.2587023874612721,
      "grad_norm": 2.949350357055664,
      "learning_rate": 4.21878275327572e-05,
      "loss": 3.1399,
      "step": 2839
    },
    {
      "epoch": 0.2587935119373064,
      "grad_norm": 1.464654564857483,
      "learning_rate": 4.218262968860575e-05,
      "loss": 2.9169,
      "step": 2840
    },
    {
      "epoch": 0.25888463641334064,
      "grad_norm": 2.5851809978485107,
      "learning_rate": 4.217743043627013e-05,
      "loss": 3.0918,
      "step": 2841
    },
    {
      "epoch": 0.2589757608893749,
      "grad_norm": 3.0708165168762207,
      "learning_rate": 4.217222977617642e-05,
      "loss": 2.0169,
      "step": 2842
    },
    {
      "epoch": 0.25906688536540917,
      "grad_norm": 2.1215906143188477,
      "learning_rate": 4.2167027708750846e-05,
      "loss": 2.5325,
      "step": 2843
    },
    {
      "epoch": 0.25915800984144344,
      "grad_norm": 2.9789392948150635,
      "learning_rate": 4.2161824234419735e-05,
      "loss": 3.0718,
      "step": 2844
    },
    {
      "epoch": 0.25924913431747765,
      "grad_norm": 1.9051270484924316,
      "learning_rate": 4.215661935360953e-05,
      "loss": 2.9504,
      "step": 2845
    },
    {
      "epoch": 0.2593402587935119,
      "grad_norm": 2.4061474800109863,
      "learning_rate": 4.2151413066746806e-05,
      "loss": 3.101,
      "step": 2846
    },
    {
      "epoch": 0.2594313832695462,
      "grad_norm": 2.7923941612243652,
      "learning_rate": 4.214620537425821e-05,
      "loss": 3.7933,
      "step": 2847
    },
    {
      "epoch": 0.25952250774558044,
      "grad_norm": 9.324788093566895,
      "learning_rate": 4.214099627657055e-05,
      "loss": 3.2573,
      "step": 2848
    },
    {
      "epoch": 0.2596136322216147,
      "grad_norm": 1.2549262046813965,
      "learning_rate": 4.2135785774110745e-05,
      "loss": 2.8284,
      "step": 2849
    },
    {
      "epoch": 0.259704756697649,
      "grad_norm": 3.186706066131592,
      "learning_rate": 4.213057386730579e-05,
      "loss": 2.9754,
      "step": 2850
    },
    {
      "epoch": 0.25979588117368324,
      "grad_norm": 2.899864435195923,
      "learning_rate": 4.212536055658284e-05,
      "loss": 2.8379,
      "step": 2851
    },
    {
      "epoch": 0.2598870056497175,
      "grad_norm": 2.50211238861084,
      "learning_rate": 4.212014584236914e-05,
      "loss": 2.7055,
      "step": 2852
    },
    {
      "epoch": 0.25997813012575177,
      "grad_norm": 2.430344581604004,
      "learning_rate": 4.2114929725092056e-05,
      "loss": 3.2453,
      "step": 2853
    },
    {
      "epoch": 0.26006925460178604,
      "grad_norm": 1.6242897510528564,
      "learning_rate": 4.2109712205179075e-05,
      "loss": 2.8145,
      "step": 2854
    },
    {
      "epoch": 0.2601603790778203,
      "grad_norm": 2.620756149291992,
      "learning_rate": 4.2104493283057775e-05,
      "loss": 2.976,
      "step": 2855
    },
    {
      "epoch": 0.26025150355385457,
      "grad_norm": 1.797751545906067,
      "learning_rate": 4.209927295915589e-05,
      "loss": 2.9425,
      "step": 2856
    },
    {
      "epoch": 0.26034262802988883,
      "grad_norm": 1.9420567750930786,
      "learning_rate": 4.209405123390124e-05,
      "loss": 3.1011,
      "step": 2857
    },
    {
      "epoch": 0.2604337525059231,
      "grad_norm": 1.9514185190200806,
      "learning_rate": 4.208882810772175e-05,
      "loss": 2.6563,
      "step": 2858
    },
    {
      "epoch": 0.26052487698195737,
      "grad_norm": 1.9131776094436646,
      "learning_rate": 4.2083603581045506e-05,
      "loss": 2.8919,
      "step": 2859
    },
    {
      "epoch": 0.26061600145799163,
      "grad_norm": 1.5755358934402466,
      "learning_rate": 4.2078377654300647e-05,
      "loss": 3.0312,
      "step": 2860
    },
    {
      "epoch": 0.2607071259340259,
      "grad_norm": 1.7530688047409058,
      "learning_rate": 4.2073150327915475e-05,
      "loss": 2.7744,
      "step": 2861
    },
    {
      "epoch": 0.26079825041006016,
      "grad_norm": 1.400647521018982,
      "learning_rate": 4.2067921602318395e-05,
      "loss": 2.8983,
      "step": 2862
    },
    {
      "epoch": 0.2608893748860944,
      "grad_norm": 1.9473682641983032,
      "learning_rate": 4.206269147793791e-05,
      "loss": 2.9917,
      "step": 2863
    },
    {
      "epoch": 0.2609804993621287,
      "grad_norm": 2.2301454544067383,
      "learning_rate": 4.205745995520265e-05,
      "loss": 3.1024,
      "step": 2864
    },
    {
      "epoch": 0.2610716238381629,
      "grad_norm": 1.9916133880615234,
      "learning_rate": 4.205222703454136e-05,
      "loss": 2.7725,
      "step": 2865
    },
    {
      "epoch": 0.26116274831419717,
      "grad_norm": 1.5755904912948608,
      "learning_rate": 4.20469927163829e-05,
      "loss": 3.069,
      "step": 2866
    },
    {
      "epoch": 0.26125387279023143,
      "grad_norm": 2.0395994186401367,
      "learning_rate": 4.204175700115625e-05,
      "loss": 3.1065,
      "step": 2867
    },
    {
      "epoch": 0.2613449972662657,
      "grad_norm": 2.2116832733154297,
      "learning_rate": 4.203651988929049e-05,
      "loss": 3.3151,
      "step": 2868
    },
    {
      "epoch": 0.26143612174229997,
      "grad_norm": 1.7351018190383911,
      "learning_rate": 4.2031281381214815e-05,
      "loss": 2.9803,
      "step": 2869
    },
    {
      "epoch": 0.26152724621833423,
      "grad_norm": 1.9820733070373535,
      "learning_rate": 4.2026041477358556e-05,
      "loss": 3.0491,
      "step": 2870
    },
    {
      "epoch": 0.2616183706943685,
      "grad_norm": 2.00107741355896,
      "learning_rate": 4.202080017815114e-05,
      "loss": 2.7203,
      "step": 2871
    },
    {
      "epoch": 0.26170949517040276,
      "grad_norm": 1.942094087600708,
      "learning_rate": 4.2015557484022104e-05,
      "loss": 2.9808,
      "step": 2872
    },
    {
      "epoch": 0.261800619646437,
      "grad_norm": 2.569413423538208,
      "learning_rate": 4.2010313395401114e-05,
      "loss": 2.8955,
      "step": 2873
    },
    {
      "epoch": 0.2618917441224713,
      "grad_norm": 1.4627418518066406,
      "learning_rate": 4.2005067912717945e-05,
      "loss": 3.0037,
      "step": 2874
    },
    {
      "epoch": 0.26198286859850556,
      "grad_norm": 3.213196277618408,
      "learning_rate": 4.1999821036402476e-05,
      "loss": 4.1602,
      "step": 2875
    },
    {
      "epoch": 0.2620739930745398,
      "grad_norm": 2.5163495540618896,
      "learning_rate": 4.199457276688472e-05,
      "loss": 2.6366,
      "step": 2876
    },
    {
      "epoch": 0.2621651175505741,
      "grad_norm": 1.687812328338623,
      "learning_rate": 4.1989323104594784e-05,
      "loss": 2.8833,
      "step": 2877
    },
    {
      "epoch": 0.26225624202660835,
      "grad_norm": 3.4560883045196533,
      "learning_rate": 4.198407204996291e-05,
      "loss": 2.7973,
      "step": 2878
    },
    {
      "epoch": 0.2623473665026426,
      "grad_norm": 2.7664990425109863,
      "learning_rate": 4.1978819603419425e-05,
      "loss": 2.8157,
      "step": 2879
    },
    {
      "epoch": 0.2624384909786769,
      "grad_norm": 2.8775832653045654,
      "learning_rate": 4.197356576539481e-05,
      "loss": 2.6991,
      "step": 2880
    },
    {
      "epoch": 0.26252961545471115,
      "grad_norm": 2.5702507495880127,
      "learning_rate": 4.196831053631962e-05,
      "loss": 3.136,
      "step": 2881
    },
    {
      "epoch": 0.2626207399307454,
      "grad_norm": 2.569242477416992,
      "learning_rate": 4.1963053916624554e-05,
      "loss": 3.2191,
      "step": 2882
    },
    {
      "epoch": 0.2627118644067797,
      "grad_norm": 2.0394222736358643,
      "learning_rate": 4.195779590674041e-05,
      "loss": 3.3191,
      "step": 2883
    },
    {
      "epoch": 0.26280298888281395,
      "grad_norm": 2.271517038345337,
      "learning_rate": 4.195253650709808e-05,
      "loss": 2.9842,
      "step": 2884
    },
    {
      "epoch": 0.2628941133588482,
      "grad_norm": 2.832256555557251,
      "learning_rate": 4.194727571812863e-05,
      "loss": 3.2062,
      "step": 2885
    },
    {
      "epoch": 0.2629852378348824,
      "grad_norm": 2.3550362586975098,
      "learning_rate": 4.194201354026318e-05,
      "loss": 2.9997,
      "step": 2886
    },
    {
      "epoch": 0.2630763623109167,
      "grad_norm": 3.5529887676239014,
      "learning_rate": 4.1936749973932986e-05,
      "loss": 3.092,
      "step": 2887
    },
    {
      "epoch": 0.26316748678695095,
      "grad_norm": 2.2113375663757324,
      "learning_rate": 4.1931485019569425e-05,
      "loss": 3.1864,
      "step": 2888
    },
    {
      "epoch": 0.2632586112629852,
      "grad_norm": 3.2207071781158447,
      "learning_rate": 4.1926218677603984e-05,
      "loss": 2.9743,
      "step": 2889
    },
    {
      "epoch": 0.2633497357390195,
      "grad_norm": 1.705082654953003,
      "learning_rate": 4.192095094846825e-05,
      "loss": 3.1398,
      "step": 2890
    },
    {
      "epoch": 0.26344086021505375,
      "grad_norm": 1.941869854927063,
      "learning_rate": 4.191568183259394e-05,
      "loss": 2.683,
      "step": 2891
    },
    {
      "epoch": 0.263531984691088,
      "grad_norm": 2.2992329597473145,
      "learning_rate": 4.1910411330412877e-05,
      "loss": 2.9955,
      "step": 2892
    },
    {
      "epoch": 0.2636231091671223,
      "grad_norm": 1.9153778553009033,
      "learning_rate": 4.190513944235699e-05,
      "loss": 3.0078,
      "step": 2893
    },
    {
      "epoch": 0.26371423364315655,
      "grad_norm": 1.2461416721343994,
      "learning_rate": 4.189986616885835e-05,
      "loss": 2.8457,
      "step": 2894
    },
    {
      "epoch": 0.2638053581191908,
      "grad_norm": 1.2349356412887573,
      "learning_rate": 4.189459151034912e-05,
      "loss": 2.9239,
      "step": 2895
    },
    {
      "epoch": 0.2638964825952251,
      "grad_norm": 3.2244863510131836,
      "learning_rate": 4.188931546726156e-05,
      "loss": 2.9746,
      "step": 2896
    },
    {
      "epoch": 0.26398760707125934,
      "grad_norm": 2.0441231727600098,
      "learning_rate": 4.1884038040028085e-05,
      "loss": 3.1106,
      "step": 2897
    },
    {
      "epoch": 0.2640787315472936,
      "grad_norm": 2.3116815090179443,
      "learning_rate": 4.187875922908119e-05,
      "loss": 2.8152,
      "step": 2898
    },
    {
      "epoch": 0.2641698560233279,
      "grad_norm": 2.285905122756958,
      "learning_rate": 4.187347903485349e-05,
      "loss": 3.0358,
      "step": 2899
    },
    {
      "epoch": 0.26426098049936214,
      "grad_norm": 2.3877058029174805,
      "learning_rate": 4.186819745777772e-05,
      "loss": 3.1298,
      "step": 2900
    },
    {
      "epoch": 0.2643521049753964,
      "grad_norm": 2.243659019470215,
      "learning_rate": 4.1862914498286735e-05,
      "loss": 3.1799,
      "step": 2901
    },
    {
      "epoch": 0.2644432294514307,
      "grad_norm": 3.333629608154297,
      "learning_rate": 4.1857630156813486e-05,
      "loss": 2.9324,
      "step": 2902
    },
    {
      "epoch": 0.26453435392746494,
      "grad_norm": 2.4319705963134766,
      "learning_rate": 4.185234443379106e-05,
      "loss": 3.0472,
      "step": 2903
    },
    {
      "epoch": 0.2646254784034992,
      "grad_norm": 3.688075304031372,
      "learning_rate": 4.1847057329652616e-05,
      "loss": 3.1337,
      "step": 2904
    },
    {
      "epoch": 0.26471660287953347,
      "grad_norm": 2.4134609699249268,
      "learning_rate": 4.184176884483148e-05,
      "loss": 3.1465,
      "step": 2905
    },
    {
      "epoch": 0.2648077273555677,
      "grad_norm": 1.944526195526123,
      "learning_rate": 4.183647897976104e-05,
      "loss": 3.1009,
      "step": 2906
    },
    {
      "epoch": 0.26489885183160194,
      "grad_norm": 2.1194651126861572,
      "learning_rate": 4.183118773487484e-05,
      "loss": 3.1505,
      "step": 2907
    },
    {
      "epoch": 0.2649899763076362,
      "grad_norm": 1.7941718101501465,
      "learning_rate": 4.1825895110606506e-05,
      "loss": 2.9283,
      "step": 2908
    },
    {
      "epoch": 0.2650811007836705,
      "grad_norm": 2.7354705333709717,
      "learning_rate": 4.18206011073898e-05,
      "loss": 2.8447,
      "step": 2909
    },
    {
      "epoch": 0.26517222525970474,
      "grad_norm": 2.974231719970703,
      "learning_rate": 4.181530572565858e-05,
      "loss": 4.4824,
      "step": 2910
    },
    {
      "epoch": 0.265263349735739,
      "grad_norm": 1.413096308708191,
      "learning_rate": 4.1810008965846824e-05,
      "loss": 3.0008,
      "step": 2911
    },
    {
      "epoch": 0.2653544742117733,
      "grad_norm": 1.7608695030212402,
      "learning_rate": 4.1804710828388624e-05,
      "loss": 3.1591,
      "step": 2912
    },
    {
      "epoch": 0.26544559868780754,
      "grad_norm": 3.11283802986145,
      "learning_rate": 4.179941131371818e-05,
      "loss": 4.252,
      "step": 2913
    },
    {
      "epoch": 0.2655367231638418,
      "grad_norm": 2.6903934478759766,
      "learning_rate": 4.1794110422269825e-05,
      "loss": 2.93,
      "step": 2914
    },
    {
      "epoch": 0.26562784763987607,
      "grad_norm": 1.0775539875030518,
      "learning_rate": 4.178880815447796e-05,
      "loss": 2.8022,
      "step": 2915
    },
    {
      "epoch": 0.26571897211591033,
      "grad_norm": 1.9804686307907104,
      "learning_rate": 4.178350451077714e-05,
      "loss": 3.0698,
      "step": 2916
    },
    {
      "epoch": 0.2658100965919446,
      "grad_norm": 3.5438413619995117,
      "learning_rate": 4.177819949160202e-05,
      "loss": 2.9603,
      "step": 2917
    },
    {
      "epoch": 0.26590122106797887,
      "grad_norm": 2.804898500442505,
      "learning_rate": 4.177289309738737e-05,
      "loss": 3.1387,
      "step": 2918
    },
    {
      "epoch": 0.26599234554401313,
      "grad_norm": 2.3502540588378906,
      "learning_rate": 4.1767585328568073e-05,
      "loss": 2.9494,
      "step": 2919
    },
    {
      "epoch": 0.2660834700200474,
      "grad_norm": 1.4958268404006958,
      "learning_rate": 4.1762276185579104e-05,
      "loss": 2.8952,
      "step": 2920
    },
    {
      "epoch": 0.26617459449608166,
      "grad_norm": 1.3441364765167236,
      "learning_rate": 4.175696566885559e-05,
      "loss": 2.9578,
      "step": 2921
    },
    {
      "epoch": 0.26626571897211593,
      "grad_norm": 2.9649033546447754,
      "learning_rate": 4.175165377883272e-05,
      "loss": 4.2919,
      "step": 2922
    },
    {
      "epoch": 0.2663568434481502,
      "grad_norm": 2.2443289756774902,
      "learning_rate": 4.174634051594586e-05,
      "loss": 3.1502,
      "step": 2923
    },
    {
      "epoch": 0.26644796792418446,
      "grad_norm": 2.578035831451416,
      "learning_rate": 4.1741025880630435e-05,
      "loss": 2.9404,
      "step": 2924
    },
    {
      "epoch": 0.2665390924002187,
      "grad_norm": 1.770917296409607,
      "learning_rate": 4.1735709873321996e-05,
      "loss": 3.1208,
      "step": 2925
    },
    {
      "epoch": 0.26663021687625293,
      "grad_norm": 1.2455826997756958,
      "learning_rate": 4.1730392494456216e-05,
      "loss": 2.9474,
      "step": 2926
    },
    {
      "epoch": 0.2667213413522872,
      "grad_norm": 1.8772273063659668,
      "learning_rate": 4.1725073744468876e-05,
      "loss": 2.9388,
      "step": 2927
    },
    {
      "epoch": 0.26681246582832147,
      "grad_norm": 2.286301374435425,
      "learning_rate": 4.171975362379587e-05,
      "loss": 2.9037,
      "step": 2928
    },
    {
      "epoch": 0.26690359030435573,
      "grad_norm": 1.3723170757293701,
      "learning_rate": 4.1714432132873194e-05,
      "loss": 2.9344,
      "step": 2929
    },
    {
      "epoch": 0.26699471478039,
      "grad_norm": 2.1705482006073,
      "learning_rate": 4.170910927213697e-05,
      "loss": 3.0364,
      "step": 2930
    },
    {
      "epoch": 0.26708583925642426,
      "grad_norm": 1.8776051998138428,
      "learning_rate": 4.170378504202343e-05,
      "loss": 2.8574,
      "step": 2931
    },
    {
      "epoch": 0.26717696373245853,
      "grad_norm": 1.1812306642532349,
      "learning_rate": 4.169845944296892e-05,
      "loss": 2.9265,
      "step": 2932
    },
    {
      "epoch": 0.2672680882084928,
      "grad_norm": 2.226942777633667,
      "learning_rate": 4.169313247540988e-05,
      "loss": 2.6596,
      "step": 2933
    },
    {
      "epoch": 0.26735921268452706,
      "grad_norm": 1.819907307624817,
      "learning_rate": 4.168780413978289e-05,
      "loss": 2.8753,
      "step": 2934
    },
    {
      "epoch": 0.2674503371605613,
      "grad_norm": 2.7767977714538574,
      "learning_rate": 4.168247443652462e-05,
      "loss": 2.7379,
      "step": 2935
    },
    {
      "epoch": 0.2675414616365956,
      "grad_norm": 2.0983338356018066,
      "learning_rate": 4.167714336607186e-05,
      "loss": 2.7582,
      "step": 2936
    },
    {
      "epoch": 0.26763258611262986,
      "grad_norm": 1.998102068901062,
      "learning_rate": 4.167181092886152e-05,
      "loss": 3.0997,
      "step": 2937
    },
    {
      "epoch": 0.2677237105886641,
      "grad_norm": 1.392076849937439,
      "learning_rate": 4.16664771253306e-05,
      "loss": 2.7837,
      "step": 2938
    },
    {
      "epoch": 0.2678148350646984,
      "grad_norm": 2.2109456062316895,
      "learning_rate": 4.166114195591624e-05,
      "loss": 2.8533,
      "step": 2939
    },
    {
      "epoch": 0.26790595954073265,
      "grad_norm": 2.3383126258850098,
      "learning_rate": 4.165580542105567e-05,
      "loss": 2.979,
      "step": 2940
    },
    {
      "epoch": 0.2679970840167669,
      "grad_norm": 1.8859506845474243,
      "learning_rate": 4.165046752118625e-05,
      "loss": 2.811,
      "step": 2941
    },
    {
      "epoch": 0.2680882084928012,
      "grad_norm": 2.0716004371643066,
      "learning_rate": 4.164512825674543e-05,
      "loss": 2.9589,
      "step": 2942
    },
    {
      "epoch": 0.26817933296883545,
      "grad_norm": 3.4772074222564697,
      "learning_rate": 4.163978762817079e-05,
      "loss": 3.1844,
      "step": 2943
    },
    {
      "epoch": 0.2682704574448697,
      "grad_norm": 2.007626533508301,
      "learning_rate": 4.163444563590001e-05,
      "loss": 2.9307,
      "step": 2944
    },
    {
      "epoch": 0.268361581920904,
      "grad_norm": 2.6389293670654297,
      "learning_rate": 4.1629102280370904e-05,
      "loss": 2.9209,
      "step": 2945
    },
    {
      "epoch": 0.2684527063969382,
      "grad_norm": 1.9777435064315796,
      "learning_rate": 4.162375756202136e-05,
      "loss": 2.9719,
      "step": 2946
    },
    {
      "epoch": 0.26854383087297246,
      "grad_norm": 2.184433937072754,
      "learning_rate": 4.1618411481289404e-05,
      "loss": 2.9495,
      "step": 2947
    },
    {
      "epoch": 0.2686349553490067,
      "grad_norm": 1.8075201511383057,
      "learning_rate": 4.1613064038613174e-05,
      "loss": 2.9348,
      "step": 2948
    },
    {
      "epoch": 0.268726079825041,
      "grad_norm": 1.3251420259475708,
      "learning_rate": 4.1607715234430916e-05,
      "loss": 2.8124,
      "step": 2949
    },
    {
      "epoch": 0.26881720430107525,
      "grad_norm": 2.379141092300415,
      "learning_rate": 4.160236506918098e-05,
      "loss": 2.8,
      "step": 2950
    },
    {
      "epoch": 0.2689083287771095,
      "grad_norm": 2.3257620334625244,
      "learning_rate": 4.159701354330183e-05,
      "loss": 3.0946,
      "step": 2951
    },
    {
      "epoch": 0.2689994532531438,
      "grad_norm": 4.043161392211914,
      "learning_rate": 4.1591660657232055e-05,
      "loss": 2.7247,
      "step": 2952
    },
    {
      "epoch": 0.26909057772917805,
      "grad_norm": 2.545809030532837,
      "learning_rate": 4.1586306411410335e-05,
      "loss": 3.0837,
      "step": 2953
    },
    {
      "epoch": 0.2691817022052123,
      "grad_norm": 3.755012273788452,
      "learning_rate": 4.1580950806275474e-05,
      "loss": 2.2888,
      "step": 2954
    },
    {
      "epoch": 0.2692728266812466,
      "grad_norm": 2.286578416824341,
      "learning_rate": 4.157559384226639e-05,
      "loss": 3.1083,
      "step": 2955
    },
    {
      "epoch": 0.26936395115728085,
      "grad_norm": 2.1875030994415283,
      "learning_rate": 4.157023551982211e-05,
      "loss": 2.8354,
      "step": 2956
    },
    {
      "epoch": 0.2694550756333151,
      "grad_norm": 2.2270419597625732,
      "learning_rate": 4.156487583938176e-05,
      "loss": 2.787,
      "step": 2957
    },
    {
      "epoch": 0.2695462001093494,
      "grad_norm": 2.4909961223602295,
      "learning_rate": 4.155951480138458e-05,
      "loss": 2.9608,
      "step": 2958
    },
    {
      "epoch": 0.26963732458538364,
      "grad_norm": 2.71979022026062,
      "learning_rate": 4.1554152406269946e-05,
      "loss": 3.0165,
      "step": 2959
    },
    {
      "epoch": 0.2697284490614179,
      "grad_norm": 2.4888389110565186,
      "learning_rate": 4.1548788654477325e-05,
      "loss": 3.1186,
      "step": 2960
    },
    {
      "epoch": 0.2698195735374522,
      "grad_norm": 2.6805078983306885,
      "learning_rate": 4.154342354644628e-05,
      "loss": 2.9463,
      "step": 2961
    },
    {
      "epoch": 0.26991069801348644,
      "grad_norm": 2.269162178039551,
      "learning_rate": 4.153805708261653e-05,
      "loss": 3.0191,
      "step": 2962
    },
    {
      "epoch": 0.2700018224895207,
      "grad_norm": 2.733771562576294,
      "learning_rate": 4.1532689263427856e-05,
      "loss": 2.9263,
      "step": 2963
    },
    {
      "epoch": 0.27009294696555497,
      "grad_norm": 2.1901724338531494,
      "learning_rate": 4.152732008932018e-05,
      "loss": 3.0208,
      "step": 2964
    },
    {
      "epoch": 0.27018407144158924,
      "grad_norm": 1.9762758016586304,
      "learning_rate": 4.152194956073352e-05,
      "loss": 3.0961,
      "step": 2965
    },
    {
      "epoch": 0.2702751959176235,
      "grad_norm": 1.227144718170166,
      "learning_rate": 4.151657767810803e-05,
      "loss": 2.8602,
      "step": 2966
    },
    {
      "epoch": 0.2703663203936577,
      "grad_norm": 1.5557177066802979,
      "learning_rate": 4.151120444188394e-05,
      "loss": 2.9072,
      "step": 2967
    },
    {
      "epoch": 0.270457444869692,
      "grad_norm": 3.3849549293518066,
      "learning_rate": 4.150582985250161e-05,
      "loss": 3.0636,
      "step": 2968
    },
    {
      "epoch": 0.27054856934572624,
      "grad_norm": 2.751173973083496,
      "learning_rate": 4.150045391040151e-05,
      "loss": 2.9929,
      "step": 2969
    },
    {
      "epoch": 0.2706396938217605,
      "grad_norm": 2.15324068069458,
      "learning_rate": 4.149507661602423e-05,
      "loss": 3.0027,
      "step": 2970
    },
    {
      "epoch": 0.2707308182977948,
      "grad_norm": 3.102419137954712,
      "learning_rate": 4.148969796981044e-05,
      "loss": 2.8506,
      "step": 2971
    },
    {
      "epoch": 0.27082194277382904,
      "grad_norm": 2.6834075450897217,
      "learning_rate": 4.1484317972200956e-05,
      "loss": 2.7609,
      "step": 2972
    },
    {
      "epoch": 0.2709130672498633,
      "grad_norm": 2.1569125652313232,
      "learning_rate": 4.14789366236367e-05,
      "loss": 3.1541,
      "step": 2973
    },
    {
      "epoch": 0.27100419172589757,
      "grad_norm": 2.019385814666748,
      "learning_rate": 4.147355392455867e-05,
      "loss": 2.874,
      "step": 2974
    },
    {
      "epoch": 0.27109531620193184,
      "grad_norm": 2.1039791107177734,
      "learning_rate": 4.146816987540801e-05,
      "loss": 2.8463,
      "step": 2975
    },
    {
      "epoch": 0.2711864406779661,
      "grad_norm": 1.5508084297180176,
      "learning_rate": 4.146278447662597e-05,
      "loss": 2.9177,
      "step": 2976
    },
    {
      "epoch": 0.27127756515400037,
      "grad_norm": 1.7709184885025024,
      "learning_rate": 4.14573977286539e-05,
      "loss": 3.2116,
      "step": 2977
    },
    {
      "epoch": 0.27136868963003463,
      "grad_norm": 2.500171661376953,
      "learning_rate": 4.145200963193326e-05,
      "loss": 2.9528,
      "step": 2978
    },
    {
      "epoch": 0.2714598141060689,
      "grad_norm": 2.336371660232544,
      "learning_rate": 4.144662018690563e-05,
      "loss": 3.1582,
      "step": 2979
    },
    {
      "epoch": 0.27155093858210316,
      "grad_norm": 2.5994913578033447,
      "learning_rate": 4.14412293940127e-05,
      "loss": 3.9571,
      "step": 2980
    },
    {
      "epoch": 0.27164206305813743,
      "grad_norm": 3.1966841220855713,
      "learning_rate": 4.143583725369627e-05,
      "loss": 3.0287,
      "step": 2981
    },
    {
      "epoch": 0.2717331875341717,
      "grad_norm": 2.3707077503204346,
      "learning_rate": 4.143044376639823e-05,
      "loss": 4.2527,
      "step": 2982
    },
    {
      "epoch": 0.27182431201020596,
      "grad_norm": 1.8139830827713013,
      "learning_rate": 4.142504893256062e-05,
      "loss": 3.0854,
      "step": 2983
    },
    {
      "epoch": 0.2719154364862402,
      "grad_norm": 1.2827011346817017,
      "learning_rate": 4.141965275262555e-05,
      "loss": 2.9712,
      "step": 2984
    },
    {
      "epoch": 0.2720065609622745,
      "grad_norm": 2.2477238178253174,
      "learning_rate": 4.141425522703526e-05,
      "loss": 2.9919,
      "step": 2985
    },
    {
      "epoch": 0.27209768543830876,
      "grad_norm": 2.0824711322784424,
      "learning_rate": 4.140885635623211e-05,
      "loss": 3.0183,
      "step": 2986
    },
    {
      "epoch": 0.27218880991434297,
      "grad_norm": 2.366689682006836,
      "learning_rate": 4.1403456140658546e-05,
      "loss": 3.0467,
      "step": 2987
    },
    {
      "epoch": 0.27227993439037723,
      "grad_norm": 2.5079288482666016,
      "learning_rate": 4.139805458075715e-05,
      "loss": 2.9279,
      "step": 2988
    },
    {
      "epoch": 0.2723710588664115,
      "grad_norm": 1.2765588760375977,
      "learning_rate": 4.1392651676970585e-05,
      "loss": 2.8332,
      "step": 2989
    },
    {
      "epoch": 0.27246218334244576,
      "grad_norm": 2.022974967956543,
      "learning_rate": 4.138724742974165e-05,
      "loss": 2.8385,
      "step": 2990
    },
    {
      "epoch": 0.27255330781848003,
      "grad_norm": 1.1765350103378296,
      "learning_rate": 4.138184183951325e-05,
      "loss": 2.8514,
      "step": 2991
    },
    {
      "epoch": 0.2726444322945143,
      "grad_norm": 3.014350175857544,
      "learning_rate": 4.137643490672838e-05,
      "loss": 4.3709,
      "step": 2992
    },
    {
      "epoch": 0.27273555677054856,
      "grad_norm": 2.385418653488159,
      "learning_rate": 4.137102663183017e-05,
      "loss": 2.6381,
      "step": 2993
    },
    {
      "epoch": 0.2728266812465828,
      "grad_norm": 3.5804362297058105,
      "learning_rate": 4.136561701526185e-05,
      "loss": 2.8735,
      "step": 2994
    },
    {
      "epoch": 0.2729178057226171,
      "grad_norm": 3.390531063079834,
      "learning_rate": 4.1360206057466754e-05,
      "loss": 2.9447,
      "step": 2995
    },
    {
      "epoch": 0.27300893019865136,
      "grad_norm": 1.230154037475586,
      "learning_rate": 4.1354793758888334e-05,
      "loss": 2.8414,
      "step": 2996
    },
    {
      "epoch": 0.2731000546746856,
      "grad_norm": 2.92771315574646,
      "learning_rate": 4.134938011997014e-05,
      "loss": 2.993,
      "step": 2997
    },
    {
      "epoch": 0.2731911791507199,
      "grad_norm": 2.2258763313293457,
      "learning_rate": 4.134396514115586e-05,
      "loss": 3.1771,
      "step": 2998
    },
    {
      "epoch": 0.27328230362675415,
      "grad_norm": 2.0975964069366455,
      "learning_rate": 4.133854882288926e-05,
      "loss": 2.9581,
      "step": 2999
    },
    {
      "epoch": 0.2733734281027884,
      "grad_norm": 2.405278444290161,
      "learning_rate": 4.1333131165614224e-05,
      "loss": 3.0246,
      "step": 3000
    },
    {
      "epoch": 0.2734645525788227,
      "grad_norm": 3.3285224437713623,
      "learning_rate": 4.132771216977477e-05,
      "loss": 3.2024,
      "step": 3001
    },
    {
      "epoch": 0.27355567705485695,
      "grad_norm": 1.3723026514053345,
      "learning_rate": 4.132229183581498e-05,
      "loss": 2.8227,
      "step": 3002
    },
    {
      "epoch": 0.2736468015308912,
      "grad_norm": 3.028465509414673,
      "learning_rate": 4.13168701641791e-05,
      "loss": 3.2139,
      "step": 3003
    },
    {
      "epoch": 0.2737379260069255,
      "grad_norm": 2.8351643085479736,
      "learning_rate": 4.131144715531142e-05,
      "loss": 3.1651,
      "step": 3004
    },
    {
      "epoch": 0.27382905048295975,
      "grad_norm": 2.1554107666015625,
      "learning_rate": 4.130602280965642e-05,
      "loss": 3.0299,
      "step": 3005
    },
    {
      "epoch": 0.273920174958994,
      "grad_norm": 3.3620924949645996,
      "learning_rate": 4.130059712765862e-05,
      "loss": 4.1353,
      "step": 3006
    },
    {
      "epoch": 0.2740112994350282,
      "grad_norm": 3.0658280849456787,
      "learning_rate": 4.1295170109762685e-05,
      "loss": 3.0738,
      "step": 3007
    },
    {
      "epoch": 0.2741024239110625,
      "grad_norm": 2.090789556503296,
      "learning_rate": 4.128974175641337e-05,
      "loss": 3.0287,
      "step": 3008
    },
    {
      "epoch": 0.27419354838709675,
      "grad_norm": 2.25471568107605,
      "learning_rate": 4.128431206805557e-05,
      "loss": 3.2681,
      "step": 3009
    },
    {
      "epoch": 0.274284672863131,
      "grad_norm": 1.705100178718567,
      "learning_rate": 4.1278881045134235e-05,
      "loss": 2.7826,
      "step": 3010
    },
    {
      "epoch": 0.2743757973391653,
      "grad_norm": 2.0742764472961426,
      "learning_rate": 4.127344868809449e-05,
      "loss": 2.874,
      "step": 3011
    },
    {
      "epoch": 0.27446692181519955,
      "grad_norm": 1.6148096323013306,
      "learning_rate": 4.126801499738153e-05,
      "loss": 2.9403,
      "step": 3012
    },
    {
      "epoch": 0.2745580462912338,
      "grad_norm": 3.127495288848877,
      "learning_rate": 4.126257997344067e-05,
      "loss": 2.8545,
      "step": 3013
    },
    {
      "epoch": 0.2746491707672681,
      "grad_norm": 1.8852620124816895,
      "learning_rate": 4.125714361671732e-05,
      "loss": 2.9606,
      "step": 3014
    },
    {
      "epoch": 0.27474029524330235,
      "grad_norm": 1.6896710395812988,
      "learning_rate": 4.125170592765702e-05,
      "loss": 2.9172,
      "step": 3015
    },
    {
      "epoch": 0.2748314197193366,
      "grad_norm": 2.73445725440979,
      "learning_rate": 4.124626690670541e-05,
      "loss": 2.3503,
      "step": 3016
    },
    {
      "epoch": 0.2749225441953709,
      "grad_norm": 2.2814648151397705,
      "learning_rate": 4.124082655430824e-05,
      "loss": 3.0827,
      "step": 3017
    },
    {
      "epoch": 0.27501366867140514,
      "grad_norm": 2.6513009071350098,
      "learning_rate": 4.1235384870911366e-05,
      "loss": 2.8368,
      "step": 3018
    },
    {
      "epoch": 0.2751047931474394,
      "grad_norm": 1.2182458639144897,
      "learning_rate": 4.1229941856960745e-05,
      "loss": 2.9421,
      "step": 3019
    },
    {
      "epoch": 0.2751959176234737,
      "grad_norm": 2.8962607383728027,
      "learning_rate": 4.122449751290247e-05,
      "loss": 3.0273,
      "step": 3020
    },
    {
      "epoch": 0.27528704209950794,
      "grad_norm": 1.771925687789917,
      "learning_rate": 4.121905183918272e-05,
      "loss": 3.0695,
      "step": 3021
    },
    {
      "epoch": 0.2753781665755422,
      "grad_norm": 1.9327272176742554,
      "learning_rate": 4.121360483624779e-05,
      "loss": 3.1047,
      "step": 3022
    },
    {
      "epoch": 0.27546929105157647,
      "grad_norm": 1.114397406578064,
      "learning_rate": 4.120815650454408e-05,
      "loss": 2.8116,
      "step": 3023
    },
    {
      "epoch": 0.27556041552761074,
      "grad_norm": 3.4601972103118896,
      "learning_rate": 4.1202706844518114e-05,
      "loss": 2.7185,
      "step": 3024
    },
    {
      "epoch": 0.275651540003645,
      "grad_norm": 1.8018712997436523,
      "learning_rate": 4.11972558566165e-05,
      "loss": 3.1515,
      "step": 3025
    },
    {
      "epoch": 0.27574266447967927,
      "grad_norm": 1.3639010190963745,
      "learning_rate": 4.1191803541285965e-05,
      "loss": 2.8261,
      "step": 3026
    },
    {
      "epoch": 0.2758337889557135,
      "grad_norm": 1.382752537727356,
      "learning_rate": 4.118634989897336e-05,
      "loss": 2.9267,
      "step": 3027
    },
    {
      "epoch": 0.27592491343174774,
      "grad_norm": 1.1389694213867188,
      "learning_rate": 4.118089493012563e-05,
      "loss": 2.7134,
      "step": 3028
    },
    {
      "epoch": 0.276016037907782,
      "grad_norm": 1.4542425870895386,
      "learning_rate": 4.117543863518982e-05,
      "loss": 2.8894,
      "step": 3029
    },
    {
      "epoch": 0.2761071623838163,
      "grad_norm": 2.178499698638916,
      "learning_rate": 4.1169981014613116e-05,
      "loss": 3.0908,
      "step": 3030
    },
    {
      "epoch": 0.27619828685985054,
      "grad_norm": 1.3387235403060913,
      "learning_rate": 4.116452206884277e-05,
      "loss": 2.9164,
      "step": 3031
    },
    {
      "epoch": 0.2762894113358848,
      "grad_norm": 2.384028911590576,
      "learning_rate": 4.115906179832617e-05,
      "loss": 2.9827,
      "step": 3032
    },
    {
      "epoch": 0.27638053581191907,
      "grad_norm": 1.5141146183013916,
      "learning_rate": 4.115360020351081e-05,
      "loss": 2.9374,
      "step": 3033
    },
    {
      "epoch": 0.27647166028795334,
      "grad_norm": 2.4265248775482178,
      "learning_rate": 4.114813728484429e-05,
      "loss": 2.3161,
      "step": 3034
    },
    {
      "epoch": 0.2765627847639876,
      "grad_norm": 1.502545714378357,
      "learning_rate": 4.1142673042774325e-05,
      "loss": 2.9125,
      "step": 3035
    },
    {
      "epoch": 0.27665390924002187,
      "grad_norm": 2.1876635551452637,
      "learning_rate": 4.1137207477748716e-05,
      "loss": 2.9966,
      "step": 3036
    },
    {
      "epoch": 0.27674503371605613,
      "grad_norm": 2.8062307834625244,
      "learning_rate": 4.11317405902154e-05,
      "loss": 3.0079,
      "step": 3037
    },
    {
      "epoch": 0.2768361581920904,
      "grad_norm": 2.1036934852600098,
      "learning_rate": 4.112627238062239e-05,
      "loss": 2.8785,
      "step": 3038
    },
    {
      "epoch": 0.27692728266812466,
      "grad_norm": 1.6789398193359375,
      "learning_rate": 4.112080284941785e-05,
      "loss": 2.9496,
      "step": 3039
    },
    {
      "epoch": 0.27701840714415893,
      "grad_norm": 2.5206503868103027,
      "learning_rate": 4.1115331997050025e-05,
      "loss": 3.0704,
      "step": 3040
    },
    {
      "epoch": 0.2771095316201932,
      "grad_norm": 2.713686466217041,
      "learning_rate": 4.110985982396726e-05,
      "loss": 4.4716,
      "step": 3041
    },
    {
      "epoch": 0.27720065609622746,
      "grad_norm": 2.923307418823242,
      "learning_rate": 4.110438633061804e-05,
      "loss": 2.6297,
      "step": 3042
    },
    {
      "epoch": 0.2772917805722617,
      "grad_norm": 2.2153007984161377,
      "learning_rate": 4.109891151745092e-05,
      "loss": 2.825,
      "step": 3043
    },
    {
      "epoch": 0.277382905048296,
      "grad_norm": 2.2353713512420654,
      "learning_rate": 4.10934353849146e-05,
      "loss": 2.9551,
      "step": 3044
    },
    {
      "epoch": 0.27747402952433026,
      "grad_norm": 2.0905039310455322,
      "learning_rate": 4.108795793345785e-05,
      "loss": 2.8659,
      "step": 3045
    },
    {
      "epoch": 0.2775651540003645,
      "grad_norm": 2.1138522624969482,
      "learning_rate": 4.10824791635296e-05,
      "loss": 3.0356,
      "step": 3046
    },
    {
      "epoch": 0.2776562784763988,
      "grad_norm": 2.1203792095184326,
      "learning_rate": 4.107699907557882e-05,
      "loss": 2.7962,
      "step": 3047
    },
    {
      "epoch": 0.277747402952433,
      "grad_norm": 2.9755184650421143,
      "learning_rate": 4.107151767005466e-05,
      "loss": 4.0465,
      "step": 3048
    },
    {
      "epoch": 0.27783852742846726,
      "grad_norm": 1.3968240022659302,
      "learning_rate": 4.106603494740632e-05,
      "loss": 2.8263,
      "step": 3049
    },
    {
      "epoch": 0.27792965190450153,
      "grad_norm": 1.7882198095321655,
      "learning_rate": 4.1060550908083126e-05,
      "loss": 2.9292,
      "step": 3050
    },
    {
      "epoch": 0.2780207763805358,
      "grad_norm": 2.3797526359558105,
      "learning_rate": 4.105506555253453e-05,
      "loss": 2.8449,
      "step": 3051
    },
    {
      "epoch": 0.27811190085657006,
      "grad_norm": 2.242389678955078,
      "learning_rate": 4.1049578881210084e-05,
      "loss": 2.6795,
      "step": 3052
    },
    {
      "epoch": 0.2782030253326043,
      "grad_norm": 2.0701541900634766,
      "learning_rate": 4.1044090894559425e-05,
      "loss": 3.2265,
      "step": 3053
    },
    {
      "epoch": 0.2782941498086386,
      "grad_norm": 1.9562104940414429,
      "learning_rate": 4.103860159303233e-05,
      "loss": 3.1245,
      "step": 3054
    },
    {
      "epoch": 0.27838527428467286,
      "grad_norm": 1.5704617500305176,
      "learning_rate": 4.103311097707867e-05,
      "loss": 3.0319,
      "step": 3055
    },
    {
      "epoch": 0.2784763987607071,
      "grad_norm": 2.854062557220459,
      "learning_rate": 4.10276190471484e-05,
      "loss": 3.3314,
      "step": 3056
    },
    {
      "epoch": 0.2785675232367414,
      "grad_norm": 1.8975136280059814,
      "learning_rate": 4.102212580369163e-05,
      "loss": 3.1597,
      "step": 3057
    },
    {
      "epoch": 0.27865864771277565,
      "grad_norm": 1.4480185508728027,
      "learning_rate": 4.101663124715854e-05,
      "loss": 2.9346,
      "step": 3058
    },
    {
      "epoch": 0.2787497721888099,
      "grad_norm": 1.1331015825271606,
      "learning_rate": 4.101113537799943e-05,
      "loss": 2.9386,
      "step": 3059
    },
    {
      "epoch": 0.2788408966648442,
      "grad_norm": 1.3968732357025146,
      "learning_rate": 4.100563819666472e-05,
      "loss": 2.9618,
      "step": 3060
    },
    {
      "epoch": 0.27893202114087845,
      "grad_norm": 2.7948813438415527,
      "learning_rate": 4.100013970360492e-05,
      "loss": 3.0101,
      "step": 3061
    },
    {
      "epoch": 0.2790231456169127,
      "grad_norm": 1.5576274394989014,
      "learning_rate": 4.0994639899270644e-05,
      "loss": 2.9587,
      "step": 3062
    },
    {
      "epoch": 0.279114270092947,
      "grad_norm": 2.4363927841186523,
      "learning_rate": 4.098913878411264e-05,
      "loss": 2.7456,
      "step": 3063
    },
    {
      "epoch": 0.27920539456898125,
      "grad_norm": 2.3081634044647217,
      "learning_rate": 4.0983636358581725e-05,
      "loss": 2.7127,
      "step": 3064
    },
    {
      "epoch": 0.2792965190450155,
      "grad_norm": 2.942758321762085,
      "learning_rate": 4.097813262312885e-05,
      "loss": 3.0279,
      "step": 3065
    },
    {
      "epoch": 0.2793876435210498,
      "grad_norm": 1.4077153205871582,
      "learning_rate": 4.097262757820508e-05,
      "loss": 2.9768,
      "step": 3066
    },
    {
      "epoch": 0.27947876799708404,
      "grad_norm": 1.411299228668213,
      "learning_rate": 4.0967121224261575e-05,
      "loss": 2.8721,
      "step": 3067
    },
    {
      "epoch": 0.27956989247311825,
      "grad_norm": 1.1664859056472778,
      "learning_rate": 4.096161356174959e-05,
      "loss": 2.8674,
      "step": 3068
    },
    {
      "epoch": 0.2796610169491525,
      "grad_norm": 1.813663363456726,
      "learning_rate": 4.0956104591120506e-05,
      "loss": 2.627,
      "step": 3069
    },
    {
      "epoch": 0.2797521414251868,
      "grad_norm": 3.076895236968994,
      "learning_rate": 4.0950594312825806e-05,
      "loss": 3.0169,
      "step": 3070
    },
    {
      "epoch": 0.27984326590122105,
      "grad_norm": 1.3895238637924194,
      "learning_rate": 4.094508272731708e-05,
      "loss": 2.909,
      "step": 3071
    },
    {
      "epoch": 0.2799343903772553,
      "grad_norm": 2.488847255706787,
      "learning_rate": 4.093956983504601e-05,
      "loss": 3.037,
      "step": 3072
    },
    {
      "epoch": 0.2800255148532896,
      "grad_norm": 2.3130156993865967,
      "learning_rate": 4.0934055636464414e-05,
      "loss": 4.2506,
      "step": 3073
    },
    {
      "epoch": 0.28011663932932385,
      "grad_norm": 2.3649771213531494,
      "learning_rate": 4.09285401320242e-05,
      "loss": 2.7599,
      "step": 3074
    },
    {
      "epoch": 0.2802077638053581,
      "grad_norm": 3.0996756553649902,
      "learning_rate": 4.092302332217738e-05,
      "loss": 2.7088,
      "step": 3075
    },
    {
      "epoch": 0.2802988882813924,
      "grad_norm": 1.5356602668762207,
      "learning_rate": 4.09175052073761e-05,
      "loss": 3.0688,
      "step": 3076
    },
    {
      "epoch": 0.28039001275742664,
      "grad_norm": 2.7321038246154785,
      "learning_rate": 4.091198578807255e-05,
      "loss": 3.9926,
      "step": 3077
    },
    {
      "epoch": 0.2804811372334609,
      "grad_norm": 2.0339033603668213,
      "learning_rate": 4.09064650647191e-05,
      "loss": 3.0543,
      "step": 3078
    },
    {
      "epoch": 0.2805722617094952,
      "grad_norm": 2.6184940338134766,
      "learning_rate": 4.0900943037768194e-05,
      "loss": 2.9659,
      "step": 3079
    },
    {
      "epoch": 0.28066338618552944,
      "grad_norm": 2.1051831245422363,
      "learning_rate": 4.089541970767237e-05,
      "loss": 2.8876,
      "step": 3080
    },
    {
      "epoch": 0.2807545106615637,
      "grad_norm": 1.3357549905776978,
      "learning_rate": 4.08898950748843e-05,
      "loss": 2.9818,
      "step": 3081
    },
    {
      "epoch": 0.28084563513759797,
      "grad_norm": 3.977363348007202,
      "learning_rate": 4.088436913985674e-05,
      "loss": 2.8039,
      "step": 3082
    },
    {
      "epoch": 0.28093675961363224,
      "grad_norm": 2.108959197998047,
      "learning_rate": 4.087884190304255e-05,
      "loss": 2.9756,
      "step": 3083
    },
    {
      "epoch": 0.2810278840896665,
      "grad_norm": 2.8710925579071045,
      "learning_rate": 4.087331336489474e-05,
      "loss": 2.9915,
      "step": 3084
    },
    {
      "epoch": 0.28111900856570077,
      "grad_norm": 1.4994525909423828,
      "learning_rate": 4.0867783525866374e-05,
      "loss": 2.9115,
      "step": 3085
    },
    {
      "epoch": 0.28121013304173503,
      "grad_norm": 3.1304166316986084,
      "learning_rate": 4.0862252386410656e-05,
      "loss": 2.7033,
      "step": 3086
    },
    {
      "epoch": 0.2813012575177693,
      "grad_norm": 1.4027676582336426,
      "learning_rate": 4.085671994698087e-05,
      "loss": 2.8927,
      "step": 3087
    },
    {
      "epoch": 0.2813923819938035,
      "grad_norm": 1.5131675004959106,
      "learning_rate": 4.085118620803043e-05,
      "loss": 3.0274,
      "step": 3088
    },
    {
      "epoch": 0.2814835064698378,
      "grad_norm": 1.6290714740753174,
      "learning_rate": 4.0845651170012855e-05,
      "loss": 2.9214,
      "step": 3089
    },
    {
      "epoch": 0.28157463094587204,
      "grad_norm": 2.1281321048736572,
      "learning_rate": 4.084011483338175e-05,
      "loss": 2.95,
      "step": 3090
    },
    {
      "epoch": 0.2816657554219063,
      "grad_norm": 2.067457914352417,
      "learning_rate": 4.083457719859084e-05,
      "loss": 3.1958,
      "step": 3091
    },
    {
      "epoch": 0.28175687989794057,
      "grad_norm": 1.802259087562561,
      "learning_rate": 4.082903826609396e-05,
      "loss": 2.7802,
      "step": 3092
    },
    {
      "epoch": 0.28184800437397484,
      "grad_norm": 3.961319923400879,
      "learning_rate": 4.082349803634505e-05,
      "loss": 3.1137,
      "step": 3093
    },
    {
      "epoch": 0.2819391288500091,
      "grad_norm": 1.3029671907424927,
      "learning_rate": 4.081795650979816e-05,
      "loss": 2.8012,
      "step": 3094
    },
    {
      "epoch": 0.28203025332604337,
      "grad_norm": 2.979952812194824,
      "learning_rate": 4.081241368690742e-05,
      "loss": 3.0304,
      "step": 3095
    },
    {
      "epoch": 0.28212137780207763,
      "grad_norm": 2.4823622703552246,
      "learning_rate": 4.08068695681271e-05,
      "loss": 3.1061,
      "step": 3096
    },
    {
      "epoch": 0.2822125022781119,
      "grad_norm": 2.0187978744506836,
      "learning_rate": 4.080132415391156e-05,
      "loss": 2.7383,
      "step": 3097
    },
    {
      "epoch": 0.28230362675414616,
      "grad_norm": 1.5783172845840454,
      "learning_rate": 4.079577744471527e-05,
      "loss": 3.038,
      "step": 3098
    },
    {
      "epoch": 0.28239475123018043,
      "grad_norm": 2.1749415397644043,
      "learning_rate": 4.0790229440992804e-05,
      "loss": 2.873,
      "step": 3099
    },
    {
      "epoch": 0.2824858757062147,
      "grad_norm": 2.54717755317688,
      "learning_rate": 4.078468014319884e-05,
      "loss": 4.2937,
      "step": 3100
    },
    {
      "epoch": 0.28257700018224896,
      "grad_norm": 1.896461009979248,
      "learning_rate": 4.077912955178817e-05,
      "loss": 2.9222,
      "step": 3101
    },
    {
      "epoch": 0.2826681246582832,
      "grad_norm": 1.5036392211914062,
      "learning_rate": 4.077357766721568e-05,
      "loss": 2.859,
      "step": 3102
    },
    {
      "epoch": 0.2827592491343175,
      "grad_norm": 2.965679883956909,
      "learning_rate": 4.076802448993637e-05,
      "loss": 2.92,
      "step": 3103
    },
    {
      "epoch": 0.28285037361035176,
      "grad_norm": 2.196608304977417,
      "learning_rate": 4.0762470020405355e-05,
      "loss": 2.9066,
      "step": 3104
    },
    {
      "epoch": 0.282941498086386,
      "grad_norm": 3.5181519985198975,
      "learning_rate": 4.075691425907784e-05,
      "loss": 2.9364,
      "step": 3105
    },
    {
      "epoch": 0.2830326225624203,
      "grad_norm": 2.003502130508423,
      "learning_rate": 4.075135720640913e-05,
      "loss": 2.7792,
      "step": 3106
    },
    {
      "epoch": 0.28312374703845455,
      "grad_norm": 2.005478620529175,
      "learning_rate": 4.0745798862854655e-05,
      "loss": 2.908,
      "step": 3107
    },
    {
      "epoch": 0.28321487151448876,
      "grad_norm": 1.5894314050674438,
      "learning_rate": 4.0740239228869956e-05,
      "loss": 3.029,
      "step": 3108
    },
    {
      "epoch": 0.28330599599052303,
      "grad_norm": 2.1124114990234375,
      "learning_rate": 4.073467830491065e-05,
      "loss": 2.6733,
      "step": 3109
    },
    {
      "epoch": 0.2833971204665573,
      "grad_norm": 2.159187078475952,
      "learning_rate": 4.0729116091432483e-05,
      "loss": 2.3038,
      "step": 3110
    },
    {
      "epoch": 0.28348824494259156,
      "grad_norm": 2.5558083057403564,
      "learning_rate": 4.0723552588891294e-05,
      "loss": 3.2783,
      "step": 3111
    },
    {
      "epoch": 0.2835793694186258,
      "grad_norm": 3.4408469200134277,
      "learning_rate": 4.0717987797743046e-05,
      "loss": 3.1452,
      "step": 3112
    },
    {
      "epoch": 0.2836704938946601,
      "grad_norm": 1.1896499395370483,
      "learning_rate": 4.071242171844379e-05,
      "loss": 2.8594,
      "step": 3113
    },
    {
      "epoch": 0.28376161837069436,
      "grad_norm": 1.5479761362075806,
      "learning_rate": 4.070685435144968e-05,
      "loss": 3.0272,
      "step": 3114
    },
    {
      "epoch": 0.2838527428467286,
      "grad_norm": 1.250723958015442,
      "learning_rate": 4.0701285697217e-05,
      "loss": 2.8531,
      "step": 3115
    },
    {
      "epoch": 0.2839438673227629,
      "grad_norm": 2.473517656326294,
      "learning_rate": 4.06957157562021e-05,
      "loss": 2.7666,
      "step": 3116
    },
    {
      "epoch": 0.28403499179879715,
      "grad_norm": 2.088315486907959,
      "learning_rate": 4.0690144528861486e-05,
      "loss": 3.0013,
      "step": 3117
    },
    {
      "epoch": 0.2841261162748314,
      "grad_norm": 3.127229928970337,
      "learning_rate": 4.068457201565172e-05,
      "loss": 3.256,
      "step": 3118
    },
    {
      "epoch": 0.2842172407508657,
      "grad_norm": 1.7258127927780151,
      "learning_rate": 4.067899821702951e-05,
      "loss": 3.0192,
      "step": 3119
    },
    {
      "epoch": 0.28430836522689995,
      "grad_norm": 2.2799999713897705,
      "learning_rate": 4.067342313345164e-05,
      "loss": 3.0717,
      "step": 3120
    },
    {
      "epoch": 0.2843994897029342,
      "grad_norm": 2.2524468898773193,
      "learning_rate": 4.0667846765375e-05,
      "loss": 2.7437,
      "step": 3121
    },
    {
      "epoch": 0.2844906141789685,
      "grad_norm": 1.848656177520752,
      "learning_rate": 4.066226911325661e-05,
      "loss": 2.5744,
      "step": 3122
    },
    {
      "epoch": 0.28458173865500275,
      "grad_norm": 1.5403884649276733,
      "learning_rate": 4.065669017755358e-05,
      "loss": 2.9401,
      "step": 3123
    },
    {
      "epoch": 0.284672863131037,
      "grad_norm": 2.7212581634521484,
      "learning_rate": 4.0651109958723124e-05,
      "loss": 3.0483,
      "step": 3124
    },
    {
      "epoch": 0.2847639876070713,
      "grad_norm": 1.3790080547332764,
      "learning_rate": 4.0645528457222556e-05,
      "loss": 2.8756,
      "step": 3125
    },
    {
      "epoch": 0.28485511208310554,
      "grad_norm": 1.8684055805206299,
      "learning_rate": 4.063994567350932e-05,
      "loss": 3.122,
      "step": 3126
    },
    {
      "epoch": 0.2849462365591398,
      "grad_norm": 1.200626015663147,
      "learning_rate": 4.063436160804092e-05,
      "loss": 2.8193,
      "step": 3127
    },
    {
      "epoch": 0.2850373610351741,
      "grad_norm": 2.565258264541626,
      "learning_rate": 4.062877626127501e-05,
      "loss": 2.9939,
      "step": 3128
    },
    {
      "epoch": 0.2851284855112083,
      "grad_norm": 2.7247445583343506,
      "learning_rate": 4.0623189633669337e-05,
      "loss": 3.106,
      "step": 3129
    },
    {
      "epoch": 0.28521960998724255,
      "grad_norm": 2.6962335109710693,
      "learning_rate": 4.0617601725681734e-05,
      "loss": 2.8885,
      "step": 3130
    },
    {
      "epoch": 0.2853107344632768,
      "grad_norm": 2.9033639430999756,
      "learning_rate": 4.061201253777015e-05,
      "loss": 3.0203,
      "step": 3131
    },
    {
      "epoch": 0.2854018589393111,
      "grad_norm": 1.6251353025436401,
      "learning_rate": 4.060642207039265e-05,
      "loss": 2.8588,
      "step": 3132
    },
    {
      "epoch": 0.28549298341534535,
      "grad_norm": 1.522720456123352,
      "learning_rate": 4.06008303240074e-05,
      "loss": 3.0277,
      "step": 3133
    },
    {
      "epoch": 0.2855841078913796,
      "grad_norm": 2.004148244857788,
      "learning_rate": 4.0595237299072645e-05,
      "loss": 2.6331,
      "step": 3134
    },
    {
      "epoch": 0.2856752323674139,
      "grad_norm": 1.2495026588439941,
      "learning_rate": 4.0589642996046775e-05,
      "loss": 2.807,
      "step": 3135
    },
    {
      "epoch": 0.28576635684344814,
      "grad_norm": 1.2666634321212769,
      "learning_rate": 4.0584047415388256e-05,
      "loss": 2.9529,
      "step": 3136
    },
    {
      "epoch": 0.2858574813194824,
      "grad_norm": 2.379206657409668,
      "learning_rate": 4.057845055755567e-05,
      "loss": 2.9462,
      "step": 3137
    },
    {
      "epoch": 0.2859486057955167,
      "grad_norm": 2.012214422225952,
      "learning_rate": 4.05728524230077e-05,
      "loss": 3.09,
      "step": 3138
    },
    {
      "epoch": 0.28603973027155094,
      "grad_norm": 1.9616063833236694,
      "learning_rate": 4.056725301220314e-05,
      "loss": 3.3086,
      "step": 3139
    },
    {
      "epoch": 0.2861308547475852,
      "grad_norm": 1.4520832300186157,
      "learning_rate": 4.056165232560087e-05,
      "loss": 2.9785,
      "step": 3140
    },
    {
      "epoch": 0.28622197922361947,
      "grad_norm": 2.816049337387085,
      "learning_rate": 4.0556050363659914e-05,
      "loss": 2.9001,
      "step": 3141
    },
    {
      "epoch": 0.28631310369965374,
      "grad_norm": 1.0482550859451294,
      "learning_rate": 4.0550447126839354e-05,
      "loss": 2.8402,
      "step": 3142
    },
    {
      "epoch": 0.286404228175688,
      "grad_norm": 1.2596871852874756,
      "learning_rate": 4.05448426155984e-05,
      "loss": 2.8468,
      "step": 3143
    },
    {
      "epoch": 0.28649535265172227,
      "grad_norm": 1.4817097187042236,
      "learning_rate": 4.053923683039637e-05,
      "loss": 2.8616,
      "step": 3144
    },
    {
      "epoch": 0.28658647712775653,
      "grad_norm": 2.240272045135498,
      "learning_rate": 4.053362977169268e-05,
      "loss": 3.0863,
      "step": 3145
    },
    {
      "epoch": 0.2866776016037908,
      "grad_norm": 1.858825922012329,
      "learning_rate": 4.052802143994684e-05,
      "loss": 2.5702,
      "step": 3146
    },
    {
      "epoch": 0.28676872607982506,
      "grad_norm": 2.58378005027771,
      "learning_rate": 4.052241183561849e-05,
      "loss": 2.9325,
      "step": 3147
    },
    {
      "epoch": 0.28685985055585933,
      "grad_norm": 2.3868408203125,
      "learning_rate": 4.051680095916735e-05,
      "loss": 3.089,
      "step": 3148
    },
    {
      "epoch": 0.28695097503189354,
      "grad_norm": 2.238407850265503,
      "learning_rate": 4.051118881105325e-05,
      "loss": 2.9419,
      "step": 3149
    },
    {
      "epoch": 0.2870420995079278,
      "grad_norm": 2.0565924644470215,
      "learning_rate": 4.050557539173614e-05,
      "loss": 2.9607,
      "step": 3150
    },
    {
      "epoch": 0.28713322398396207,
      "grad_norm": 2.3215625286102295,
      "learning_rate": 4.049996070167604e-05,
      "loss": 3.1243,
      "step": 3151
    },
    {
      "epoch": 0.28722434845999634,
      "grad_norm": 2.0705113410949707,
      "learning_rate": 4.049434474133313e-05,
      "loss": 3.0493,
      "step": 3152
    },
    {
      "epoch": 0.2873154729360306,
      "grad_norm": 1.2343486547470093,
      "learning_rate": 4.048872751116763e-05,
      "loss": 2.7698,
      "step": 3153
    },
    {
      "epoch": 0.28740659741206487,
      "grad_norm": 1.4453444480895996,
      "learning_rate": 4.04831090116399e-05,
      "loss": 2.9355,
      "step": 3154
    },
    {
      "epoch": 0.28749772188809913,
      "grad_norm": 2.309598207473755,
      "learning_rate": 4.047748924321041e-05,
      "loss": 2.8676,
      "step": 3155
    },
    {
      "epoch": 0.2875888463641334,
      "grad_norm": 1.4498906135559082,
      "learning_rate": 4.047186820633971e-05,
      "loss": 2.8025,
      "step": 3156
    },
    {
      "epoch": 0.28767997084016766,
      "grad_norm": 3.5558416843414307,
      "learning_rate": 4.046624590148848e-05,
      "loss": 2.4051,
      "step": 3157
    },
    {
      "epoch": 0.28777109531620193,
      "grad_norm": 1.8619962930679321,
      "learning_rate": 4.046062232911747e-05,
      "loss": 3.0246,
      "step": 3158
    },
    {
      "epoch": 0.2878622197922362,
      "grad_norm": 2.1450588703155518,
      "learning_rate": 4.045499748968757e-05,
      "loss": 3.1598,
      "step": 3159
    },
    {
      "epoch": 0.28795334426827046,
      "grad_norm": 2.6680386066436768,
      "learning_rate": 4.044937138365976e-05,
      "loss": 3.1214,
      "step": 3160
    },
    {
      "epoch": 0.2880444687443047,
      "grad_norm": 1.9604525566101074,
      "learning_rate": 4.04437440114951e-05,
      "loss": 2.8118,
      "step": 3161
    },
    {
      "epoch": 0.288135593220339,
      "grad_norm": 1.8768948316574097,
      "learning_rate": 4.04381153736548e-05,
      "loss": 3.1507,
      "step": 3162
    },
    {
      "epoch": 0.28822671769637326,
      "grad_norm": 2.356567144393921,
      "learning_rate": 4.043248547060014e-05,
      "loss": 3.099,
      "step": 3163
    },
    {
      "epoch": 0.2883178421724075,
      "grad_norm": 1.449814796447754,
      "learning_rate": 4.0426854302792505e-05,
      "loss": 2.9297,
      "step": 3164
    },
    {
      "epoch": 0.2884089666484418,
      "grad_norm": 2.388646125793457,
      "learning_rate": 4.042122187069339e-05,
      "loss": 2.943,
      "step": 3165
    },
    {
      "epoch": 0.28850009112447605,
      "grad_norm": 2.2703914642333984,
      "learning_rate": 4.041558817476442e-05,
      "loss": 3.0674,
      "step": 3166
    },
    {
      "epoch": 0.2885912156005103,
      "grad_norm": 1.4150704145431519,
      "learning_rate": 4.040995321546727e-05,
      "loss": 2.9532,
      "step": 3167
    },
    {
      "epoch": 0.2886823400765446,
      "grad_norm": 2.1361725330352783,
      "learning_rate": 4.0404316993263766e-05,
      "loss": 3.0051,
      "step": 3168
    },
    {
      "epoch": 0.2887734645525788,
      "grad_norm": 1.8056014776229858,
      "learning_rate": 4.039867950861581e-05,
      "loss": 2.9793,
      "step": 3169
    },
    {
      "epoch": 0.28886458902861306,
      "grad_norm": 3.0440027713775635,
      "learning_rate": 4.039304076198541e-05,
      "loss": 3.2264,
      "step": 3170
    },
    {
      "epoch": 0.2889557135046473,
      "grad_norm": 2.0881311893463135,
      "learning_rate": 4.03874007538347e-05,
      "loss": 3.2213,
      "step": 3171
    },
    {
      "epoch": 0.2890468379806816,
      "grad_norm": 1.2232310771942139,
      "learning_rate": 4.0381759484625884e-05,
      "loss": 2.8005,
      "step": 3172
    },
    {
      "epoch": 0.28913796245671586,
      "grad_norm": 2.051590919494629,
      "learning_rate": 4.0376116954821297e-05,
      "loss": 2.5229,
      "step": 3173
    },
    {
      "epoch": 0.2892290869327501,
      "grad_norm": 2.102226495742798,
      "learning_rate": 4.037047316488336e-05,
      "loss": 3.0042,
      "step": 3174
    },
    {
      "epoch": 0.2893202114087844,
      "grad_norm": 2.191962242126465,
      "learning_rate": 4.036482811527461e-05,
      "loss": 2.8772,
      "step": 3175
    },
    {
      "epoch": 0.28941133588481865,
      "grad_norm": 2.0225160121917725,
      "learning_rate": 4.0359181806457677e-05,
      "loss": 3.2194,
      "step": 3176
    },
    {
      "epoch": 0.2895024603608529,
      "grad_norm": 2.2885079383850098,
      "learning_rate": 4.0353534238895294e-05,
      "loss": 2.7606,
      "step": 3177
    },
    {
      "epoch": 0.2895935848368872,
      "grad_norm": 3.0076608657836914,
      "learning_rate": 4.034788541305031e-05,
      "loss": 3.1868,
      "step": 3178
    },
    {
      "epoch": 0.28968470931292145,
      "grad_norm": 2.0696005821228027,
      "learning_rate": 4.034223532938567e-05,
      "loss": 3.1871,
      "step": 3179
    },
    {
      "epoch": 0.2897758337889557,
      "grad_norm": 2.3871467113494873,
      "learning_rate": 4.0336583988364416e-05,
      "loss": 2.9273,
      "step": 3180
    },
    {
      "epoch": 0.28986695826499,
      "grad_norm": 1.8930153846740723,
      "learning_rate": 4.03309313904497e-05,
      "loss": 2.9644,
      "step": 3181
    },
    {
      "epoch": 0.28995808274102425,
      "grad_norm": 2.1080682277679443,
      "learning_rate": 4.0325277536104766e-05,
      "loss": 2.8488,
      "step": 3182
    },
    {
      "epoch": 0.2900492072170585,
      "grad_norm": 3.400601863861084,
      "learning_rate": 4.031962242579297e-05,
      "loss": 2.3319,
      "step": 3183
    },
    {
      "epoch": 0.2901403316930928,
      "grad_norm": 1.519457221031189,
      "learning_rate": 4.0313966059977794e-05,
      "loss": 2.9065,
      "step": 3184
    },
    {
      "epoch": 0.29023145616912704,
      "grad_norm": 2.3127503395080566,
      "learning_rate": 4.030830843912277e-05,
      "loss": 2.5845,
      "step": 3185
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 1.2162214517593384,
      "learning_rate": 4.030264956369157e-05,
      "loss": 2.858,
      "step": 3186
    },
    {
      "epoch": 0.2904137051211956,
      "grad_norm": 1.4161421060562134,
      "learning_rate": 4.029698943414798e-05,
      "loss": 2.9668,
      "step": 3187
    },
    {
      "epoch": 0.29050482959722984,
      "grad_norm": 3.2886459827423096,
      "learning_rate": 4.029132805095584e-05,
      "loss": 2.98,
      "step": 3188
    },
    {
      "epoch": 0.29059595407326405,
      "grad_norm": 1.3265904188156128,
      "learning_rate": 4.028566541457915e-05,
      "loss": 2.9382,
      "step": 3189
    },
    {
      "epoch": 0.2906870785492983,
      "grad_norm": 2.6687467098236084,
      "learning_rate": 4.0280001525481964e-05,
      "loss": 2.8594,
      "step": 3190
    },
    {
      "epoch": 0.2907782030253326,
      "grad_norm": 1.4941143989562988,
      "learning_rate": 4.027433638412847e-05,
      "loss": 2.9134,
      "step": 3191
    },
    {
      "epoch": 0.29086932750136685,
      "grad_norm": 2.8184497356414795,
      "learning_rate": 4.0268669990982956e-05,
      "loss": 2.9753,
      "step": 3192
    },
    {
      "epoch": 0.2909604519774011,
      "grad_norm": 3.0961005687713623,
      "learning_rate": 4.026300234650979e-05,
      "loss": 3.1302,
      "step": 3193
    },
    {
      "epoch": 0.2910515764534354,
      "grad_norm": 4.637017250061035,
      "learning_rate": 4.025733345117347e-05,
      "loss": 3.2605,
      "step": 3194
    },
    {
      "epoch": 0.29114270092946964,
      "grad_norm": 1.2614662647247314,
      "learning_rate": 4.025166330543857e-05,
      "loss": 2.8781,
      "step": 3195
    },
    {
      "epoch": 0.2912338254055039,
      "grad_norm": 2.0450456142425537,
      "learning_rate": 4.02459919097698e-05,
      "loss": 2.7941,
      "step": 3196
    },
    {
      "epoch": 0.2913249498815382,
      "grad_norm": 2.877882957458496,
      "learning_rate": 4.024031926463194e-05,
      "loss": 3.4894,
      "step": 3197
    },
    {
      "epoch": 0.29141607435757244,
      "grad_norm": 2.055412769317627,
      "learning_rate": 4.0234645370489886e-05,
      "loss": 3.0161,
      "step": 3198
    },
    {
      "epoch": 0.2915071988336067,
      "grad_norm": 1.9826300144195557,
      "learning_rate": 4.022897022780864e-05,
      "loss": 2.9795,
      "step": 3199
    },
    {
      "epoch": 0.29159832330964097,
      "grad_norm": 2.228168249130249,
      "learning_rate": 4.02232938370533e-05,
      "loss": 2.8907,
      "step": 3200
    },
    {
      "epoch": 0.29168944778567524,
      "grad_norm": 2.060887098312378,
      "learning_rate": 4.021761619868907e-05,
      "loss": 2.8398,
      "step": 3201
    },
    {
      "epoch": 0.2917805722617095,
      "grad_norm": 1.7748042345046997,
      "learning_rate": 4.021193731318126e-05,
      "loss": 2.8734,
      "step": 3202
    },
    {
      "epoch": 0.29187169673774377,
      "grad_norm": 1.488072156906128,
      "learning_rate": 4.0206257180995265e-05,
      "loss": 2.8624,
      "step": 3203
    },
    {
      "epoch": 0.29196282121377803,
      "grad_norm": 2.653682231903076,
      "learning_rate": 4.020057580259661e-05,
      "loss": 3.2964,
      "step": 3204
    },
    {
      "epoch": 0.2920539456898123,
      "grad_norm": 2.2000529766082764,
      "learning_rate": 4.0194893178450887e-05,
      "loss": 2.9225,
      "step": 3205
    },
    {
      "epoch": 0.29214507016584657,
      "grad_norm": 1.4411041736602783,
      "learning_rate": 4.0189209309023826e-05,
      "loss": 2.9038,
      "step": 3206
    },
    {
      "epoch": 0.29223619464188083,
      "grad_norm": 2.053433656692505,
      "learning_rate": 4.018352419478123e-05,
      "loss": 3.0627,
      "step": 3207
    },
    {
      "epoch": 0.2923273191179151,
      "grad_norm": 1.3217010498046875,
      "learning_rate": 4.017783783618902e-05,
      "loss": 2.8452,
      "step": 3208
    },
    {
      "epoch": 0.29241844359394936,
      "grad_norm": 1.2600178718566895,
      "learning_rate": 4.017215023371323e-05,
      "loss": 2.8611,
      "step": 3209
    },
    {
      "epoch": 0.29250956806998357,
      "grad_norm": 1.9216219186782837,
      "learning_rate": 4.0166461387819967e-05,
      "loss": 2.9396,
      "step": 3210
    },
    {
      "epoch": 0.29260069254601784,
      "grad_norm": 1.288878321647644,
      "learning_rate": 4.016077129897545e-05,
      "loss": 2.8242,
      "step": 3211
    },
    {
      "epoch": 0.2926918170220521,
      "grad_norm": 2.8166205883026123,
      "learning_rate": 4.0155079967646014e-05,
      "loss": 2.9887,
      "step": 3212
    },
    {
      "epoch": 0.29278294149808637,
      "grad_norm": 1.7326747179031372,
      "learning_rate": 4.0149387394298075e-05,
      "loss": 2.7878,
      "step": 3213
    },
    {
      "epoch": 0.29287406597412063,
      "grad_norm": 1.28786301612854,
      "learning_rate": 4.014369357939818e-05,
      "loss": 2.8091,
      "step": 3214
    },
    {
      "epoch": 0.2929651904501549,
      "grad_norm": 2.2235400676727295,
      "learning_rate": 4.0137998523412946e-05,
      "loss": 3.0029,
      "step": 3215
    },
    {
      "epoch": 0.29305631492618917,
      "grad_norm": 2.5055344104766846,
      "learning_rate": 4.013230222680911e-05,
      "loss": 2.9037,
      "step": 3216
    },
    {
      "epoch": 0.29314743940222343,
      "grad_norm": 2.1128478050231934,
      "learning_rate": 4.0126604690053504e-05,
      "loss": 2.6346,
      "step": 3217
    },
    {
      "epoch": 0.2932385638782577,
      "grad_norm": 2.6443307399749756,
      "learning_rate": 4.012090591361307e-05,
      "loss": 3.1072,
      "step": 3218
    },
    {
      "epoch": 0.29332968835429196,
      "grad_norm": 2.7865054607391357,
      "learning_rate": 4.0115205897954825e-05,
      "loss": 2.309,
      "step": 3219
    },
    {
      "epoch": 0.2934208128303262,
      "grad_norm": 2.879323720932007,
      "learning_rate": 4.010950464354593e-05,
      "loss": 2.7029,
      "step": 3220
    },
    {
      "epoch": 0.2935119373063605,
      "grad_norm": 1.2220463752746582,
      "learning_rate": 4.010380215085362e-05,
      "loss": 2.8177,
      "step": 3221
    },
    {
      "epoch": 0.29360306178239476,
      "grad_norm": 2.6750919818878174,
      "learning_rate": 4.009809842034523e-05,
      "loss": 3.9967,
      "step": 3222
    },
    {
      "epoch": 0.293694186258429,
      "grad_norm": 2.4236345291137695,
      "learning_rate": 4.009239345248821e-05,
      "loss": 2.9009,
      "step": 3223
    },
    {
      "epoch": 0.2937853107344633,
      "grad_norm": 2.1761162281036377,
      "learning_rate": 4.00866872477501e-05,
      "loss": 2.7543,
      "step": 3224
    },
    {
      "epoch": 0.29387643521049756,
      "grad_norm": 2.463320016860962,
      "learning_rate": 4.008097980659854e-05,
      "loss": 2.8841,
      "step": 3225
    },
    {
      "epoch": 0.2939675596865318,
      "grad_norm": 1.114108681678772,
      "learning_rate": 4.00752711295013e-05,
      "loss": 2.8808,
      "step": 3226
    },
    {
      "epoch": 0.2940586841625661,
      "grad_norm": 1.5252712965011597,
      "learning_rate": 4.006956121692621e-05,
      "loss": 3.0401,
      "step": 3227
    },
    {
      "epoch": 0.29414980863860035,
      "grad_norm": 1.914833426475525,
      "learning_rate": 4.006385006934121e-05,
      "loss": 3.2073,
      "step": 3228
    },
    {
      "epoch": 0.2942409331146346,
      "grad_norm": 1.9320087432861328,
      "learning_rate": 4.005813768721438e-05,
      "loss": 2.9819,
      "step": 3229
    },
    {
      "epoch": 0.2943320575906688,
      "grad_norm": 2.109797954559326,
      "learning_rate": 4.0052424071013855e-05,
      "loss": 3.0249,
      "step": 3230
    },
    {
      "epoch": 0.2944231820667031,
      "grad_norm": 3.8972291946411133,
      "learning_rate": 4.004670922120789e-05,
      "loss": 4.4035,
      "step": 3231
    },
    {
      "epoch": 0.29451430654273736,
      "grad_norm": 2.512706995010376,
      "learning_rate": 4.004099313826483e-05,
      "loss": 2.956,
      "step": 3232
    },
    {
      "epoch": 0.2946054310187716,
      "grad_norm": 2.3679397106170654,
      "learning_rate": 4.003527582265315e-05,
      "loss": 2.8603,
      "step": 3233
    },
    {
      "epoch": 0.2946965554948059,
      "grad_norm": 2.6128008365631104,
      "learning_rate": 4.002955727484139e-05,
      "loss": 2.7529,
      "step": 3234
    },
    {
      "epoch": 0.29478767997084016,
      "grad_norm": 1.8564624786376953,
      "learning_rate": 4.0023837495298215e-05,
      "loss": 3.0689,
      "step": 3235
    },
    {
      "epoch": 0.2948788044468744,
      "grad_norm": 1.4559752941131592,
      "learning_rate": 4.001811648449238e-05,
      "loss": 2.9659,
      "step": 3236
    },
    {
      "epoch": 0.2949699289229087,
      "grad_norm": 2.5086910724639893,
      "learning_rate": 4.0012394242892756e-05,
      "loss": 3.0598,
      "step": 3237
    },
    {
      "epoch": 0.29506105339894295,
      "grad_norm": 1.2314717769622803,
      "learning_rate": 4.0006670770968286e-05,
      "loss": 2.9922,
      "step": 3238
    },
    {
      "epoch": 0.2951521778749772,
      "grad_norm": 2.0781307220458984,
      "learning_rate": 4.000094606918805e-05,
      "loss": 3.1752,
      "step": 3239
    },
    {
      "epoch": 0.2952433023510115,
      "grad_norm": 2.5030677318573,
      "learning_rate": 3.999522013802118e-05,
      "loss": 3.1133,
      "step": 3240
    },
    {
      "epoch": 0.29533442682704575,
      "grad_norm": 1.9572101831436157,
      "learning_rate": 3.998949297793697e-05,
      "loss": 2.8764,
      "step": 3241
    },
    {
      "epoch": 0.29542555130308,
      "grad_norm": 2.1786046028137207,
      "learning_rate": 3.998376458940477e-05,
      "loss": 2.8178,
      "step": 3242
    },
    {
      "epoch": 0.2955166757791143,
      "grad_norm": 1.3023468255996704,
      "learning_rate": 3.997803497289405e-05,
      "loss": 2.9703,
      "step": 3243
    },
    {
      "epoch": 0.29560780025514855,
      "grad_norm": 1.6568340063095093,
      "learning_rate": 3.997230412887436e-05,
      "loss": 2.7847,
      "step": 3244
    },
    {
      "epoch": 0.2956989247311828,
      "grad_norm": 2.457477331161499,
      "learning_rate": 3.9966572057815373e-05,
      "loss": 3.0544,
      "step": 3245
    },
    {
      "epoch": 0.2957900492072171,
      "grad_norm": 2.076369047164917,
      "learning_rate": 3.996083876018687e-05,
      "loss": 3.026,
      "step": 3246
    },
    {
      "epoch": 0.29588117368325134,
      "grad_norm": 2.2015979290008545,
      "learning_rate": 3.99551042364587e-05,
      "loss": 2.9765,
      "step": 3247
    },
    {
      "epoch": 0.2959722981592856,
      "grad_norm": 2.2368123531341553,
      "learning_rate": 3.9949368487100824e-05,
      "loss": 3.0788,
      "step": 3248
    },
    {
      "epoch": 0.2960634226353199,
      "grad_norm": 1.8182624578475952,
      "learning_rate": 3.994363151258332e-05,
      "loss": 2.7695,
      "step": 3249
    },
    {
      "epoch": 0.2961545471113541,
      "grad_norm": 1.6182432174682617,
      "learning_rate": 3.993789331337637e-05,
      "loss": 3.0731,
      "step": 3250
    },
    {
      "epoch": 0.29624567158738835,
      "grad_norm": 1.9096829891204834,
      "learning_rate": 3.993215388995021e-05,
      "loss": 3.224,
      "step": 3251
    },
    {
      "epoch": 0.2963367960634226,
      "grad_norm": 1.5368551015853882,
      "learning_rate": 3.992641324277524e-05,
      "loss": 2.9904,
      "step": 3252
    },
    {
      "epoch": 0.2964279205394569,
      "grad_norm": 2.3574771881103516,
      "learning_rate": 3.9920671372321903e-05,
      "loss": 2.7395,
      "step": 3253
    },
    {
      "epoch": 0.29651904501549115,
      "grad_norm": 2.0518367290496826,
      "learning_rate": 3.991492827906078e-05,
      "loss": 2.6001,
      "step": 3254
    },
    {
      "epoch": 0.2966101694915254,
      "grad_norm": 2.1570491790771484,
      "learning_rate": 3.990918396346254e-05,
      "loss": 3.2409,
      "step": 3255
    },
    {
      "epoch": 0.2967012939675597,
      "grad_norm": 2.8746585845947266,
      "learning_rate": 3.9903438425997954e-05,
      "loss": 3.1801,
      "step": 3256
    },
    {
      "epoch": 0.29679241844359394,
      "grad_norm": 3.1660873889923096,
      "learning_rate": 3.989769166713788e-05,
      "loss": 3.0465,
      "step": 3257
    },
    {
      "epoch": 0.2968835429196282,
      "grad_norm": 3.509960651397705,
      "learning_rate": 3.9891943687353306e-05,
      "loss": 3.1366,
      "step": 3258
    },
    {
      "epoch": 0.2969746673956625,
      "grad_norm": 1.1822510957717896,
      "learning_rate": 3.988619448711528e-05,
      "loss": 2.8601,
      "step": 3259
    },
    {
      "epoch": 0.29706579187169674,
      "grad_norm": 2.035754442214966,
      "learning_rate": 3.9880444066894995e-05,
      "loss": 2.8577,
      "step": 3260
    },
    {
      "epoch": 0.297156916347731,
      "grad_norm": 2.285456418991089,
      "learning_rate": 3.98746924271637e-05,
      "loss": 2.9062,
      "step": 3261
    },
    {
      "epoch": 0.29724804082376527,
      "grad_norm": 1.431156873703003,
      "learning_rate": 3.986893956839278e-05,
      "loss": 2.9957,
      "step": 3262
    },
    {
      "epoch": 0.29733916529979953,
      "grad_norm": 1.2853423357009888,
      "learning_rate": 3.986318549105369e-05,
      "loss": 2.9029,
      "step": 3263
    },
    {
      "epoch": 0.2974302897758338,
      "grad_norm": 5.948197364807129,
      "learning_rate": 3.9857430195618015e-05,
      "loss": 2.8601,
      "step": 3264
    },
    {
      "epoch": 0.29752141425186807,
      "grad_norm": 1.2040901184082031,
      "learning_rate": 3.9851673682557414e-05,
      "loss": 2.9263,
      "step": 3265
    },
    {
      "epoch": 0.29761253872790233,
      "grad_norm": 1.1452372074127197,
      "learning_rate": 3.984591595234366e-05,
      "loss": 2.8083,
      "step": 3266
    },
    {
      "epoch": 0.2977036632039366,
      "grad_norm": 1.1049842834472656,
      "learning_rate": 3.984015700544861e-05,
      "loss": 2.8566,
      "step": 3267
    },
    {
      "epoch": 0.29779478767997086,
      "grad_norm": 2.093379497528076,
      "learning_rate": 3.983439684234425e-05,
      "loss": 2.9786,
      "step": 3268
    },
    {
      "epoch": 0.29788591215600513,
      "grad_norm": 2.380145788192749,
      "learning_rate": 3.982863546350264e-05,
      "loss": 2.7262,
      "step": 3269
    },
    {
      "epoch": 0.29797703663203934,
      "grad_norm": 2.7174794673919678,
      "learning_rate": 3.982287286939594e-05,
      "loss": 3.2054,
      "step": 3270
    },
    {
      "epoch": 0.2980681611080736,
      "grad_norm": 2.228996753692627,
      "learning_rate": 3.9817109060496435e-05,
      "loss": 3.1691,
      "step": 3271
    },
    {
      "epoch": 0.29815928558410787,
      "grad_norm": 2.1046950817108154,
      "learning_rate": 3.981134403727648e-05,
      "loss": 3.2226,
      "step": 3272
    },
    {
      "epoch": 0.29825041006014213,
      "grad_norm": 1.3394527435302734,
      "learning_rate": 3.980557780020854e-05,
      "loss": 2.7918,
      "step": 3273
    },
    {
      "epoch": 0.2983415345361764,
      "grad_norm": 1.8363741636276245,
      "learning_rate": 3.9799810349765185e-05,
      "loss": 2.9368,
      "step": 3274
    },
    {
      "epoch": 0.29843265901221067,
      "grad_norm": 1.4822293519973755,
      "learning_rate": 3.979404168641908e-05,
      "loss": 2.9215,
      "step": 3275
    },
    {
      "epoch": 0.29852378348824493,
      "grad_norm": 2.7651150226593018,
      "learning_rate": 3.9788271810642996e-05,
      "loss": 2.924,
      "step": 3276
    },
    {
      "epoch": 0.2986149079642792,
      "grad_norm": 2.661771535873413,
      "learning_rate": 3.9782500722909783e-05,
      "loss": 2.8861,
      "step": 3277
    },
    {
      "epoch": 0.29870603244031346,
      "grad_norm": 2.571808338165283,
      "learning_rate": 3.977672842369241e-05,
      "loss": 2.8453,
      "step": 3278
    },
    {
      "epoch": 0.29879715691634773,
      "grad_norm": 1.2326580286026,
      "learning_rate": 3.977095491346394e-05,
      "loss": 2.9522,
      "step": 3279
    },
    {
      "epoch": 0.298888281392382,
      "grad_norm": 2.018003225326538,
      "learning_rate": 3.9765180192697545e-05,
      "loss": 3.0655,
      "step": 3280
    },
    {
      "epoch": 0.29897940586841626,
      "grad_norm": 2.9884445667266846,
      "learning_rate": 3.975940426186647e-05,
      "loss": 2.659,
      "step": 3281
    },
    {
      "epoch": 0.2990705303444505,
      "grad_norm": 2.3893842697143555,
      "learning_rate": 3.9753627121444086e-05,
      "loss": 2.9678,
      "step": 3282
    },
    {
      "epoch": 0.2991616548204848,
      "grad_norm": 2.146080732345581,
      "learning_rate": 3.9747848771903846e-05,
      "loss": 2.9625,
      "step": 3283
    },
    {
      "epoch": 0.29925277929651906,
      "grad_norm": 1.6857579946517944,
      "learning_rate": 3.974206921371931e-05,
      "loss": 3.0054,
      "step": 3284
    },
    {
      "epoch": 0.2993439037725533,
      "grad_norm": 2.3731446266174316,
      "learning_rate": 3.9736288447364146e-05,
      "loss": 4.2994,
      "step": 3285
    },
    {
      "epoch": 0.2994350282485876,
      "grad_norm": 2.621272325515747,
      "learning_rate": 3.973050647331209e-05,
      "loss": 3.1006,
      "step": 3286
    },
    {
      "epoch": 0.29952615272462185,
      "grad_norm": 1.618704080581665,
      "learning_rate": 3.9724723292037017e-05,
      "loss": 2.8963,
      "step": 3287
    },
    {
      "epoch": 0.2996172772006561,
      "grad_norm": 2.5786056518554688,
      "learning_rate": 3.971893890401287e-05,
      "loss": 3.1384,
      "step": 3288
    },
    {
      "epoch": 0.2997084016766904,
      "grad_norm": 1.691583275794983,
      "learning_rate": 3.971315330971371e-05,
      "loss": 3.0044,
      "step": 3289
    },
    {
      "epoch": 0.29979952615272465,
      "grad_norm": 3.025012969970703,
      "learning_rate": 3.970736650961368e-05,
      "loss": 2.8774,
      "step": 3290
    },
    {
      "epoch": 0.29989065062875886,
      "grad_norm": 2.379237174987793,
      "learning_rate": 3.9701578504187044e-05,
      "loss": 3.8189,
      "step": 3291
    },
    {
      "epoch": 0.2999817751047931,
      "grad_norm": 1.183005690574646,
      "learning_rate": 3.969578929390814e-05,
      "loss": 2.7636,
      "step": 3292
    },
    {
      "epoch": 0.3000728995808274,
      "grad_norm": 1.9405378103256226,
      "learning_rate": 3.968999887925141e-05,
      "loss": 2.8386,
      "step": 3293
    },
    {
      "epoch": 0.30016402405686166,
      "grad_norm": 2.12691330909729,
      "learning_rate": 3.9684207260691426e-05,
      "loss": 4.0828,
      "step": 3294
    },
    {
      "epoch": 0.3002551485328959,
      "grad_norm": 2.2112016677856445,
      "learning_rate": 3.967841443870281e-05,
      "loss": 3.0512,
      "step": 3295
    },
    {
      "epoch": 0.3003462730089302,
      "grad_norm": 2.01067852973938,
      "learning_rate": 3.9672620413760326e-05,
      "loss": 3.0759,
      "step": 3296
    },
    {
      "epoch": 0.30043739748496445,
      "grad_norm": 3.682785987854004,
      "learning_rate": 3.9666825186338805e-05,
      "loss": 3.2243,
      "step": 3297
    },
    {
      "epoch": 0.3005285219609987,
      "grad_norm": 2.1730833053588867,
      "learning_rate": 3.966102875691319e-05,
      "loss": 3.0942,
      "step": 3298
    },
    {
      "epoch": 0.300619646437033,
      "grad_norm": 3.4905192852020264,
      "learning_rate": 3.965523112595852e-05,
      "loss": 2.679,
      "step": 3299
    },
    {
      "epoch": 0.30071077091306725,
      "grad_norm": 2.834045648574829,
      "learning_rate": 3.964943229394994e-05,
      "loss": 2.9419,
      "step": 3300
    },
    {
      "epoch": 0.3008018953891015,
      "grad_norm": 3.034579038619995,
      "learning_rate": 3.964363226136268e-05,
      "loss": 3.0899,
      "step": 3301
    },
    {
      "epoch": 0.3008930198651358,
      "grad_norm": 2.0670230388641357,
      "learning_rate": 3.963783102867208e-05,
      "loss": 2.8372,
      "step": 3302
    },
    {
      "epoch": 0.30098414434117005,
      "grad_norm": 3.313547134399414,
      "learning_rate": 3.963202859635358e-05,
      "loss": 2.4141,
      "step": 3303
    },
    {
      "epoch": 0.3010752688172043,
      "grad_norm": 1.739801049232483,
      "learning_rate": 3.962622496488269e-05,
      "loss": 2.9116,
      "step": 3304
    },
    {
      "epoch": 0.3011663932932386,
      "grad_norm": 1.6590124368667603,
      "learning_rate": 3.962042013473507e-05,
      "loss": 2.9334,
      "step": 3305
    },
    {
      "epoch": 0.30125751776927284,
      "grad_norm": 2.543511152267456,
      "learning_rate": 3.961461410638643e-05,
      "loss": 1.6596,
      "step": 3306
    },
    {
      "epoch": 0.3013486422453071,
      "grad_norm": 2.2923457622528076,
      "learning_rate": 3.9608806880312596e-05,
      "loss": 2.9481,
      "step": 3307
    },
    {
      "epoch": 0.3014397667213414,
      "grad_norm": 2.4285995960235596,
      "learning_rate": 3.96029984569895e-05,
      "loss": 2.8431,
      "step": 3308
    },
    {
      "epoch": 0.30153089119737564,
      "grad_norm": 2.1827337741851807,
      "learning_rate": 3.9597188836893165e-05,
      "loss": 3.1386,
      "step": 3309
    },
    {
      "epoch": 0.3016220156734099,
      "grad_norm": 1.8767396211624146,
      "learning_rate": 3.959137802049971e-05,
      "loss": 3.2463,
      "step": 3310
    },
    {
      "epoch": 0.3017131401494441,
      "grad_norm": 1.0802583694458008,
      "learning_rate": 3.958556600828536e-05,
      "loss": 2.824,
      "step": 3311
    },
    {
      "epoch": 0.3018042646254784,
      "grad_norm": 1.9512442350387573,
      "learning_rate": 3.957975280072642e-05,
      "loss": 3.1158,
      "step": 3312
    },
    {
      "epoch": 0.30189538910151265,
      "grad_norm": 1.836696743965149,
      "learning_rate": 3.9573938398299315e-05,
      "loss": 2.9109,
      "step": 3313
    },
    {
      "epoch": 0.3019865135775469,
      "grad_norm": 2.190807819366455,
      "learning_rate": 3.956812280148056e-05,
      "loss": 3.2499,
      "step": 3314
    },
    {
      "epoch": 0.3020776380535812,
      "grad_norm": 2.0290489196777344,
      "learning_rate": 3.956230601074675e-05,
      "loss": 2.9713,
      "step": 3315
    },
    {
      "epoch": 0.30216876252961544,
      "grad_norm": 2.25105619430542,
      "learning_rate": 3.955648802657461e-05,
      "loss": 3.0507,
      "step": 3316
    },
    {
      "epoch": 0.3022598870056497,
      "grad_norm": 1.398346185684204,
      "learning_rate": 3.955066884944094e-05,
      "loss": 2.9131,
      "step": 3317
    },
    {
      "epoch": 0.302351011481684,
      "grad_norm": 1.8840746879577637,
      "learning_rate": 3.9544848479822656e-05,
      "loss": 3.0496,
      "step": 3318
    },
    {
      "epoch": 0.30244213595771824,
      "grad_norm": 1.3354434967041016,
      "learning_rate": 3.9539026918196745e-05,
      "loss": 2.9216,
      "step": 3319
    },
    {
      "epoch": 0.3025332604337525,
      "grad_norm": 2.936734199523926,
      "learning_rate": 3.953320416504032e-05,
      "loss": 3.1942,
      "step": 3320
    },
    {
      "epoch": 0.30262438490978677,
      "grad_norm": 1.2609974145889282,
      "learning_rate": 3.952738022083056e-05,
      "loss": 2.8596,
      "step": 3321
    },
    {
      "epoch": 0.30271550938582104,
      "grad_norm": 2.2867767810821533,
      "learning_rate": 3.952155508604477e-05,
      "loss": 2.9499,
      "step": 3322
    },
    {
      "epoch": 0.3028066338618553,
      "grad_norm": 2.0242626667022705,
      "learning_rate": 3.9515728761160356e-05,
      "loss": 2.8125,
      "step": 3323
    },
    {
      "epoch": 0.30289775833788957,
      "grad_norm": 2.8502004146575928,
      "learning_rate": 3.950990124665479e-05,
      "loss": 4.1017,
      "step": 3324
    },
    {
      "epoch": 0.30298888281392383,
      "grad_norm": 2.4715144634246826,
      "learning_rate": 3.950407254300567e-05,
      "loss": 2.9614,
      "step": 3325
    },
    {
      "epoch": 0.3030800072899581,
      "grad_norm": 2.2321619987487793,
      "learning_rate": 3.949824265069068e-05,
      "loss": 3.2357,
      "step": 3326
    },
    {
      "epoch": 0.30317113176599236,
      "grad_norm": 2.116144895553589,
      "learning_rate": 3.9492411570187586e-05,
      "loss": 2.6493,
      "step": 3327
    },
    {
      "epoch": 0.30326225624202663,
      "grad_norm": 2.5487303733825684,
      "learning_rate": 3.948657930197429e-05,
      "loss": 2.9952,
      "step": 3328
    },
    {
      "epoch": 0.3033533807180609,
      "grad_norm": 2.552366256713867,
      "learning_rate": 3.9480745846528766e-05,
      "loss": 2.928,
      "step": 3329
    },
    {
      "epoch": 0.30344450519409516,
      "grad_norm": 2.000490188598633,
      "learning_rate": 3.947491120432908e-05,
      "loss": 2.6485,
      "step": 3330
    },
    {
      "epoch": 0.30353562967012937,
      "grad_norm": 1.4275639057159424,
      "learning_rate": 3.946907537585342e-05,
      "loss": 2.9451,
      "step": 3331
    },
    {
      "epoch": 0.30362675414616364,
      "grad_norm": 1.5579934120178223,
      "learning_rate": 3.9463238361580024e-05,
      "loss": 2.8283,
      "step": 3332
    },
    {
      "epoch": 0.3037178786221979,
      "grad_norm": 2.1149392127990723,
      "learning_rate": 3.945740016198729e-05,
      "loss": 2.8972,
      "step": 3333
    },
    {
      "epoch": 0.30380900309823217,
      "grad_norm": 1.3068115711212158,
      "learning_rate": 3.9451560777553664e-05,
      "loss": 2.7776,
      "step": 3334
    },
    {
      "epoch": 0.30390012757426643,
      "grad_norm": 2.2610819339752197,
      "learning_rate": 3.9445720208757716e-05,
      "loss": 3.4191,
      "step": 3335
    },
    {
      "epoch": 0.3039912520503007,
      "grad_norm": 1.4317198991775513,
      "learning_rate": 3.9439878456078095e-05,
      "loss": 2.855,
      "step": 3336
    },
    {
      "epoch": 0.30408237652633496,
      "grad_norm": 2.051969528198242,
      "learning_rate": 3.9434035519993565e-05,
      "loss": 3.0506,
      "step": 3337
    },
    {
      "epoch": 0.30417350100236923,
      "grad_norm": 1.7823721170425415,
      "learning_rate": 3.942819140098296e-05,
      "loss": 2.7049,
      "step": 3338
    },
    {
      "epoch": 0.3042646254784035,
      "grad_norm": 2.949921131134033,
      "learning_rate": 3.9422346099525256e-05,
      "loss": 3.1549,
      "step": 3339
    },
    {
      "epoch": 0.30435574995443776,
      "grad_norm": 1.3091589212417603,
      "learning_rate": 3.9416499616099465e-05,
      "loss": 2.84,
      "step": 3340
    },
    {
      "epoch": 0.304446874430472,
      "grad_norm": 2.3337550163269043,
      "learning_rate": 3.941065195118476e-05,
      "loss": 2.8407,
      "step": 3341
    },
    {
      "epoch": 0.3045379989065063,
      "grad_norm": 1.8827712535858154,
      "learning_rate": 3.940480310526037e-05,
      "loss": 3.0224,
      "step": 3342
    },
    {
      "epoch": 0.30462912338254056,
      "grad_norm": 1.2533998489379883,
      "learning_rate": 3.9398953078805626e-05,
      "loss": 2.9044,
      "step": 3343
    },
    {
      "epoch": 0.3047202478585748,
      "grad_norm": 2.530714511871338,
      "learning_rate": 3.9393101872299964e-05,
      "loss": 2.8814,
      "step": 3344
    },
    {
      "epoch": 0.3048113723346091,
      "grad_norm": 2.681927442550659,
      "learning_rate": 3.9387249486222914e-05,
      "loss": 2.8176,
      "step": 3345
    },
    {
      "epoch": 0.30490249681064335,
      "grad_norm": 2.611804485321045,
      "learning_rate": 3.93813959210541e-05,
      "loss": 3.2744,
      "step": 3346
    },
    {
      "epoch": 0.3049936212866776,
      "grad_norm": 1.0551481246948242,
      "learning_rate": 3.9375541177273243e-05,
      "loss": 2.7216,
      "step": 3347
    },
    {
      "epoch": 0.3050847457627119,
      "grad_norm": 2.0895392894744873,
      "learning_rate": 3.9369685255360175e-05,
      "loss": 3.1297,
      "step": 3348
    },
    {
      "epoch": 0.30517587023874615,
      "grad_norm": 1.1545242071151733,
      "learning_rate": 3.936382815579479e-05,
      "loss": 2.8835,
      "step": 3349
    },
    {
      "epoch": 0.3052669947147804,
      "grad_norm": 2.8112313747406006,
      "learning_rate": 3.935796987905712e-05,
      "loss": 3.3825,
      "step": 3350
    },
    {
      "epoch": 0.3053581191908146,
      "grad_norm": 1.7366673946380615,
      "learning_rate": 3.935211042562727e-05,
      "loss": 2.8136,
      "step": 3351
    },
    {
      "epoch": 0.3054492436668489,
      "grad_norm": 2.2351937294006348,
      "learning_rate": 3.9346249795985436e-05,
      "loss": 3.0125,
      "step": 3352
    },
    {
      "epoch": 0.30554036814288316,
      "grad_norm": 1.7440356016159058,
      "learning_rate": 3.934038799061193e-05,
      "loss": 2.9998,
      "step": 3353
    },
    {
      "epoch": 0.3056314926189174,
      "grad_norm": 2.459136724472046,
      "learning_rate": 3.933452500998713e-05,
      "loss": 3.1725,
      "step": 3354
    },
    {
      "epoch": 0.3057226170949517,
      "grad_norm": 2.940506935119629,
      "learning_rate": 3.9328660854591557e-05,
      "loss": 2.6375,
      "step": 3355
    },
    {
      "epoch": 0.30581374157098595,
      "grad_norm": 2.137131929397583,
      "learning_rate": 3.932279552490579e-05,
      "loss": 2.9244,
      "step": 3356
    },
    {
      "epoch": 0.3059048660470202,
      "grad_norm": 3.571561574935913,
      "learning_rate": 3.931692902141052e-05,
      "loss": 2.9585,
      "step": 3357
    },
    {
      "epoch": 0.3059959905230545,
      "grad_norm": 1.5293288230895996,
      "learning_rate": 3.931106134458652e-05,
      "loss": 2.8735,
      "step": 3358
    },
    {
      "epoch": 0.30608711499908875,
      "grad_norm": 1.6468578577041626,
      "learning_rate": 3.930519249491468e-05,
      "loss": 2.82,
      "step": 3359
    },
    {
      "epoch": 0.306178239475123,
      "grad_norm": 1.2114201784133911,
      "learning_rate": 3.929932247287597e-05,
      "loss": 2.8182,
      "step": 3360
    },
    {
      "epoch": 0.3062693639511573,
      "grad_norm": 1.5789092779159546,
      "learning_rate": 3.9293451278951455e-05,
      "loss": 2.9722,
      "step": 3361
    },
    {
      "epoch": 0.30636048842719155,
      "grad_norm": 1.8198671340942383,
      "learning_rate": 3.928757891362231e-05,
      "loss": 2.837,
      "step": 3362
    },
    {
      "epoch": 0.3064516129032258,
      "grad_norm": 1.23991060256958,
      "learning_rate": 3.928170537736981e-05,
      "loss": 2.848,
      "step": 3363
    },
    {
      "epoch": 0.3065427373792601,
      "grad_norm": 1.6767492294311523,
      "learning_rate": 3.92758306706753e-05,
      "loss": 2.9375,
      "step": 3364
    },
    {
      "epoch": 0.30663386185529434,
      "grad_norm": 2.920522451400757,
      "learning_rate": 3.926995479402023e-05,
      "loss": 2.6961,
      "step": 3365
    },
    {
      "epoch": 0.3067249863313286,
      "grad_norm": 2.108842134475708,
      "learning_rate": 3.926407774788616e-05,
      "loss": 3.1125,
      "step": 3366
    },
    {
      "epoch": 0.3068161108073629,
      "grad_norm": 2.1443002223968506,
      "learning_rate": 3.925819953275474e-05,
      "loss": 2.9375,
      "step": 3367
    },
    {
      "epoch": 0.30690723528339714,
      "grad_norm": 1.7184861898422241,
      "learning_rate": 3.925232014910771e-05,
      "loss": 2.8422,
      "step": 3368
    },
    {
      "epoch": 0.3069983597594314,
      "grad_norm": 2.0444021224975586,
      "learning_rate": 3.924643959742691e-05,
      "loss": 2.7924,
      "step": 3369
    },
    {
      "epoch": 0.30708948423546567,
      "grad_norm": 1.7933214902877808,
      "learning_rate": 3.924055787819426e-05,
      "loss": 3.2035,
      "step": 3370
    },
    {
      "epoch": 0.30718060871149994,
      "grad_norm": 1.3837817907333374,
      "learning_rate": 3.923467499189181e-05,
      "loss": 2.8557,
      "step": 3371
    },
    {
      "epoch": 0.30727173318753415,
      "grad_norm": 2.3284969329833984,
      "learning_rate": 3.922879093900168e-05,
      "loss": 2.9159,
      "step": 3372
    },
    {
      "epoch": 0.3073628576635684,
      "grad_norm": 3.3297312259674072,
      "learning_rate": 3.92229057200061e-05,
      "loss": 3.2188,
      "step": 3373
    },
    {
      "epoch": 0.3074539821396027,
      "grad_norm": 2.3218531608581543,
      "learning_rate": 3.9217019335387364e-05,
      "loss": 3.0182,
      "step": 3374
    },
    {
      "epoch": 0.30754510661563694,
      "grad_norm": 2.4900476932525635,
      "learning_rate": 3.921113178562788e-05,
      "loss": 4.1782,
      "step": 3375
    },
    {
      "epoch": 0.3076362310916712,
      "grad_norm": 3.342829942703247,
      "learning_rate": 3.92052430712102e-05,
      "loss": 3.2694,
      "step": 3376
    },
    {
      "epoch": 0.3077273555677055,
      "grad_norm": 2.4581966400146484,
      "learning_rate": 3.9199353192616886e-05,
      "loss": 3.1881,
      "step": 3377
    },
    {
      "epoch": 0.30781848004373974,
      "grad_norm": 1.084965467453003,
      "learning_rate": 3.919346215033065e-05,
      "loss": 2.8036,
      "step": 3378
    },
    {
      "epoch": 0.307909604519774,
      "grad_norm": 2.2800076007843018,
      "learning_rate": 3.91875699448343e-05,
      "loss": 2.9318,
      "step": 3379
    },
    {
      "epoch": 0.30800072899580827,
      "grad_norm": 1.3962650299072266,
      "learning_rate": 3.918167657661069e-05,
      "loss": 2.8835,
      "step": 3380
    },
    {
      "epoch": 0.30809185347184254,
      "grad_norm": 2.4758121967315674,
      "learning_rate": 3.917578204614285e-05,
      "loss": 2.6315,
      "step": 3381
    },
    {
      "epoch": 0.3081829779478768,
      "grad_norm": 2.7477855682373047,
      "learning_rate": 3.916988635391382e-05,
      "loss": 3.1567,
      "step": 3382
    },
    {
      "epoch": 0.30827410242391107,
      "grad_norm": 1.7894096374511719,
      "learning_rate": 3.9163989500406794e-05,
      "loss": 2.8155,
      "step": 3383
    },
    {
      "epoch": 0.30836522689994533,
      "grad_norm": 2.1731622219085693,
      "learning_rate": 3.9158091486105055e-05,
      "loss": 2.7443,
      "step": 3384
    },
    {
      "epoch": 0.3084563513759796,
      "grad_norm": 2.7089898586273193,
      "learning_rate": 3.9152192311491934e-05,
      "loss": 3.064,
      "step": 3385
    },
    {
      "epoch": 0.30854747585201386,
      "grad_norm": 1.9532382488250732,
      "learning_rate": 3.914629197705092e-05,
      "loss": 2.7077,
      "step": 3386
    },
    {
      "epoch": 0.30863860032804813,
      "grad_norm": 2.2338039875030518,
      "learning_rate": 3.914039048326556e-05,
      "loss": 2.7853,
      "step": 3387
    },
    {
      "epoch": 0.3087297248040824,
      "grad_norm": 1.908815860748291,
      "learning_rate": 3.913448783061951e-05,
      "loss": 3.3591,
      "step": 3388
    },
    {
      "epoch": 0.30882084928011666,
      "grad_norm": 2.1926043033599854,
      "learning_rate": 3.912858401959651e-05,
      "loss": 3.0356,
      "step": 3389
    },
    {
      "epoch": 0.3089119737561509,
      "grad_norm": 2.130268096923828,
      "learning_rate": 3.912267905068039e-05,
      "loss": 2.8195,
      "step": 3390
    },
    {
      "epoch": 0.3090030982321852,
      "grad_norm": 2.1502511501312256,
      "learning_rate": 3.91167729243551e-05,
      "loss": 3.0072,
      "step": 3391
    },
    {
      "epoch": 0.3090942227082194,
      "grad_norm": 2.494056224822998,
      "learning_rate": 3.911086564110468e-05,
      "loss": 4.1426,
      "step": 3392
    },
    {
      "epoch": 0.30918534718425367,
      "grad_norm": 2.4329824447631836,
      "learning_rate": 3.910495720141323e-05,
      "loss": 2.8342,
      "step": 3393
    },
    {
      "epoch": 0.30927647166028793,
      "grad_norm": 2.467113494873047,
      "learning_rate": 3.9099047605764985e-05,
      "loss": 3.2334,
      "step": 3394
    },
    {
      "epoch": 0.3093675961363222,
      "grad_norm": 1.4591069221496582,
      "learning_rate": 3.9093136854644255e-05,
      "loss": 2.9314,
      "step": 3395
    },
    {
      "epoch": 0.30945872061235646,
      "grad_norm": 2.8783016204833984,
      "learning_rate": 3.9087224948535446e-05,
      "loss": 3.0722,
      "step": 3396
    },
    {
      "epoch": 0.30954984508839073,
      "grad_norm": 2.5037381649017334,
      "learning_rate": 3.9081311887923075e-05,
      "loss": 2.9057,
      "step": 3397
    },
    {
      "epoch": 0.309640969564425,
      "grad_norm": 1.7653948068618774,
      "learning_rate": 3.9075397673291725e-05,
      "loss": 2.9262,
      "step": 3398
    },
    {
      "epoch": 0.30973209404045926,
      "grad_norm": 1.1712982654571533,
      "learning_rate": 3.90694823051261e-05,
      "loss": 2.7673,
      "step": 3399
    },
    {
      "epoch": 0.3098232185164935,
      "grad_norm": 2.7521884441375732,
      "learning_rate": 3.906356578391098e-05,
      "loss": 2.9271,
      "step": 3400
    },
    {
      "epoch": 0.3099143429925278,
      "grad_norm": 1.6257795095443726,
      "learning_rate": 3.905764811013126e-05,
      "loss": 2.7494,
      "step": 3401
    },
    {
      "epoch": 0.31000546746856206,
      "grad_norm": 1.846659779548645,
      "learning_rate": 3.9051729284271904e-05,
      "loss": 2.6853,
      "step": 3402
    },
    {
      "epoch": 0.3100965919445963,
      "grad_norm": 2.120213031768799,
      "learning_rate": 3.9045809306817996e-05,
      "loss": 3.0508,
      "step": 3403
    },
    {
      "epoch": 0.3101877164206306,
      "grad_norm": 2.231119155883789,
      "learning_rate": 3.903988817825468e-05,
      "loss": 2.5887,
      "step": 3404
    },
    {
      "epoch": 0.31027884089666485,
      "grad_norm": 2.842471122741699,
      "learning_rate": 3.9033965899067235e-05,
      "loss": 3.0507,
      "step": 3405
    },
    {
      "epoch": 0.3103699653726991,
      "grad_norm": 2.0481667518615723,
      "learning_rate": 3.9028042469741e-05,
      "loss": 2.9401,
      "step": 3406
    },
    {
      "epoch": 0.3104610898487334,
      "grad_norm": 2.715531587600708,
      "learning_rate": 3.902211789076144e-05,
      "loss": 3.0729,
      "step": 3407
    },
    {
      "epoch": 0.31055221432476765,
      "grad_norm": 1.92277991771698,
      "learning_rate": 3.901619216261408e-05,
      "loss": 3.0396,
      "step": 3408
    },
    {
      "epoch": 0.3106433388008019,
      "grad_norm": 1.9949686527252197,
      "learning_rate": 3.901026528578458e-05,
      "loss": 3.021,
      "step": 3409
    },
    {
      "epoch": 0.3107344632768362,
      "grad_norm": 1.3039088249206543,
      "learning_rate": 3.900433726075865e-05,
      "loss": 2.8234,
      "step": 3410
    },
    {
      "epoch": 0.31082558775287045,
      "grad_norm": 3.799696207046509,
      "learning_rate": 3.899840808802213e-05,
      "loss": 3.0159,
      "step": 3411
    },
    {
      "epoch": 0.31091671222890466,
      "grad_norm": 1.349643349647522,
      "learning_rate": 3.899247776806093e-05,
      "loss": 2.8853,
      "step": 3412
    },
    {
      "epoch": 0.3110078367049389,
      "grad_norm": 2.0609564781188965,
      "learning_rate": 3.898654630136106e-05,
      "loss": 3.0315,
      "step": 3413
    },
    {
      "epoch": 0.3110989611809732,
      "grad_norm": 1.2567555904388428,
      "learning_rate": 3.8980613688408626e-05,
      "loss": 2.8108,
      "step": 3414
    },
    {
      "epoch": 0.31119008565700745,
      "grad_norm": 2.219871997833252,
      "learning_rate": 3.897467992968984e-05,
      "loss": 3.1944,
      "step": 3415
    },
    {
      "epoch": 0.3112812101330417,
      "grad_norm": 2.005918502807617,
      "learning_rate": 3.896874502569099e-05,
      "loss": 3.0303,
      "step": 3416
    },
    {
      "epoch": 0.311372334609076,
      "grad_norm": 1.587030291557312,
      "learning_rate": 3.896280897689847e-05,
      "loss": 2.8696,
      "step": 3417
    },
    {
      "epoch": 0.31146345908511025,
      "grad_norm": 2.618238687515259,
      "learning_rate": 3.895687178379875e-05,
      "loss": 3.0447,
      "step": 3418
    },
    {
      "epoch": 0.3115545835611445,
      "grad_norm": 2.410687208175659,
      "learning_rate": 3.8950933446878425e-05,
      "loss": 2.6785,
      "step": 3419
    },
    {
      "epoch": 0.3116457080371788,
      "grad_norm": 1.8375332355499268,
      "learning_rate": 3.894499396662414e-05,
      "loss": 3.0767,
      "step": 3420
    },
    {
      "epoch": 0.31173683251321305,
      "grad_norm": 1.1950252056121826,
      "learning_rate": 3.893905334352269e-05,
      "loss": 2.8322,
      "step": 3421
    },
    {
      "epoch": 0.3118279569892473,
      "grad_norm": 1.197407603263855,
      "learning_rate": 3.893311157806091e-05,
      "loss": 2.8477,
      "step": 3422
    },
    {
      "epoch": 0.3119190814652816,
      "grad_norm": 1.0251518487930298,
      "learning_rate": 3.892716867072576e-05,
      "loss": 2.7472,
      "step": 3423
    },
    {
      "epoch": 0.31201020594131584,
      "grad_norm": 2.943599224090576,
      "learning_rate": 3.892122462200428e-05,
      "loss": 4.0594,
      "step": 3424
    },
    {
      "epoch": 0.3121013304173501,
      "grad_norm": 1.174837350845337,
      "learning_rate": 3.8915279432383613e-05,
      "loss": 2.8497,
      "step": 3425
    },
    {
      "epoch": 0.3121924548933844,
      "grad_norm": 1.6954190731048584,
      "learning_rate": 3.890933310235099e-05,
      "loss": 2.8095,
      "step": 3426
    },
    {
      "epoch": 0.31228357936941864,
      "grad_norm": 2.6426968574523926,
      "learning_rate": 3.890338563239373e-05,
      "loss": 2.8812,
      "step": 3427
    },
    {
      "epoch": 0.3123747038454529,
      "grad_norm": 2.1078271865844727,
      "learning_rate": 3.889743702299925e-05,
      "loss": 2.9346,
      "step": 3428
    },
    {
      "epoch": 0.31246582832148717,
      "grad_norm": 2.252901792526245,
      "learning_rate": 3.889148727465508e-05,
      "loss": 3.0428,
      "step": 3429
    },
    {
      "epoch": 0.31255695279752144,
      "grad_norm": 1.5403831005096436,
      "learning_rate": 3.88855363878488e-05,
      "loss": 2.9247,
      "step": 3430
    },
    {
      "epoch": 0.3126480772735557,
      "grad_norm": 1.4234778881072998,
      "learning_rate": 3.887958436306813e-05,
      "loss": 2.904,
      "step": 3431
    },
    {
      "epoch": 0.3127392017495899,
      "grad_norm": 2.466709613800049,
      "learning_rate": 3.8873631200800854e-05,
      "loss": 3.2266,
      "step": 3432
    },
    {
      "epoch": 0.3128303262256242,
      "grad_norm": 2.5826117992401123,
      "learning_rate": 3.886767690153486e-05,
      "loss": 2.8585,
      "step": 3433
    },
    {
      "epoch": 0.31292145070165844,
      "grad_norm": 2.541489601135254,
      "learning_rate": 3.8861721465758114e-05,
      "loss": 2.928,
      "step": 3434
    },
    {
      "epoch": 0.3130125751776927,
      "grad_norm": 2.3053932189941406,
      "learning_rate": 3.8855764893958704e-05,
      "loss": 4.2166,
      "step": 3435
    },
    {
      "epoch": 0.313103699653727,
      "grad_norm": 3.8019392490386963,
      "learning_rate": 3.884980718662478e-05,
      "loss": 2.7299,
      "step": 3436
    },
    {
      "epoch": 0.31319482412976124,
      "grad_norm": 1.6723371744155884,
      "learning_rate": 3.884384834424461e-05,
      "loss": 3.0948,
      "step": 3437
    },
    {
      "epoch": 0.3132859486057955,
      "grad_norm": 2.334886074066162,
      "learning_rate": 3.8837888367306544e-05,
      "loss": 2.7228,
      "step": 3438
    },
    {
      "epoch": 0.31337707308182977,
      "grad_norm": 2.170823335647583,
      "learning_rate": 3.8831927256299014e-05,
      "loss": 2.716,
      "step": 3439
    },
    {
      "epoch": 0.31346819755786404,
      "grad_norm": 1.7788320779800415,
      "learning_rate": 3.882596501171058e-05,
      "loss": 2.711,
      "step": 3440
    },
    {
      "epoch": 0.3135593220338983,
      "grad_norm": 2.244816541671753,
      "learning_rate": 3.882000163402984e-05,
      "loss": 2.6875,
      "step": 3441
    },
    {
      "epoch": 0.31365044650993257,
      "grad_norm": 1.8564950227737427,
      "learning_rate": 3.8814037123745536e-05,
      "loss": 2.8691,
      "step": 3442
    },
    {
      "epoch": 0.31374157098596683,
      "grad_norm": 2.3491432666778564,
      "learning_rate": 3.880807148134649e-05,
      "loss": 2.9287,
      "step": 3443
    },
    {
      "epoch": 0.3138326954620011,
      "grad_norm": 2.26316499710083,
      "learning_rate": 3.880210470732159e-05,
      "loss": 3.0138,
      "step": 3444
    },
    {
      "epoch": 0.31392381993803536,
      "grad_norm": 2.00628662109375,
      "learning_rate": 3.879613680215985e-05,
      "loss": 3.1226,
      "step": 3445
    },
    {
      "epoch": 0.31401494441406963,
      "grad_norm": 1.8705519437789917,
      "learning_rate": 3.879016776635036e-05,
      "loss": 2.8709,
      "step": 3446
    },
    {
      "epoch": 0.3141060688901039,
      "grad_norm": 1.2020343542099,
      "learning_rate": 3.8784197600382305e-05,
      "loss": 2.8842,
      "step": 3447
    },
    {
      "epoch": 0.31419719336613816,
      "grad_norm": 1.6848336458206177,
      "learning_rate": 3.877822630474497e-05,
      "loss": 3.0986,
      "step": 3448
    },
    {
      "epoch": 0.3142883178421724,
      "grad_norm": 1.8549423217773438,
      "learning_rate": 3.877225387992771e-05,
      "loss": 2.8903,
      "step": 3449
    },
    {
      "epoch": 0.3143794423182067,
      "grad_norm": 2.239652156829834,
      "learning_rate": 3.876628032642001e-05,
      "loss": 2.5453,
      "step": 3450
    },
    {
      "epoch": 0.31447056679424096,
      "grad_norm": 2.2154347896575928,
      "learning_rate": 3.8760305644711405e-05,
      "loss": 3.2198,
      "step": 3451
    },
    {
      "epoch": 0.3145616912702752,
      "grad_norm": 1.4838500022888184,
      "learning_rate": 3.875432983529156e-05,
      "loss": 2.8723,
      "step": 3452
    },
    {
      "epoch": 0.31465281574630943,
      "grad_norm": 1.413851261138916,
      "learning_rate": 3.874835289865022e-05,
      "loss": 2.9931,
      "step": 3453
    },
    {
      "epoch": 0.3147439402223437,
      "grad_norm": 1.8817594051361084,
      "learning_rate": 3.874237483527719e-05,
      "loss": 2.9313,
      "step": 3454
    },
    {
      "epoch": 0.31483506469837796,
      "grad_norm": 1.378745436668396,
      "learning_rate": 3.8736395645662435e-05,
      "loss": 2.4312,
      "step": 3455
    },
    {
      "epoch": 0.31492618917441223,
      "grad_norm": 2.4630062580108643,
      "learning_rate": 3.873041533029594e-05,
      "loss": 2.8665,
      "step": 3456
    },
    {
      "epoch": 0.3150173136504465,
      "grad_norm": 1.920432209968567,
      "learning_rate": 3.872443388966783e-05,
      "loss": 2.9277,
      "step": 3457
    },
    {
      "epoch": 0.31510843812648076,
      "grad_norm": 2.144620895385742,
      "learning_rate": 3.871845132426831e-05,
      "loss": 2.9204,
      "step": 3458
    },
    {
      "epoch": 0.315199562602515,
      "grad_norm": 3.0165557861328125,
      "learning_rate": 3.871246763458767e-05,
      "loss": 3.9735,
      "step": 3459
    },
    {
      "epoch": 0.3152906870785493,
      "grad_norm": 3.541795015335083,
      "learning_rate": 3.87064828211163e-05,
      "loss": 2.6056,
      "step": 3460
    },
    {
      "epoch": 0.31538181155458356,
      "grad_norm": 1.8446309566497803,
      "learning_rate": 3.870049688434466e-05,
      "loss": 2.9104,
      "step": 3461
    },
    {
      "epoch": 0.3154729360306178,
      "grad_norm": 1.9840519428253174,
      "learning_rate": 3.869450982476335e-05,
      "loss": 3.0413,
      "step": 3462
    },
    {
      "epoch": 0.3155640605066521,
      "grad_norm": 3.635915756225586,
      "learning_rate": 3.8688521642863026e-05,
      "loss": 2.5001,
      "step": 3463
    },
    {
      "epoch": 0.31565518498268635,
      "grad_norm": 4.021359443664551,
      "learning_rate": 3.868253233913444e-05,
      "loss": 3.108,
      "step": 3464
    },
    {
      "epoch": 0.3157463094587206,
      "grad_norm": 2.231050491333008,
      "learning_rate": 3.867654191406842e-05,
      "loss": 2.9983,
      "step": 3465
    },
    {
      "epoch": 0.3158374339347549,
      "grad_norm": 2.057079553604126,
      "learning_rate": 3.867055036815593e-05,
      "loss": 2.9212,
      "step": 3466
    },
    {
      "epoch": 0.31592855841078915,
      "grad_norm": 1.2925565242767334,
      "learning_rate": 3.8664557701888e-05,
      "loss": 2.9256,
      "step": 3467
    },
    {
      "epoch": 0.3160196828868234,
      "grad_norm": 1.6090786457061768,
      "learning_rate": 3.865856391575574e-05,
      "loss": 2.8651,
      "step": 3468
    },
    {
      "epoch": 0.3161108073628577,
      "grad_norm": 2.4991142749786377,
      "learning_rate": 3.865256901025037e-05,
      "loss": 3.0867,
      "step": 3469
    },
    {
      "epoch": 0.31620193183889195,
      "grad_norm": 1.8312122821807861,
      "learning_rate": 3.864657298586319e-05,
      "loss": 2.568,
      "step": 3470
    },
    {
      "epoch": 0.3162930563149262,
      "grad_norm": 3.1910581588745117,
      "learning_rate": 3.8640575843085613e-05,
      "loss": 3.032,
      "step": 3471
    },
    {
      "epoch": 0.3163841807909605,
      "grad_norm": 1.945701003074646,
      "learning_rate": 3.863457758240912e-05,
      "loss": 2.8683,
      "step": 3472
    },
    {
      "epoch": 0.3164753052669947,
      "grad_norm": 1.8651494979858398,
      "learning_rate": 3.86285782043253e-05,
      "loss": 3.1088,
      "step": 3473
    },
    {
      "epoch": 0.31656642974302895,
      "grad_norm": 1.870724081993103,
      "learning_rate": 3.862257770932579e-05,
      "loss": 2.9664,
      "step": 3474
    },
    {
      "epoch": 0.3166575542190632,
      "grad_norm": 2.212446689605713,
      "learning_rate": 3.86165760979024e-05,
      "loss": 4.0265,
      "step": 3475
    },
    {
      "epoch": 0.3167486786950975,
      "grad_norm": 1.37941312789917,
      "learning_rate": 3.861057337054696e-05,
      "loss": 3.0043,
      "step": 3476
    },
    {
      "epoch": 0.31683980317113175,
      "grad_norm": 2.1608974933624268,
      "learning_rate": 3.860456952775143e-05,
      "loss": 3.1096,
      "step": 3477
    },
    {
      "epoch": 0.316930927647166,
      "grad_norm": 2.8855795860290527,
      "learning_rate": 3.8598564570007835e-05,
      "loss": 3.1338,
      "step": 3478
    },
    {
      "epoch": 0.3170220521232003,
      "grad_norm": 2.24932599067688,
      "learning_rate": 3.8592558497808316e-05,
      "loss": 2.9609,
      "step": 3479
    },
    {
      "epoch": 0.31711317659923455,
      "grad_norm": 2.3022964000701904,
      "learning_rate": 3.858655131164509e-05,
      "loss": 2.9631,
      "step": 3480
    },
    {
      "epoch": 0.3172043010752688,
      "grad_norm": 1.7896229028701782,
      "learning_rate": 3.858054301201047e-05,
      "loss": 3.0719,
      "step": 3481
    },
    {
      "epoch": 0.3172954255513031,
      "grad_norm": 2.1512372493743896,
      "learning_rate": 3.8574533599396866e-05,
      "loss": 2.9905,
      "step": 3482
    },
    {
      "epoch": 0.31738655002733734,
      "grad_norm": 1.5560556650161743,
      "learning_rate": 3.856852307429676e-05,
      "loss": 2.9078,
      "step": 3483
    },
    {
      "epoch": 0.3174776745033716,
      "grad_norm": 3.001721143722534,
      "learning_rate": 3.856251143720275e-05,
      "loss": 3.0922,
      "step": 3484
    },
    {
      "epoch": 0.3175687989794059,
      "grad_norm": 1.7830616235733032,
      "learning_rate": 3.85564986886075e-05,
      "loss": 2.5057,
      "step": 3485
    },
    {
      "epoch": 0.31765992345544014,
      "grad_norm": 2.075493812561035,
      "learning_rate": 3.8550484829003796e-05,
      "loss": 3.0588,
      "step": 3486
    },
    {
      "epoch": 0.3177510479314744,
      "grad_norm": 2.102257490158081,
      "learning_rate": 3.854446985888449e-05,
      "loss": 2.3655,
      "step": 3487
    },
    {
      "epoch": 0.31784217240750867,
      "grad_norm": 1.036489486694336,
      "learning_rate": 3.8538453778742524e-05,
      "loss": 2.7815,
      "step": 3488
    },
    {
      "epoch": 0.31793329688354294,
      "grad_norm": 2.0835602283477783,
      "learning_rate": 3.853243658907095e-05,
      "loss": 3.1369,
      "step": 3489
    },
    {
      "epoch": 0.3180244213595772,
      "grad_norm": 3.401007890701294,
      "learning_rate": 3.8526418290362895e-05,
      "loss": 2.8836,
      "step": 3490
    },
    {
      "epoch": 0.31811554583561147,
      "grad_norm": 2.4872357845306396,
      "learning_rate": 3.852039888311158e-05,
      "loss": 4.0426,
      "step": 3491
    },
    {
      "epoch": 0.31820667031164573,
      "grad_norm": 1.3159306049346924,
      "learning_rate": 3.851437836781034e-05,
      "loss": 2.9779,
      "step": 3492
    },
    {
      "epoch": 0.31829779478767994,
      "grad_norm": 2.644984722137451,
      "learning_rate": 3.850835674495255e-05,
      "loss": 2.9028,
      "step": 3493
    },
    {
      "epoch": 0.3183889192637142,
      "grad_norm": 2.3216521739959717,
      "learning_rate": 3.850233401503173e-05,
      "loss": 2.9687,
      "step": 3494
    },
    {
      "epoch": 0.3184800437397485,
      "grad_norm": 1.871736764907837,
      "learning_rate": 3.849631017854145e-05,
      "loss": 3.2603,
      "step": 3495
    },
    {
      "epoch": 0.31857116821578274,
      "grad_norm": 2.2219650745391846,
      "learning_rate": 3.849028523597539e-05,
      "loss": 2.7961,
      "step": 3496
    },
    {
      "epoch": 0.318662292691817,
      "grad_norm": 1.8738983869552612,
      "learning_rate": 3.848425918782731e-05,
      "loss": 2.773,
      "step": 3497
    },
    {
      "epoch": 0.31875341716785127,
      "grad_norm": 2.212796449661255,
      "learning_rate": 3.84782320345911e-05,
      "loss": 2.8145,
      "step": 3498
    },
    {
      "epoch": 0.31884454164388554,
      "grad_norm": 1.3867895603179932,
      "learning_rate": 3.847220377676067e-05,
      "loss": 2.8709,
      "step": 3499
    },
    {
      "epoch": 0.3189356661199198,
      "grad_norm": 2.09828519821167,
      "learning_rate": 3.8466174414830086e-05,
      "loss": 3.2135,
      "step": 3500
    },
    {
      "epoch": 0.31902679059595407,
      "grad_norm": 2.3266730308532715,
      "learning_rate": 3.846014394929347e-05,
      "loss": 2.8935,
      "step": 3501
    },
    {
      "epoch": 0.31911791507198833,
      "grad_norm": 1.177200436592102,
      "learning_rate": 3.8454112380645034e-05,
      "loss": 2.9348,
      "step": 3502
    },
    {
      "epoch": 0.3192090395480226,
      "grad_norm": 1.4278395175933838,
      "learning_rate": 3.84480797093791e-05,
      "loss": 2.8831,
      "step": 3503
    },
    {
      "epoch": 0.31930016402405687,
      "grad_norm": 1.414811372756958,
      "learning_rate": 3.844204593599007e-05,
      "loss": 2.8614,
      "step": 3504
    },
    {
      "epoch": 0.31939128850009113,
      "grad_norm": 2.419528007507324,
      "learning_rate": 3.8436011060972426e-05,
      "loss": 4.0818,
      "step": 3505
    },
    {
      "epoch": 0.3194824129761254,
      "grad_norm": 2.0046491622924805,
      "learning_rate": 3.8429975084820756e-05,
      "loss": 2.6729,
      "step": 3506
    },
    {
      "epoch": 0.31957353745215966,
      "grad_norm": 2.0134594440460205,
      "learning_rate": 3.842393800802973e-05,
      "loss": 3.4072,
      "step": 3507
    },
    {
      "epoch": 0.3196646619281939,
      "grad_norm": 2.404008388519287,
      "learning_rate": 3.841789983109411e-05,
      "loss": 3.0271,
      "step": 3508
    },
    {
      "epoch": 0.3197557864042282,
      "grad_norm": 2.4090816974639893,
      "learning_rate": 3.841186055450876e-05,
      "loss": 2.8736,
      "step": 3509
    },
    {
      "epoch": 0.31984691088026246,
      "grad_norm": 1.9521605968475342,
      "learning_rate": 3.8405820178768606e-05,
      "loss": 3.0929,
      "step": 3510
    },
    {
      "epoch": 0.3199380353562967,
      "grad_norm": 4.211760997772217,
      "learning_rate": 3.8399778704368676e-05,
      "loss": 4.2096,
      "step": 3511
    },
    {
      "epoch": 0.320029159832331,
      "grad_norm": 2.291348695755005,
      "learning_rate": 3.839373613180412e-05,
      "loss": 2.9645,
      "step": 3512
    },
    {
      "epoch": 0.3201202843083652,
      "grad_norm": 1.4912363290786743,
      "learning_rate": 3.838769246157012e-05,
      "loss": 2.9435,
      "step": 3513
    },
    {
      "epoch": 0.32021140878439946,
      "grad_norm": 3.197099447250366,
      "learning_rate": 3.8381647694162e-05,
      "loss": 3.0547,
      "step": 3514
    },
    {
      "epoch": 0.32030253326043373,
      "grad_norm": 1.9738296270370483,
      "learning_rate": 3.837560183007514e-05,
      "loss": 3.0424,
      "step": 3515
    },
    {
      "epoch": 0.320393657736468,
      "grad_norm": 1.6268190145492554,
      "learning_rate": 3.836955486980502e-05,
      "loss": 3.0961,
      "step": 3516
    },
    {
      "epoch": 0.32048478221250226,
      "grad_norm": 1.450237512588501,
      "learning_rate": 3.8363506813847236e-05,
      "loss": 2.9248,
      "step": 3517
    },
    {
      "epoch": 0.3205759066885365,
      "grad_norm": 1.2793781757354736,
      "learning_rate": 3.8357457662697425e-05,
      "loss": 2.8814,
      "step": 3518
    },
    {
      "epoch": 0.3206670311645708,
      "grad_norm": 1.2869077920913696,
      "learning_rate": 3.8351407416851354e-05,
      "loss": 2.8814,
      "step": 3519
    },
    {
      "epoch": 0.32075815564060506,
      "grad_norm": 2.2317440509796143,
      "learning_rate": 3.834535607680485e-05,
      "loss": 2.871,
      "step": 3520
    },
    {
      "epoch": 0.3208492801166393,
      "grad_norm": 2.2160017490386963,
      "learning_rate": 3.8339303643053857e-05,
      "loss": 2.6179,
      "step": 3521
    },
    {
      "epoch": 0.3209404045926736,
      "grad_norm": 1.2876050472259521,
      "learning_rate": 3.833325011609439e-05,
      "loss": 2.8891,
      "step": 3522
    },
    {
      "epoch": 0.32103152906870785,
      "grad_norm": 2.57661771774292,
      "learning_rate": 3.832719549642255e-05,
      "loss": 3.1396,
      "step": 3523
    },
    {
      "epoch": 0.3211226535447421,
      "grad_norm": 2.7133378982543945,
      "learning_rate": 3.8321139784534555e-05,
      "loss": 3.1798,
      "step": 3524
    },
    {
      "epoch": 0.3212137780207764,
      "grad_norm": 2.961493968963623,
      "learning_rate": 3.8315082980926684e-05,
      "loss": 2.945,
      "step": 3525
    },
    {
      "epoch": 0.32130490249681065,
      "grad_norm": 2.778820037841797,
      "learning_rate": 3.8309025086095326e-05,
      "loss": 3.9902,
      "step": 3526
    },
    {
      "epoch": 0.3213960269728449,
      "grad_norm": 2.2622756958007812,
      "learning_rate": 3.830296610053693e-05,
      "loss": 2.8874,
      "step": 3527
    },
    {
      "epoch": 0.3214871514488792,
      "grad_norm": 1.2785701751708984,
      "learning_rate": 3.829690602474807e-05,
      "loss": 2.9193,
      "step": 3528
    },
    {
      "epoch": 0.32157827592491345,
      "grad_norm": 3.271308183670044,
      "learning_rate": 3.829084485922539e-05,
      "loss": 2.8519,
      "step": 3529
    },
    {
      "epoch": 0.3216694004009477,
      "grad_norm": 3.134761095046997,
      "learning_rate": 3.828478260446563e-05,
      "loss": 3.1724,
      "step": 3530
    },
    {
      "epoch": 0.321760524876982,
      "grad_norm": 2.2734291553497314,
      "learning_rate": 3.82787192609656e-05,
      "loss": 3.0336,
      "step": 3531
    },
    {
      "epoch": 0.32185164935301624,
      "grad_norm": 2.258697748184204,
      "learning_rate": 3.8272654829222234e-05,
      "loss": 2.9495,
      "step": 3532
    },
    {
      "epoch": 0.3219427738290505,
      "grad_norm": 2.4480526447296143,
      "learning_rate": 3.826658930973252e-05,
      "loss": 3.2463,
      "step": 3533
    },
    {
      "epoch": 0.3220338983050847,
      "grad_norm": 2.011742115020752,
      "learning_rate": 3.826052270299356e-05,
      "loss": 3.0032,
      "step": 3534
    },
    {
      "epoch": 0.322125022781119,
      "grad_norm": 2.63670015335083,
      "learning_rate": 3.8254455009502545e-05,
      "loss": 3.2477,
      "step": 3535
    },
    {
      "epoch": 0.32221614725715325,
      "grad_norm": 2.2136001586914062,
      "learning_rate": 3.8248386229756724e-05,
      "loss": 2.9649,
      "step": 3536
    },
    {
      "epoch": 0.3223072717331875,
      "grad_norm": 2.030099391937256,
      "learning_rate": 3.8242316364253477e-05,
      "loss": 2.8169,
      "step": 3537
    },
    {
      "epoch": 0.3223983962092218,
      "grad_norm": 2.3711049556732178,
      "learning_rate": 3.823624541349024e-05,
      "loss": 2.1437,
      "step": 3538
    },
    {
      "epoch": 0.32248952068525605,
      "grad_norm": 2.879248857498169,
      "learning_rate": 3.8230173377964564e-05,
      "loss": 3.9261,
      "step": 3539
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 2.2520620822906494,
      "learning_rate": 3.822410025817406e-05,
      "loss": 4.1645,
      "step": 3540
    },
    {
      "epoch": 0.3226717696373246,
      "grad_norm": 1.2370309829711914,
      "learning_rate": 3.8218026054616464e-05,
      "loss": 2.9165,
      "step": 3541
    },
    {
      "epoch": 0.32276289411335884,
      "grad_norm": 1.6983673572540283,
      "learning_rate": 3.821195076778957e-05,
      "loss": 2.8017,
      "step": 3542
    },
    {
      "epoch": 0.3228540185893931,
      "grad_norm": 1.8381743431091309,
      "learning_rate": 3.820587439819127e-05,
      "loss": 3.0078,
      "step": 3543
    },
    {
      "epoch": 0.3229451430654274,
      "grad_norm": 2.213977813720703,
      "learning_rate": 3.819979694631954e-05,
      "loss": 2.9064,
      "step": 3544
    },
    {
      "epoch": 0.32303626754146164,
      "grad_norm": 2.326500177383423,
      "learning_rate": 3.819371841267247e-05,
      "loss": 2.9588,
      "step": 3545
    },
    {
      "epoch": 0.3231273920174959,
      "grad_norm": 2.162550687789917,
      "learning_rate": 3.818763879774821e-05,
      "loss": 3.0946,
      "step": 3546
    },
    {
      "epoch": 0.3232185164935302,
      "grad_norm": 2.8642733097076416,
      "learning_rate": 3.8181558102045e-05,
      "loss": 2.8678,
      "step": 3547
    },
    {
      "epoch": 0.32330964096956444,
      "grad_norm": 2.1797633171081543,
      "learning_rate": 3.8175476326061194e-05,
      "loss": 2.6603,
      "step": 3548
    },
    {
      "epoch": 0.3234007654455987,
      "grad_norm": 3.641514778137207,
      "learning_rate": 3.81693934702952e-05,
      "loss": 2.7332,
      "step": 3549
    },
    {
      "epoch": 0.32349188992163297,
      "grad_norm": 1.9680896997451782,
      "learning_rate": 3.816330953524555e-05,
      "loss": 2.8305,
      "step": 3550
    },
    {
      "epoch": 0.32358301439766723,
      "grad_norm": 1.7928955554962158,
      "learning_rate": 3.815722452141083e-05,
      "loss": 2.9732,
      "step": 3551
    },
    {
      "epoch": 0.3236741388737015,
      "grad_norm": 2.6675219535827637,
      "learning_rate": 3.815113842928975e-05,
      "loss": 2.7421,
      "step": 3552
    },
    {
      "epoch": 0.32376526334973577,
      "grad_norm": 1.9762758016586304,
      "learning_rate": 3.814505125938106e-05,
      "loss": 2.8005,
      "step": 3553
    },
    {
      "epoch": 0.32385638782577,
      "grad_norm": 2.081859588623047,
      "learning_rate": 3.8138963012183655e-05,
      "loss": 3.0469,
      "step": 3554
    },
    {
      "epoch": 0.32394751230180424,
      "grad_norm": 2.805670976638794,
      "learning_rate": 3.813287368819648e-05,
      "loss": 3.0508,
      "step": 3555
    },
    {
      "epoch": 0.3240386367778385,
      "grad_norm": 1.4577879905700684,
      "learning_rate": 3.812678328791856e-05,
      "loss": 2.8639,
      "step": 3556
    },
    {
      "epoch": 0.3241297612538728,
      "grad_norm": 1.8353952169418335,
      "learning_rate": 3.8120691811849073e-05,
      "loss": 2.8903,
      "step": 3557
    },
    {
      "epoch": 0.32422088572990704,
      "grad_norm": 1.7973177433013916,
      "learning_rate": 3.81145992604872e-05,
      "loss": 2.7886,
      "step": 3558
    },
    {
      "epoch": 0.3243120102059413,
      "grad_norm": 1.246038556098938,
      "learning_rate": 3.810850563433227e-05,
      "loss": 2.7515,
      "step": 3559
    },
    {
      "epoch": 0.32440313468197557,
      "grad_norm": 1.1420886516571045,
      "learning_rate": 3.810241093388367e-05,
      "loss": 2.9152,
      "step": 3560
    },
    {
      "epoch": 0.32449425915800983,
      "grad_norm": 1.2245725393295288,
      "learning_rate": 3.809631515964088e-05,
      "loss": 2.8555,
      "step": 3561
    },
    {
      "epoch": 0.3245853836340441,
      "grad_norm": 2.0982954502105713,
      "learning_rate": 3.8090218312103485e-05,
      "loss": 2.8679,
      "step": 3562
    },
    {
      "epoch": 0.32467650811007837,
      "grad_norm": 2.0381715297698975,
      "learning_rate": 3.808412039177114e-05,
      "loss": 3.1348,
      "step": 3563
    },
    {
      "epoch": 0.32476763258611263,
      "grad_norm": 1.8540499210357666,
      "learning_rate": 3.8078021399143596e-05,
      "loss": 3.0058,
      "step": 3564
    },
    {
      "epoch": 0.3248587570621469,
      "grad_norm": 1.596105694770813,
      "learning_rate": 3.8071921334720696e-05,
      "loss": 2.6388,
      "step": 3565
    },
    {
      "epoch": 0.32494988153818116,
      "grad_norm": 1.4206198453903198,
      "learning_rate": 3.8065820199002346e-05,
      "loss": 2.8938,
      "step": 3566
    },
    {
      "epoch": 0.32504100601421543,
      "grad_norm": 2.0634827613830566,
      "learning_rate": 3.8059717992488566e-05,
      "loss": 4.1185,
      "step": 3567
    },
    {
      "epoch": 0.3251321304902497,
      "grad_norm": 2.3967719078063965,
      "learning_rate": 3.805361471567945e-05,
      "loss": 3.0636,
      "step": 3568
    },
    {
      "epoch": 0.32522325496628396,
      "grad_norm": 1.81990385055542,
      "learning_rate": 3.804751036907521e-05,
      "loss": 3.0077,
      "step": 3569
    },
    {
      "epoch": 0.3253143794423182,
      "grad_norm": 1.2807711362838745,
      "learning_rate": 3.8041404953176105e-05,
      "loss": 2.8365,
      "step": 3570
    },
    {
      "epoch": 0.3254055039183525,
      "grad_norm": 1.9929265975952148,
      "learning_rate": 3.803529846848249e-05,
      "loss": 2.9058,
      "step": 3571
    },
    {
      "epoch": 0.32549662839438676,
      "grad_norm": 1.655985951423645,
      "learning_rate": 3.8029190915494817e-05,
      "loss": 2.8744,
      "step": 3572
    },
    {
      "epoch": 0.325587752870421,
      "grad_norm": 2.6649184226989746,
      "learning_rate": 3.8023082294713644e-05,
      "loss": 3.0605,
      "step": 3573
    },
    {
      "epoch": 0.32567887734645523,
      "grad_norm": 1.1934272050857544,
      "learning_rate": 3.801697260663958e-05,
      "loss": 2.9032,
      "step": 3574
    },
    {
      "epoch": 0.3257700018224895,
      "grad_norm": 1.47042977809906,
      "learning_rate": 3.8010861851773336e-05,
      "loss": 2.9329,
      "step": 3575
    },
    {
      "epoch": 0.32586112629852376,
      "grad_norm": 13.33100700378418,
      "learning_rate": 3.800475003061571e-05,
      "loss": 2.8847,
      "step": 3576
    },
    {
      "epoch": 0.32595225077455803,
      "grad_norm": 2.512704849243164,
      "learning_rate": 3.7998637143667617e-05,
      "loss": 2.9113,
      "step": 3577
    },
    {
      "epoch": 0.3260433752505923,
      "grad_norm": 2.946204900741577,
      "learning_rate": 3.799252319143e-05,
      "loss": 3.061,
      "step": 3578
    },
    {
      "epoch": 0.32613449972662656,
      "grad_norm": 2.181886672973633,
      "learning_rate": 3.798640817440392e-05,
      "loss": 2.9411,
      "step": 3579
    },
    {
      "epoch": 0.3262256242026608,
      "grad_norm": 2.1856305599212646,
      "learning_rate": 3.798029209309056e-05,
      "loss": 2.9324,
      "step": 3580
    },
    {
      "epoch": 0.3263167486786951,
      "grad_norm": 2.5273537635803223,
      "learning_rate": 3.7974174947991125e-05,
      "loss": 2.8295,
      "step": 3581
    },
    {
      "epoch": 0.32640787315472936,
      "grad_norm": 1.2722396850585938,
      "learning_rate": 3.796805673960696e-05,
      "loss": 2.7947,
      "step": 3582
    },
    {
      "epoch": 0.3264989976307636,
      "grad_norm": 2.0692191123962402,
      "learning_rate": 3.796193746843947e-05,
      "loss": 2.7492,
      "step": 3583
    },
    {
      "epoch": 0.3265901221067979,
      "grad_norm": 2.187669038772583,
      "learning_rate": 3.795581713499014e-05,
      "loss": 3.0926,
      "step": 3584
    },
    {
      "epoch": 0.32668124658283215,
      "grad_norm": 1.754562258720398,
      "learning_rate": 3.794969573976057e-05,
      "loss": 3.0342,
      "step": 3585
    },
    {
      "epoch": 0.3267723710588664,
      "grad_norm": 3.580096483230591,
      "learning_rate": 3.794357328325243e-05,
      "loss": 2.8849,
      "step": 3586
    },
    {
      "epoch": 0.3268634955349007,
      "grad_norm": 2.0781171321868896,
      "learning_rate": 3.7937449765967484e-05,
      "loss": 3.2012,
      "step": 3587
    },
    {
      "epoch": 0.32695462001093495,
      "grad_norm": 1.634736180305481,
      "learning_rate": 3.793132518840756e-05,
      "loss": 3.1166,
      "step": 3588
    },
    {
      "epoch": 0.3270457444869692,
      "grad_norm": 1.4602437019348145,
      "learning_rate": 3.792519955107461e-05,
      "loss": 2.9216,
      "step": 3589
    },
    {
      "epoch": 0.3271368689630035,
      "grad_norm": 1.2253246307373047,
      "learning_rate": 3.791907285447065e-05,
      "loss": 2.816,
      "step": 3590
    },
    {
      "epoch": 0.32722799343903775,
      "grad_norm": 2.5285134315490723,
      "learning_rate": 3.7912945099097783e-05,
      "loss": 3.0496,
      "step": 3591
    },
    {
      "epoch": 0.327319117915072,
      "grad_norm": 1.3302797079086304,
      "learning_rate": 3.7906816285458206e-05,
      "loss": 2.8894,
      "step": 3592
    },
    {
      "epoch": 0.3274102423911063,
      "grad_norm": 1.477612853050232,
      "learning_rate": 3.79006864140542e-05,
      "loss": 2.8261,
      "step": 3593
    },
    {
      "epoch": 0.3275013668671405,
      "grad_norm": 3.376070499420166,
      "learning_rate": 3.789455548538813e-05,
      "loss": 3.1358,
      "step": 3594
    },
    {
      "epoch": 0.32759249134317475,
      "grad_norm": 2.7934770584106445,
      "learning_rate": 3.7888423499962455e-05,
      "loss": 2.5834,
      "step": 3595
    },
    {
      "epoch": 0.327683615819209,
      "grad_norm": 1.5170634984970093,
      "learning_rate": 3.78822904582797e-05,
      "loss": 2.657,
      "step": 3596
    },
    {
      "epoch": 0.3277747402952433,
      "grad_norm": 1.0971053838729858,
      "learning_rate": 3.78761563608425e-05,
      "loss": 2.7387,
      "step": 3597
    },
    {
      "epoch": 0.32786586477127755,
      "grad_norm": 2.8880064487457275,
      "learning_rate": 3.7870021208153575e-05,
      "loss": 2.1374,
      "step": 3598
    },
    {
      "epoch": 0.3279569892473118,
      "grad_norm": 1.159522294998169,
      "learning_rate": 3.786388500071572e-05,
      "loss": 2.7721,
      "step": 3599
    },
    {
      "epoch": 0.3280481137233461,
      "grad_norm": 2.3659420013427734,
      "learning_rate": 3.7857747739031825e-05,
      "loss": 2.423,
      "step": 3600
    },
    {
      "epoch": 0.32813923819938035,
      "grad_norm": 1.2098579406738281,
      "learning_rate": 3.785160942360486e-05,
      "loss": 2.9278,
      "step": 3601
    },
    {
      "epoch": 0.3282303626754146,
      "grad_norm": 1.519789695739746,
      "learning_rate": 3.784547005493788e-05,
      "loss": 2.9118,
      "step": 3602
    },
    {
      "epoch": 0.3283214871514489,
      "grad_norm": 1.7795000076293945,
      "learning_rate": 3.783932963353404e-05,
      "loss": 2.8876,
      "step": 3603
    },
    {
      "epoch": 0.32841261162748314,
      "grad_norm": 2.018310546875,
      "learning_rate": 3.783318815989656e-05,
      "loss": 2.7162,
      "step": 3604
    },
    {
      "epoch": 0.3285037361035174,
      "grad_norm": 1.9904601573944092,
      "learning_rate": 3.782704563452876e-05,
      "loss": 2.6554,
      "step": 3605
    },
    {
      "epoch": 0.3285948605795517,
      "grad_norm": 1.8056659698486328,
      "learning_rate": 3.782090205793405e-05,
      "loss": 3.0599,
      "step": 3606
    },
    {
      "epoch": 0.32868598505558594,
      "grad_norm": 2.106795072555542,
      "learning_rate": 3.781475743061593e-05,
      "loss": 2.9979,
      "step": 3607
    },
    {
      "epoch": 0.3287771095316202,
      "grad_norm": 2.4365577697753906,
      "learning_rate": 3.780861175307796e-05,
      "loss": 1.9287,
      "step": 3608
    },
    {
      "epoch": 0.32886823400765447,
      "grad_norm": 2.552644968032837,
      "learning_rate": 3.78024650258238e-05,
      "loss": 4.1284,
      "step": 3609
    },
    {
      "epoch": 0.32895935848368874,
      "grad_norm": 1.2582393884658813,
      "learning_rate": 3.7796317249357205e-05,
      "loss": 2.871,
      "step": 3610
    },
    {
      "epoch": 0.329050482959723,
      "grad_norm": 2.0965018272399902,
      "learning_rate": 3.779016842418201e-05,
      "loss": 2.7832,
      "step": 3611
    },
    {
      "epoch": 0.32914160743575727,
      "grad_norm": 1.8264403343200684,
      "learning_rate": 3.7784018550802146e-05,
      "loss": 2.7434,
      "step": 3612
    },
    {
      "epoch": 0.32923273191179153,
      "grad_norm": 2.5371673107147217,
      "learning_rate": 3.777786762972161e-05,
      "loss": 2.1052,
      "step": 3613
    },
    {
      "epoch": 0.3293238563878258,
      "grad_norm": 2.3254425525665283,
      "learning_rate": 3.777171566144448e-05,
      "loss": 2.9953,
      "step": 3614
    },
    {
      "epoch": 0.32941498086386,
      "grad_norm": 2.7613871097564697,
      "learning_rate": 3.776556264647496e-05,
      "loss": 3.0419,
      "step": 3615
    },
    {
      "epoch": 0.3295061053398943,
      "grad_norm": 1.6124192476272583,
      "learning_rate": 3.775940858531729e-05,
      "loss": 2.6569,
      "step": 3616
    },
    {
      "epoch": 0.32959722981592854,
      "grad_norm": 2.3399152755737305,
      "learning_rate": 3.7753253478475835e-05,
      "loss": 2.533,
      "step": 3617
    },
    {
      "epoch": 0.3296883542919628,
      "grad_norm": 7.016716003417969,
      "learning_rate": 3.774709732645503e-05,
      "loss": 2.6289,
      "step": 3618
    },
    {
      "epoch": 0.32977947876799707,
      "grad_norm": 1.2630454301834106,
      "learning_rate": 3.7740940129759396e-05,
      "loss": 2.8222,
      "step": 3619
    },
    {
      "epoch": 0.32987060324403134,
      "grad_norm": 1.9310028553009033,
      "learning_rate": 3.7734781888893524e-05,
      "loss": 2.8591,
      "step": 3620
    },
    {
      "epoch": 0.3299617277200656,
      "grad_norm": 2.0908596515655518,
      "learning_rate": 3.772862260436212e-05,
      "loss": 3.0335,
      "step": 3621
    },
    {
      "epoch": 0.33005285219609987,
      "grad_norm": 1.9645522832870483,
      "learning_rate": 3.7722462276669956e-05,
      "loss": 3.0547,
      "step": 3622
    },
    {
      "epoch": 0.33014397667213413,
      "grad_norm": 2.0243303775787354,
      "learning_rate": 3.77163009063219e-05,
      "loss": 3.0732,
      "step": 3623
    },
    {
      "epoch": 0.3302351011481684,
      "grad_norm": 2.0582215785980225,
      "learning_rate": 3.77101384938229e-05,
      "loss": 2.9131,
      "step": 3624
    },
    {
      "epoch": 0.33032622562420266,
      "grad_norm": 1.0651367902755737,
      "learning_rate": 3.7703975039678e-05,
      "loss": 2.694,
      "step": 3625
    },
    {
      "epoch": 0.33041735010023693,
      "grad_norm": 2.688328742980957,
      "learning_rate": 3.7697810544392294e-05,
      "loss": 3.1618,
      "step": 3626
    },
    {
      "epoch": 0.3305084745762712,
      "grad_norm": 2.658529281616211,
      "learning_rate": 3.7691645008471e-05,
      "loss": 2.8472,
      "step": 3627
    },
    {
      "epoch": 0.33059959905230546,
      "grad_norm": 1.7140268087387085,
      "learning_rate": 3.768547843241941e-05,
      "loss": 3.0386,
      "step": 3628
    },
    {
      "epoch": 0.3306907235283397,
      "grad_norm": 2.044706344604492,
      "learning_rate": 3.7679310816742904e-05,
      "loss": 2.5032,
      "step": 3629
    },
    {
      "epoch": 0.330781848004374,
      "grad_norm": 1.1562116146087646,
      "learning_rate": 3.767314216194693e-05,
      "loss": 2.7981,
      "step": 3630
    },
    {
      "epoch": 0.33087297248040826,
      "grad_norm": 2.3712124824523926,
      "learning_rate": 3.766697246853705e-05,
      "loss": 2.8686,
      "step": 3631
    },
    {
      "epoch": 0.3309640969564425,
      "grad_norm": 1.6753591299057007,
      "learning_rate": 3.766080173701888e-05,
      "loss": 2.8225,
      "step": 3632
    },
    {
      "epoch": 0.3310552214324768,
      "grad_norm": 1.9475773572921753,
      "learning_rate": 3.765462996789813e-05,
      "loss": 2.9384,
      "step": 3633
    },
    {
      "epoch": 0.33114634590851105,
      "grad_norm": 1.664462685585022,
      "learning_rate": 3.764845716168062e-05,
      "loss": 3.0465,
      "step": 3634
    },
    {
      "epoch": 0.33123747038454526,
      "grad_norm": 4.460843086242676,
      "learning_rate": 3.764228331887223e-05,
      "loss": 2.8567,
      "step": 3635
    },
    {
      "epoch": 0.33132859486057953,
      "grad_norm": 2.0824167728424072,
      "learning_rate": 3.7636108439978924e-05,
      "loss": 2.9706,
      "step": 3636
    },
    {
      "epoch": 0.3314197193366138,
      "grad_norm": 2.212233304977417,
      "learning_rate": 3.762993252550675e-05,
      "loss": 2.9869,
      "step": 3637
    },
    {
      "epoch": 0.33151084381264806,
      "grad_norm": 2.8150484561920166,
      "learning_rate": 3.762375557596188e-05,
      "loss": 3.0454,
      "step": 3638
    },
    {
      "epoch": 0.3316019682886823,
      "grad_norm": 3.7641818523406982,
      "learning_rate": 3.7617577591850503e-05,
      "loss": 2.7087,
      "step": 3639
    },
    {
      "epoch": 0.3316930927647166,
      "grad_norm": 1.8692729473114014,
      "learning_rate": 3.761139857367896e-05,
      "loss": 3.0668,
      "step": 3640
    },
    {
      "epoch": 0.33178421724075086,
      "grad_norm": 1.8889697790145874,
      "learning_rate": 3.7605218521953614e-05,
      "loss": 3.1541,
      "step": 3641
    },
    {
      "epoch": 0.3318753417167851,
      "grad_norm": 1.141048789024353,
      "learning_rate": 3.759903743718097e-05,
      "loss": 2.8648,
      "step": 3642
    },
    {
      "epoch": 0.3319664661928194,
      "grad_norm": 1.916425108909607,
      "learning_rate": 3.759285531986759e-05,
      "loss": 2.6194,
      "step": 3643
    },
    {
      "epoch": 0.33205759066885365,
      "grad_norm": 2.4530365467071533,
      "learning_rate": 3.758667217052011e-05,
      "loss": 3.1228,
      "step": 3644
    },
    {
      "epoch": 0.3321487151448879,
      "grad_norm": 2.297433376312256,
      "learning_rate": 3.7580487989645264e-05,
      "loss": 2.8492,
      "step": 3645
    },
    {
      "epoch": 0.3322398396209222,
      "grad_norm": 2.2126307487487793,
      "learning_rate": 3.757430277774988e-05,
      "loss": 3.0181,
      "step": 3646
    },
    {
      "epoch": 0.33233096409695645,
      "grad_norm": 1.4579991102218628,
      "learning_rate": 3.7568116535340856e-05,
      "loss": 3.0084,
      "step": 3647
    },
    {
      "epoch": 0.3324220885729907,
      "grad_norm": 1.079192876815796,
      "learning_rate": 3.756192926292519e-05,
      "loss": 2.7329,
      "step": 3648
    },
    {
      "epoch": 0.332513213049025,
      "grad_norm": 1.442439317703247,
      "learning_rate": 3.755574096100994e-05,
      "loss": 2.8468,
      "step": 3649
    },
    {
      "epoch": 0.33260433752505925,
      "grad_norm": 2.3472225666046143,
      "learning_rate": 3.7549551630102256e-05,
      "loss": 2.6861,
      "step": 3650
    },
    {
      "epoch": 0.3326954620010935,
      "grad_norm": 1.904333233833313,
      "learning_rate": 3.754336127070939e-05,
      "loss": 2.9325,
      "step": 3651
    },
    {
      "epoch": 0.3327865864771278,
      "grad_norm": 2.2136824131011963,
      "learning_rate": 3.753716988333866e-05,
      "loss": 2.8606,
      "step": 3652
    },
    {
      "epoch": 0.33287771095316204,
      "grad_norm": 1.6175414323806763,
      "learning_rate": 3.753097746849749e-05,
      "loss": 2.9587,
      "step": 3653
    },
    {
      "epoch": 0.3329688354291963,
      "grad_norm": 2.9096875190734863,
      "learning_rate": 3.752478402669335e-05,
      "loss": 2.9995,
      "step": 3654
    },
    {
      "epoch": 0.3330599599052305,
      "grad_norm": 3.0065600872039795,
      "learning_rate": 3.7518589558433826e-05,
      "loss": 2.7204,
      "step": 3655
    },
    {
      "epoch": 0.3331510843812648,
      "grad_norm": 2.2882797718048096,
      "learning_rate": 3.751239406422659e-05,
      "loss": 2.5391,
      "step": 3656
    },
    {
      "epoch": 0.33324220885729905,
      "grad_norm": 2.750312328338623,
      "learning_rate": 3.750619754457937e-05,
      "loss": 3.7979,
      "step": 3657
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 3.558274269104004,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.8068,
      "step": 3658
    },
    {
      "epoch": 0.3334244578093676,
      "grad_norm": 2.719878673553467,
      "learning_rate": 3.74938014309964e-05,
      "loss": 2.5815,
      "step": 3659
    },
    {
      "epoch": 0.33351558228540185,
      "grad_norm": 1.7002638578414917,
      "learning_rate": 3.748760183807656e-05,
      "loss": 2.8407,
      "step": 3660
    },
    {
      "epoch": 0.3336067067614361,
      "grad_norm": 1.5375850200653076,
      "learning_rate": 3.748140122174857e-05,
      "loss": 2.9178,
      "step": 3661
    },
    {
      "epoch": 0.3336978312374704,
      "grad_norm": 1.8764402866363525,
      "learning_rate": 3.747519958252059e-05,
      "loss": 2.9464,
      "step": 3662
    },
    {
      "epoch": 0.33378895571350464,
      "grad_norm": 2.093611240386963,
      "learning_rate": 3.746899692090087e-05,
      "loss": 2.9676,
      "step": 3663
    },
    {
      "epoch": 0.3338800801895389,
      "grad_norm": 2.0597550868988037,
      "learning_rate": 3.746279323739773e-05,
      "loss": 2.8769,
      "step": 3664
    },
    {
      "epoch": 0.3339712046655732,
      "grad_norm": 1.3085588216781616,
      "learning_rate": 3.7456588532519596e-05,
      "loss": 2.8863,
      "step": 3665
    },
    {
      "epoch": 0.33406232914160744,
      "grad_norm": 1.0959607362747192,
      "learning_rate": 3.7450382806774976e-05,
      "loss": 2.8511,
      "step": 3666
    },
    {
      "epoch": 0.3341534536176417,
      "grad_norm": 2.0351948738098145,
      "learning_rate": 3.7444176060672446e-05,
      "loss": 3.1706,
      "step": 3667
    },
    {
      "epoch": 0.33424457809367597,
      "grad_norm": 2.588010311126709,
      "learning_rate": 3.743796829472067e-05,
      "loss": 3.1149,
      "step": 3668
    },
    {
      "epoch": 0.33433570256971024,
      "grad_norm": 1.236118197441101,
      "learning_rate": 3.743175950942841e-05,
      "loss": 2.8932,
      "step": 3669
    },
    {
      "epoch": 0.3344268270457445,
      "grad_norm": 1.674843430519104,
      "learning_rate": 3.742554970530448e-05,
      "loss": 2.7956,
      "step": 3670
    },
    {
      "epoch": 0.33451795152177877,
      "grad_norm": 3.493479013442993,
      "learning_rate": 3.741933888285782e-05,
      "loss": 3.0162,
      "step": 3671
    },
    {
      "epoch": 0.33460907599781303,
      "grad_norm": 1.3938205242156982,
      "learning_rate": 3.741312704259742e-05,
      "loss": 2.8694,
      "step": 3672
    },
    {
      "epoch": 0.3347002004738473,
      "grad_norm": 1.0905414819717407,
      "learning_rate": 3.740691418503237e-05,
      "loss": 2.7244,
      "step": 3673
    },
    {
      "epoch": 0.33479132494988156,
      "grad_norm": 1.9019707441329956,
      "learning_rate": 3.740070031067183e-05,
      "loss": 2.8423,
      "step": 3674
    },
    {
      "epoch": 0.3348824494259158,
      "grad_norm": 2.2855823040008545,
      "learning_rate": 3.739448542002506e-05,
      "loss": 3.0819,
      "step": 3675
    },
    {
      "epoch": 0.33497357390195004,
      "grad_norm": 2.6345691680908203,
      "learning_rate": 3.738826951360139e-05,
      "loss": 4.0184,
      "step": 3676
    },
    {
      "epoch": 0.3350646983779843,
      "grad_norm": 1.8370808362960815,
      "learning_rate": 3.7382052591910247e-05,
      "loss": 3.0924,
      "step": 3677
    },
    {
      "epoch": 0.33515582285401857,
      "grad_norm": 2.2869410514831543,
      "learning_rate": 3.7375834655461113e-05,
      "loss": 2.8794,
      "step": 3678
    },
    {
      "epoch": 0.33524694733005284,
      "grad_norm": 2.4011683464050293,
      "learning_rate": 3.736961570476358e-05,
      "loss": 3.055,
      "step": 3679
    },
    {
      "epoch": 0.3353380718060871,
      "grad_norm": 1.3514655828475952,
      "learning_rate": 3.7363395740327337e-05,
      "loss": 2.8567,
      "step": 3680
    },
    {
      "epoch": 0.33542919628212137,
      "grad_norm": 2.929626703262329,
      "learning_rate": 3.735717476266211e-05,
      "loss": 3.1123,
      "step": 3681
    },
    {
      "epoch": 0.33552032075815563,
      "grad_norm": 5.187368869781494,
      "learning_rate": 3.735095277227773e-05,
      "loss": 2.8137,
      "step": 3682
    },
    {
      "epoch": 0.3356114452341899,
      "grad_norm": 1.621717095375061,
      "learning_rate": 3.7344729769684136e-05,
      "loss": 2.658,
      "step": 3683
    },
    {
      "epoch": 0.33570256971022416,
      "grad_norm": 2.349963665008545,
      "learning_rate": 3.733850575539131e-05,
      "loss": 2.6163,
      "step": 3684
    },
    {
      "epoch": 0.33579369418625843,
      "grad_norm": 2.787994623184204,
      "learning_rate": 3.733228072990933e-05,
      "loss": 2.8888,
      "step": 3685
    },
    {
      "epoch": 0.3358848186622927,
      "grad_norm": 1.146318793296814,
      "learning_rate": 3.7326054693748386e-05,
      "loss": 2.8624,
      "step": 3686
    },
    {
      "epoch": 0.33597594313832696,
      "grad_norm": 1.8034451007843018,
      "learning_rate": 3.7319827647418705e-05,
      "loss": 2.6419,
      "step": 3687
    },
    {
      "epoch": 0.3360670676143612,
      "grad_norm": 2.7330210208892822,
      "learning_rate": 3.7313599591430623e-05,
      "loss": 3.2251,
      "step": 3688
    },
    {
      "epoch": 0.3361581920903955,
      "grad_norm": 2.3011486530303955,
      "learning_rate": 3.7307370526294556e-05,
      "loss": 2.9376,
      "step": 3689
    },
    {
      "epoch": 0.33624931656642976,
      "grad_norm": 2.0622589588165283,
      "learning_rate": 3.730114045252099e-05,
      "loss": 2.7786,
      "step": 3690
    },
    {
      "epoch": 0.336340441042464,
      "grad_norm": 2.5086147785186768,
      "learning_rate": 3.729490937062053e-05,
      "loss": 2.4636,
      "step": 3691
    },
    {
      "epoch": 0.3364315655184983,
      "grad_norm": 3.602062702178955,
      "learning_rate": 3.728867728110381e-05,
      "loss": 3.0093,
      "step": 3692
    },
    {
      "epoch": 0.33652268999453255,
      "grad_norm": 2.726280927658081,
      "learning_rate": 3.728244418448159e-05,
      "loss": 3.0684,
      "step": 3693
    },
    {
      "epoch": 0.3366138144705668,
      "grad_norm": 1.9867115020751953,
      "learning_rate": 3.727621008126468e-05,
      "loss": 2.8747,
      "step": 3694
    },
    {
      "epoch": 0.3367049389466011,
      "grad_norm": 2.2067136764526367,
      "learning_rate": 3.7269974971964015e-05,
      "loss": 2.8044,
      "step": 3695
    },
    {
      "epoch": 0.3367960634226353,
      "grad_norm": 1.6470836400985718,
      "learning_rate": 3.726373885709057e-05,
      "loss": 2.9638,
      "step": 3696
    },
    {
      "epoch": 0.33688718789866956,
      "grad_norm": 2.4782204627990723,
      "learning_rate": 3.725750173715542e-05,
      "loss": 3.0128,
      "step": 3697
    },
    {
      "epoch": 0.3369783123747038,
      "grad_norm": 2.0846431255340576,
      "learning_rate": 3.725126361266973e-05,
      "loss": 2.9457,
      "step": 3698
    },
    {
      "epoch": 0.3370694368507381,
      "grad_norm": 2.8557138442993164,
      "learning_rate": 3.7245024484144726e-05,
      "loss": 3.216,
      "step": 3699
    },
    {
      "epoch": 0.33716056132677236,
      "grad_norm": 1.9156532287597656,
      "learning_rate": 3.723878435209173e-05,
      "loss": 2.8454,
      "step": 3700
    },
    {
      "epoch": 0.3372516858028066,
      "grad_norm": 2.2322258949279785,
      "learning_rate": 3.723254321702216e-05,
      "loss": 3.0481,
      "step": 3701
    },
    {
      "epoch": 0.3373428102788409,
      "grad_norm": 2.0526881217956543,
      "learning_rate": 3.722630107944749e-05,
      "loss": 2.3686,
      "step": 3702
    },
    {
      "epoch": 0.33743393475487515,
      "grad_norm": 1.9518932104110718,
      "learning_rate": 3.722005793987929e-05,
      "loss": 3.0534,
      "step": 3703
    },
    {
      "epoch": 0.3375250592309094,
      "grad_norm": 1.9331350326538086,
      "learning_rate": 3.721381379882921e-05,
      "loss": 2.7305,
      "step": 3704
    },
    {
      "epoch": 0.3376161837069437,
      "grad_norm": 2.6682727336883545,
      "learning_rate": 3.720756865680899e-05,
      "loss": 2.9114,
      "step": 3705
    },
    {
      "epoch": 0.33770730818297795,
      "grad_norm": 2.155076265335083,
      "learning_rate": 3.7201322514330426e-05,
      "loss": 3.1294,
      "step": 3706
    },
    {
      "epoch": 0.3377984326590122,
      "grad_norm": 1.8749901056289673,
      "learning_rate": 3.7195075371905425e-05,
      "loss": 3.0455,
      "step": 3707
    },
    {
      "epoch": 0.3378895571350465,
      "grad_norm": 2.6369588375091553,
      "learning_rate": 3.718882723004596e-05,
      "loss": 2.7799,
      "step": 3708
    },
    {
      "epoch": 0.33798068161108075,
      "grad_norm": 1.426007866859436,
      "learning_rate": 3.718257808926411e-05,
      "loss": 2.9507,
      "step": 3709
    },
    {
      "epoch": 0.338071806087115,
      "grad_norm": 2.01214861869812,
      "learning_rate": 3.7176327950072e-05,
      "loss": 3.0599,
      "step": 3710
    },
    {
      "epoch": 0.3381629305631493,
      "grad_norm": 1.8810529708862305,
      "learning_rate": 3.717007681298185e-05,
      "loss": 2.7749,
      "step": 3711
    },
    {
      "epoch": 0.33825405503918354,
      "grad_norm": 1.4173189401626587,
      "learning_rate": 3.716382467850597e-05,
      "loss": 2.8745,
      "step": 3712
    },
    {
      "epoch": 0.3383451795152178,
      "grad_norm": 1.1644079685211182,
      "learning_rate": 3.715757154715674e-05,
      "loss": 2.8596,
      "step": 3713
    },
    {
      "epoch": 0.3384363039912521,
      "grad_norm": 2.2849273681640625,
      "learning_rate": 3.715131741944664e-05,
      "loss": 3.0262,
      "step": 3714
    },
    {
      "epoch": 0.33852742846728634,
      "grad_norm": 1.9580084085464478,
      "learning_rate": 3.7145062295888226e-05,
      "loss": 2.8999,
      "step": 3715
    },
    {
      "epoch": 0.33861855294332055,
      "grad_norm": 2.842846393585205,
      "learning_rate": 3.7138806176994115e-05,
      "loss": 3.0116,
      "step": 3716
    },
    {
      "epoch": 0.3387096774193548,
      "grad_norm": 5.252993106842041,
      "learning_rate": 3.713254906327703e-05,
      "loss": 4.2417,
      "step": 3717
    },
    {
      "epoch": 0.3388008018953891,
      "grad_norm": 2.609650135040283,
      "learning_rate": 3.712629095524976e-05,
      "loss": 3.1288,
      "step": 3718
    },
    {
      "epoch": 0.33889192637142335,
      "grad_norm": 1.4531925916671753,
      "learning_rate": 3.712003185342519e-05,
      "loss": 2.843,
      "step": 3719
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 2.7067973613739014,
      "learning_rate": 3.711377175831626e-05,
      "loss": 2.7447,
      "step": 3720
    },
    {
      "epoch": 0.3390741753234919,
      "grad_norm": 1.885167121887207,
      "learning_rate": 3.710751067043602e-05,
      "loss": 2.9192,
      "step": 3721
    },
    {
      "epoch": 0.33916529979952614,
      "grad_norm": 1.6406117677688599,
      "learning_rate": 3.710124859029761e-05,
      "loss": 2.9024,
      "step": 3722
    },
    {
      "epoch": 0.3392564242755604,
      "grad_norm": 2.372640371322632,
      "learning_rate": 3.70949855184142e-05,
      "loss": 2.8994,
      "step": 3723
    },
    {
      "epoch": 0.3393475487515947,
      "grad_norm": 2.2391128540039062,
      "learning_rate": 3.708872145529909e-05,
      "loss": 2.853,
      "step": 3724
    },
    {
      "epoch": 0.33943867322762894,
      "grad_norm": 3.2368927001953125,
      "learning_rate": 3.708245640146564e-05,
      "loss": 2.9096,
      "step": 3725
    },
    {
      "epoch": 0.3395297977036632,
      "grad_norm": 1.9533005952835083,
      "learning_rate": 3.707619035742731e-05,
      "loss": 2.996,
      "step": 3726
    },
    {
      "epoch": 0.33962092217969747,
      "grad_norm": 2.064802646636963,
      "learning_rate": 3.7069923323697603e-05,
      "loss": 3.0564,
      "step": 3727
    },
    {
      "epoch": 0.33971204665573174,
      "grad_norm": 1.7938506603240967,
      "learning_rate": 3.7063655300790154e-05,
      "loss": 3.0625,
      "step": 3728
    },
    {
      "epoch": 0.339803171131766,
      "grad_norm": 2.92805552482605,
      "learning_rate": 3.705738628921862e-05,
      "loss": 3.0712,
      "step": 3729
    },
    {
      "epoch": 0.33989429560780027,
      "grad_norm": 2.3158822059631348,
      "learning_rate": 3.70511162894968e-05,
      "loss": 3.1026,
      "step": 3730
    },
    {
      "epoch": 0.33998542008383453,
      "grad_norm": 2.0895090103149414,
      "learning_rate": 3.7044845302138534e-05,
      "loss": 3.0799,
      "step": 3731
    },
    {
      "epoch": 0.3400765445598688,
      "grad_norm": 1.9539893865585327,
      "learning_rate": 3.703857332765774e-05,
      "loss": 3.0673,
      "step": 3732
    },
    {
      "epoch": 0.34016766903590306,
      "grad_norm": 2.086512565612793,
      "learning_rate": 3.7032300366568464e-05,
      "loss": 2.7398,
      "step": 3733
    },
    {
      "epoch": 0.34025879351193733,
      "grad_norm": 3.028594970703125,
      "learning_rate": 3.7026026419384774e-05,
      "loss": 2.8648,
      "step": 3734
    },
    {
      "epoch": 0.3403499179879716,
      "grad_norm": 1.1446874141693115,
      "learning_rate": 3.701975148662085e-05,
      "loss": 2.7902,
      "step": 3735
    },
    {
      "epoch": 0.3404410424640058,
      "grad_norm": 3.361703395843506,
      "learning_rate": 3.701347556879096e-05,
      "loss": 2.4931,
      "step": 3736
    },
    {
      "epoch": 0.34053216694004007,
      "grad_norm": 1.680263876914978,
      "learning_rate": 3.7007198666409414e-05,
      "loss": 3.0087,
      "step": 3737
    },
    {
      "epoch": 0.34062329141607434,
      "grad_norm": 1.5789599418640137,
      "learning_rate": 3.700092077999066e-05,
      "loss": 2.7566,
      "step": 3738
    },
    {
      "epoch": 0.3407144158921086,
      "grad_norm": 3.1787707805633545,
      "learning_rate": 3.699464191004916e-05,
      "loss": 2.5227,
      "step": 3739
    },
    {
      "epoch": 0.34080554036814287,
      "grad_norm": 3.1545209884643555,
      "learning_rate": 3.6988362057099524e-05,
      "loss": 2.8661,
      "step": 3740
    },
    {
      "epoch": 0.34089666484417713,
      "grad_norm": 3.081024169921875,
      "learning_rate": 3.69820812216564e-05,
      "loss": 2.5312,
      "step": 3741
    },
    {
      "epoch": 0.3409877893202114,
      "grad_norm": 2.172288179397583,
      "learning_rate": 3.6975799404234524e-05,
      "loss": 2.8533,
      "step": 3742
    },
    {
      "epoch": 0.34107891379624566,
      "grad_norm": 1.9700849056243896,
      "learning_rate": 3.696951660534871e-05,
      "loss": 2.9677,
      "step": 3743
    },
    {
      "epoch": 0.34117003827227993,
      "grad_norm": 1.8857417106628418,
      "learning_rate": 3.696323282551387e-05,
      "loss": 3.1741,
      "step": 3744
    },
    {
      "epoch": 0.3412611627483142,
      "grad_norm": 1.3207206726074219,
      "learning_rate": 3.695694806524498e-05,
      "loss": 2.7903,
      "step": 3745
    },
    {
      "epoch": 0.34135228722434846,
      "grad_norm": 1.6291221380233765,
      "learning_rate": 3.695066232505711e-05,
      "loss": 3.1027,
      "step": 3746
    },
    {
      "epoch": 0.3414434117003827,
      "grad_norm": 3.3361635208129883,
      "learning_rate": 3.694437560546538e-05,
      "loss": 3.0135,
      "step": 3747
    },
    {
      "epoch": 0.341534536176417,
      "grad_norm": 1.202041745185852,
      "learning_rate": 3.6938087906985034e-05,
      "loss": 2.7888,
      "step": 3748
    },
    {
      "epoch": 0.34162566065245126,
      "grad_norm": 1.8938047885894775,
      "learning_rate": 3.6931799230131345e-05,
      "loss": 2.1924,
      "step": 3749
    },
    {
      "epoch": 0.3417167851284855,
      "grad_norm": 2.2729129791259766,
      "learning_rate": 3.6925509575419726e-05,
      "loss": 2.7178,
      "step": 3750
    },
    {
      "epoch": 0.3418079096045198,
      "grad_norm": 2.4569075107574463,
      "learning_rate": 3.691921894336563e-05,
      "loss": 2.9816,
      "step": 3751
    },
    {
      "epoch": 0.34189903408055405,
      "grad_norm": 1.3995429277420044,
      "learning_rate": 3.6912927334484594e-05,
      "loss": 2.8595,
      "step": 3752
    },
    {
      "epoch": 0.3419901585565883,
      "grad_norm": 1.6994378566741943,
      "learning_rate": 3.690663474929223e-05,
      "loss": 3.0544,
      "step": 3753
    },
    {
      "epoch": 0.3420812830326226,
      "grad_norm": 2.1677842140197754,
      "learning_rate": 3.690034118830426e-05,
      "loss": 2.7864,
      "step": 3754
    },
    {
      "epoch": 0.34217240750865685,
      "grad_norm": 1.1697204113006592,
      "learning_rate": 3.689404665203646e-05,
      "loss": 2.8578,
      "step": 3755
    },
    {
      "epoch": 0.34226353198469106,
      "grad_norm": 1.762640357017517,
      "learning_rate": 3.688775114100468e-05,
      "loss": 3.0323,
      "step": 3756
    },
    {
      "epoch": 0.3423546564607253,
      "grad_norm": 2.424316167831421,
      "learning_rate": 3.688145465572488e-05,
      "loss": 2.8762,
      "step": 3757
    },
    {
      "epoch": 0.3424457809367596,
      "grad_norm": 2.62979793548584,
      "learning_rate": 3.687515719671307e-05,
      "loss": 3.0399,
      "step": 3758
    },
    {
      "epoch": 0.34253690541279386,
      "grad_norm": 1.9522184133529663,
      "learning_rate": 3.686885876448536e-05,
      "loss": 2.9222,
      "step": 3759
    },
    {
      "epoch": 0.3426280298888281,
      "grad_norm": 1.2088966369628906,
      "learning_rate": 3.686255935955792e-05,
      "loss": 2.8072,
      "step": 3760
    },
    {
      "epoch": 0.3427191543648624,
      "grad_norm": 2.6949169635772705,
      "learning_rate": 3.685625898244702e-05,
      "loss": 3.1022,
      "step": 3761
    },
    {
      "epoch": 0.34281027884089665,
      "grad_norm": 1.8148874044418335,
      "learning_rate": 3.684995763366899e-05,
      "loss": 2.8332,
      "step": 3762
    },
    {
      "epoch": 0.3429014033169309,
      "grad_norm": 1.1772524118423462,
      "learning_rate": 3.684365531374026e-05,
      "loss": 2.9215,
      "step": 3763
    },
    {
      "epoch": 0.3429925277929652,
      "grad_norm": 1.3842970132827759,
      "learning_rate": 3.683735202317734e-05,
      "loss": 2.8118,
      "step": 3764
    },
    {
      "epoch": 0.34308365226899945,
      "grad_norm": 3.043264627456665,
      "learning_rate": 3.683104776249679e-05,
      "loss": 2.7206,
      "step": 3765
    },
    {
      "epoch": 0.3431747767450337,
      "grad_norm": 2.0299227237701416,
      "learning_rate": 3.682474253221527e-05,
      "loss": 2.8408,
      "step": 3766
    },
    {
      "epoch": 0.343265901221068,
      "grad_norm": 3.068495273590088,
      "learning_rate": 3.6818436332849537e-05,
      "loss": 2.9117,
      "step": 3767
    },
    {
      "epoch": 0.34335702569710225,
      "grad_norm": 1.3513349294662476,
      "learning_rate": 3.6812129164916385e-05,
      "loss": 2.9392,
      "step": 3768
    },
    {
      "epoch": 0.3434481501731365,
      "grad_norm": 2.681048631668091,
      "learning_rate": 3.680582102893273e-05,
      "loss": 3.8444,
      "step": 3769
    },
    {
      "epoch": 0.3435392746491708,
      "grad_norm": 2.032358407974243,
      "learning_rate": 3.679951192541555e-05,
      "loss": 2.6728,
      "step": 3770
    },
    {
      "epoch": 0.34363039912520504,
      "grad_norm": 1.452744960784912,
      "learning_rate": 3.679320185488188e-05,
      "loss": 2.8484,
      "step": 3771
    },
    {
      "epoch": 0.3437215236012393,
      "grad_norm": 1.6110174655914307,
      "learning_rate": 3.678689081784887e-05,
      "loss": 2.9387,
      "step": 3772
    },
    {
      "epoch": 0.3438126480772736,
      "grad_norm": 2.706721305847168,
      "learning_rate": 3.678057881483373e-05,
      "loss": 3.0196,
      "step": 3773
    },
    {
      "epoch": 0.34390377255330784,
      "grad_norm": 1.2235610485076904,
      "learning_rate": 3.6774265846353765e-05,
      "loss": 2.7725,
      "step": 3774
    },
    {
      "epoch": 0.3439948970293421,
      "grad_norm": 1.2344046831130981,
      "learning_rate": 3.676795191292633e-05,
      "loss": 2.7469,
      "step": 3775
    },
    {
      "epoch": 0.34408602150537637,
      "grad_norm": 1.3351609706878662,
      "learning_rate": 3.67616370150689e-05,
      "loss": 2.654,
      "step": 3776
    },
    {
      "epoch": 0.3441771459814106,
      "grad_norm": 1.189914345741272,
      "learning_rate": 3.675532115329897e-05,
      "loss": 2.849,
      "step": 3777
    },
    {
      "epoch": 0.34426827045744485,
      "grad_norm": 1.9014474153518677,
      "learning_rate": 3.674900432813418e-05,
      "loss": 2.5883,
      "step": 3778
    },
    {
      "epoch": 0.3443593949334791,
      "grad_norm": 1.2430446147918701,
      "learning_rate": 3.674268654009221e-05,
      "loss": 2.8069,
      "step": 3779
    },
    {
      "epoch": 0.3444505194095134,
      "grad_norm": 2.788770914077759,
      "learning_rate": 3.673636778969083e-05,
      "loss": 3.0801,
      "step": 3780
    },
    {
      "epoch": 0.34454164388554764,
      "grad_norm": 1.9176347255706787,
      "learning_rate": 3.6730048077447875e-05,
      "loss": 2.454,
      "step": 3781
    },
    {
      "epoch": 0.3446327683615819,
      "grad_norm": 1.211824893951416,
      "learning_rate": 3.6723727403881284e-05,
      "loss": 2.7846,
      "step": 3782
    },
    {
      "epoch": 0.3447238928376162,
      "grad_norm": 1.98549485206604,
      "learning_rate": 3.671740576950905e-05,
      "loss": 2.7239,
      "step": 3783
    },
    {
      "epoch": 0.34481501731365044,
      "grad_norm": 1.3436033725738525,
      "learning_rate": 3.671108317484927e-05,
      "loss": 2.7226,
      "step": 3784
    },
    {
      "epoch": 0.3449061417896847,
      "grad_norm": 1.1203218698501587,
      "learning_rate": 3.670475962042009e-05,
      "loss": 2.8068,
      "step": 3785
    },
    {
      "epoch": 0.34499726626571897,
      "grad_norm": 1.8679561614990234,
      "learning_rate": 3.669843510673976e-05,
      "loss": 2.8724,
      "step": 3786
    },
    {
      "epoch": 0.34508839074175324,
      "grad_norm": 1.7326226234436035,
      "learning_rate": 3.6692109634326594e-05,
      "loss": 2.7293,
      "step": 3787
    },
    {
      "epoch": 0.3451795152177875,
      "grad_norm": 2.1023612022399902,
      "learning_rate": 3.668578320369899e-05,
      "loss": 2.665,
      "step": 3788
    },
    {
      "epoch": 0.34527063969382177,
      "grad_norm": 2.4164464473724365,
      "learning_rate": 3.667945581537543e-05,
      "loss": 2.6938,
      "step": 3789
    },
    {
      "epoch": 0.34536176416985603,
      "grad_norm": 2.4057962894439697,
      "learning_rate": 3.667312746987447e-05,
      "loss": 3.0228,
      "step": 3790
    },
    {
      "epoch": 0.3454528886458903,
      "grad_norm": 2.5461204051971436,
      "learning_rate": 3.666679816771472e-05,
      "loss": 4.0145,
      "step": 3791
    },
    {
      "epoch": 0.34554401312192456,
      "grad_norm": 2.188991069793701,
      "learning_rate": 3.666046790941492e-05,
      "loss": 2.9539,
      "step": 3792
    },
    {
      "epoch": 0.34563513759795883,
      "grad_norm": 1.4209028482437134,
      "learning_rate": 3.6654136695493844e-05,
      "loss": 2.8552,
      "step": 3793
    },
    {
      "epoch": 0.3457262620739931,
      "grad_norm": 2.293485164642334,
      "learning_rate": 3.664780452647036e-05,
      "loss": 2.847,
      "step": 3794
    },
    {
      "epoch": 0.34581738655002736,
      "grad_norm": 2.201226234436035,
      "learning_rate": 3.6641471402863424e-05,
      "loss": 2.9879,
      "step": 3795
    },
    {
      "epoch": 0.3459085110260616,
      "grad_norm": 1.9210031032562256,
      "learning_rate": 3.6635137325192054e-05,
      "loss": 2.9397,
      "step": 3796
    },
    {
      "epoch": 0.34599963550209584,
      "grad_norm": 1.321592926979065,
      "learning_rate": 3.662880229397535e-05,
      "loss": 2.8732,
      "step": 3797
    },
    {
      "epoch": 0.3460907599781301,
      "grad_norm": 1.8968873023986816,
      "learning_rate": 3.6622466309732496e-05,
      "loss": 2.8581,
      "step": 3798
    },
    {
      "epoch": 0.34618188445416437,
      "grad_norm": 1.3391976356506348,
      "learning_rate": 3.661612937298275e-05,
      "loss": 2.8803,
      "step": 3799
    },
    {
      "epoch": 0.34627300893019863,
      "grad_norm": 2.992724657058716,
      "learning_rate": 3.660979148424545e-05,
      "loss": 2.9775,
      "step": 3800
    },
    {
      "epoch": 0.3463641334062329,
      "grad_norm": 2.2311832904815674,
      "learning_rate": 3.660345264404001e-05,
      "loss": 2.8163,
      "step": 3801
    },
    {
      "epoch": 0.34645525788226716,
      "grad_norm": 2.880251407623291,
      "learning_rate": 3.6597112852885926e-05,
      "loss": 2.8845,
      "step": 3802
    },
    {
      "epoch": 0.34654638235830143,
      "grad_norm": 1.9980400800704956,
      "learning_rate": 3.659077211130276e-05,
      "loss": 2.8837,
      "step": 3803
    },
    {
      "epoch": 0.3466375068343357,
      "grad_norm": 3.2880210876464844,
      "learning_rate": 3.658443041981016e-05,
      "loss": 3.0742,
      "step": 3804
    },
    {
      "epoch": 0.34672863131036996,
      "grad_norm": 2.6854491233825684,
      "learning_rate": 3.657808777892787e-05,
      "loss": 2.7018,
      "step": 3805
    },
    {
      "epoch": 0.3468197557864042,
      "grad_norm": 1.076456904411316,
      "learning_rate": 3.6571744189175676e-05,
      "loss": 2.7556,
      "step": 3806
    },
    {
      "epoch": 0.3469108802624385,
      "grad_norm": 3.2876317501068115,
      "learning_rate": 3.656539965107348e-05,
      "loss": 2.9066,
      "step": 3807
    },
    {
      "epoch": 0.34700200473847276,
      "grad_norm": 2.2565507888793945,
      "learning_rate": 3.6559054165141224e-05,
      "loss": 2.9399,
      "step": 3808
    },
    {
      "epoch": 0.347093129214507,
      "grad_norm": 1.7248157262802124,
      "learning_rate": 3.655270773189894e-05,
      "loss": 3.0327,
      "step": 3809
    },
    {
      "epoch": 0.3471842536905413,
      "grad_norm": 1.4134010076522827,
      "learning_rate": 3.654636035186677e-05,
      "loss": 2.9731,
      "step": 3810
    },
    {
      "epoch": 0.34727537816657555,
      "grad_norm": 1.9041470289230347,
      "learning_rate": 3.654001202556488e-05,
      "loss": 3.0766,
      "step": 3811
    },
    {
      "epoch": 0.3473665026426098,
      "grad_norm": 1.5501325130462646,
      "learning_rate": 3.653366275351356e-05,
      "loss": 2.848,
      "step": 3812
    },
    {
      "epoch": 0.3474576271186441,
      "grad_norm": 2.0999643802642822,
      "learning_rate": 3.652731253623315e-05,
      "loss": 2.6268,
      "step": 3813
    },
    {
      "epoch": 0.34754875159467835,
      "grad_norm": 1.2792668342590332,
      "learning_rate": 3.6520961374244064e-05,
      "loss": 2.9014,
      "step": 3814
    },
    {
      "epoch": 0.3476398760707126,
      "grad_norm": 2.8954861164093018,
      "learning_rate": 3.651460926806682e-05,
      "loss": 2.3593,
      "step": 3815
    },
    {
      "epoch": 0.3477310005467469,
      "grad_norm": 1.7956427335739136,
      "learning_rate": 3.6508256218222e-05,
      "loss": 3.1028,
      "step": 3816
    },
    {
      "epoch": 0.3478221250227811,
      "grad_norm": 1.19619619846344,
      "learning_rate": 3.650190222523026e-05,
      "loss": 2.9663,
      "step": 3817
    },
    {
      "epoch": 0.34791324949881536,
      "grad_norm": 1.2199691534042358,
      "learning_rate": 3.649554728961232e-05,
      "loss": 2.916,
      "step": 3818
    },
    {
      "epoch": 0.3480043739748496,
      "grad_norm": 1.9067312479019165,
      "learning_rate": 3.6489191411889e-05,
      "loss": 3.0703,
      "step": 3819
    },
    {
      "epoch": 0.3480954984508839,
      "grad_norm": 1.2753328084945679,
      "learning_rate": 3.6482834592581196e-05,
      "loss": 2.7866,
      "step": 3820
    },
    {
      "epoch": 0.34818662292691815,
      "grad_norm": 1.8216966390609741,
      "learning_rate": 3.647647683220987e-05,
      "loss": 2.7786,
      "step": 3821
    },
    {
      "epoch": 0.3482777474029524,
      "grad_norm": 2.4350638389587402,
      "learning_rate": 3.647011813129606e-05,
      "loss": 2.9817,
      "step": 3822
    },
    {
      "epoch": 0.3483688718789867,
      "grad_norm": 1.8148926496505737,
      "learning_rate": 3.646375849036089e-05,
      "loss": 2.6476,
      "step": 3823
    },
    {
      "epoch": 0.34845999635502095,
      "grad_norm": 1.2116892337799072,
      "learning_rate": 3.645739790992556e-05,
      "loss": 2.8599,
      "step": 3824
    },
    {
      "epoch": 0.3485511208310552,
      "grad_norm": 1.2008485794067383,
      "learning_rate": 3.645103639051135e-05,
      "loss": 2.8918,
      "step": 3825
    },
    {
      "epoch": 0.3486422453070895,
      "grad_norm": 1.8716193437576294,
      "learning_rate": 3.6444673932639606e-05,
      "loss": 2.8563,
      "step": 3826
    },
    {
      "epoch": 0.34873336978312375,
      "grad_norm": 1.8601107597351074,
      "learning_rate": 3.643831053683175e-05,
      "loss": 2.7813,
      "step": 3827
    },
    {
      "epoch": 0.348824494259158,
      "grad_norm": 2.022777795791626,
      "learning_rate": 3.643194620360929e-05,
      "loss": 2.7694,
      "step": 3828
    },
    {
      "epoch": 0.3489156187351923,
      "grad_norm": 4.16536283493042,
      "learning_rate": 3.642558093349382e-05,
      "loss": 3.048,
      "step": 3829
    },
    {
      "epoch": 0.34900674321122654,
      "grad_norm": 1.970546841621399,
      "learning_rate": 3.641921472700698e-05,
      "loss": 2.6798,
      "step": 3830
    },
    {
      "epoch": 0.3490978676872608,
      "grad_norm": 2.976733446121216,
      "learning_rate": 3.641284758467053e-05,
      "loss": 3.2391,
      "step": 3831
    },
    {
      "epoch": 0.3491889921632951,
      "grad_norm": 1.6848959922790527,
      "learning_rate": 3.6406479507006255e-05,
      "loss": 2.6043,
      "step": 3832
    },
    {
      "epoch": 0.34928011663932934,
      "grad_norm": 2.354260206222534,
      "learning_rate": 3.6400110494536064e-05,
      "loss": 2.5107,
      "step": 3833
    },
    {
      "epoch": 0.3493712411153636,
      "grad_norm": 2.0655651092529297,
      "learning_rate": 3.639374054778191e-05,
      "loss": 2.9835,
      "step": 3834
    },
    {
      "epoch": 0.34946236559139787,
      "grad_norm": 1.0317506790161133,
      "learning_rate": 3.638736966726585e-05,
      "loss": 2.8156,
      "step": 3835
    },
    {
      "epoch": 0.34955349006743214,
      "grad_norm": 2.9558255672454834,
      "learning_rate": 3.638099785350999e-05,
      "loss": 2.7105,
      "step": 3836
    },
    {
      "epoch": 0.34964461454346635,
      "grad_norm": 2.224186897277832,
      "learning_rate": 3.637462510703653e-05,
      "loss": 2.9083,
      "step": 3837
    },
    {
      "epoch": 0.3497357390195006,
      "grad_norm": 1.675590991973877,
      "learning_rate": 3.636825142836775e-05,
      "loss": 2.9128,
      "step": 3838
    },
    {
      "epoch": 0.3498268634955349,
      "grad_norm": 2.151735305786133,
      "learning_rate": 3.6361876818025975e-05,
      "loss": 2.9559,
      "step": 3839
    },
    {
      "epoch": 0.34991798797156914,
      "grad_norm": 1.2698489427566528,
      "learning_rate": 3.635550127653365e-05,
      "loss": 2.7785,
      "step": 3840
    },
    {
      "epoch": 0.3500091124476034,
      "grad_norm": 2.669459104537964,
      "learning_rate": 3.6349124804413264e-05,
      "loss": 3.0599,
      "step": 3841
    },
    {
      "epoch": 0.3501002369236377,
      "grad_norm": 1.1805731058120728,
      "learning_rate": 3.634274740218741e-05,
      "loss": 2.8263,
      "step": 3842
    },
    {
      "epoch": 0.35019136139967194,
      "grad_norm": 2.8979032039642334,
      "learning_rate": 3.6336369070378716e-05,
      "loss": 3.0349,
      "step": 3843
    },
    {
      "epoch": 0.3502824858757062,
      "grad_norm": 1.1485363245010376,
      "learning_rate": 3.632998980950993e-05,
      "loss": 2.7733,
      "step": 3844
    },
    {
      "epoch": 0.35037361035174047,
      "grad_norm": 3.1723287105560303,
      "learning_rate": 3.632360962010386e-05,
      "loss": 3.1681,
      "step": 3845
    },
    {
      "epoch": 0.35046473482777474,
      "grad_norm": 1.915490984916687,
      "learning_rate": 3.631722850268337e-05,
      "loss": 3.0303,
      "step": 3846
    },
    {
      "epoch": 0.350555859303809,
      "grad_norm": 1.3181086778640747,
      "learning_rate": 3.631084645777144e-05,
      "loss": 2.8473,
      "step": 3847
    },
    {
      "epoch": 0.35064698377984327,
      "grad_norm": 2.7099623680114746,
      "learning_rate": 3.630446348589109e-05,
      "loss": 2.9324,
      "step": 3848
    },
    {
      "epoch": 0.35073810825587753,
      "grad_norm": 1.2317121028900146,
      "learning_rate": 3.629807958756544e-05,
      "loss": 2.913,
      "step": 3849
    },
    {
      "epoch": 0.3508292327319118,
      "grad_norm": 2.2504913806915283,
      "learning_rate": 3.629169476331765e-05,
      "loss": 3.0155,
      "step": 3850
    },
    {
      "epoch": 0.35092035720794607,
      "grad_norm": 1.2953814268112183,
      "learning_rate": 3.628530901367101e-05,
      "loss": 2.8122,
      "step": 3851
    },
    {
      "epoch": 0.35101148168398033,
      "grad_norm": 1.3386671543121338,
      "learning_rate": 3.6278922339148844e-05,
      "loss": 2.7736,
      "step": 3852
    },
    {
      "epoch": 0.3511026061600146,
      "grad_norm": 2.7795135974884033,
      "learning_rate": 3.627253474027456e-05,
      "loss": 2.9188,
      "step": 3853
    },
    {
      "epoch": 0.35119373063604886,
      "grad_norm": 2.176166534423828,
      "learning_rate": 3.626614621757167e-05,
      "loss": 2.8587,
      "step": 3854
    },
    {
      "epoch": 0.3512848551120831,
      "grad_norm": 2.6318445205688477,
      "learning_rate": 3.6259756771563715e-05,
      "loss": 3.0628,
      "step": 3855
    },
    {
      "epoch": 0.3513759795881174,
      "grad_norm": 1.9014613628387451,
      "learning_rate": 3.625336640277434e-05,
      "loss": 3.1038,
      "step": 3856
    },
    {
      "epoch": 0.35146710406415166,
      "grad_norm": 2.0937674045562744,
      "learning_rate": 3.624697511172727e-05,
      "loss": 3.0356,
      "step": 3857
    },
    {
      "epoch": 0.35155822854018587,
      "grad_norm": 2.3514764308929443,
      "learning_rate": 3.624058289894629e-05,
      "loss": 2.7914,
      "step": 3858
    },
    {
      "epoch": 0.35164935301622013,
      "grad_norm": 2.3289406299591064,
      "learning_rate": 3.6234189764955274e-05,
      "loss": 3.2847,
      "step": 3859
    },
    {
      "epoch": 0.3517404774922544,
      "grad_norm": 1.815626621246338,
      "learning_rate": 3.622779571027815e-05,
      "loss": 3.0093,
      "step": 3860
    },
    {
      "epoch": 0.35183160196828867,
      "grad_norm": 2.2723069190979004,
      "learning_rate": 3.6221400735438943e-05,
      "loss": 2.9063,
      "step": 3861
    },
    {
      "epoch": 0.35192272644432293,
      "grad_norm": 1.8948240280151367,
      "learning_rate": 3.621500484096175e-05,
      "loss": 2.7407,
      "step": 3862
    },
    {
      "epoch": 0.3520138509203572,
      "grad_norm": 1.770409107208252,
      "learning_rate": 3.620860802737074e-05,
      "loss": 3.1882,
      "step": 3863
    },
    {
      "epoch": 0.35210497539639146,
      "grad_norm": 1.9358348846435547,
      "learning_rate": 3.620221029519015e-05,
      "loss": 3.0812,
      "step": 3864
    },
    {
      "epoch": 0.3521960998724257,
      "grad_norm": 2.4451181888580322,
      "learning_rate": 3.619581164494431e-05,
      "loss": 2.8547,
      "step": 3865
    },
    {
      "epoch": 0.35228722434846,
      "grad_norm": 2.7921526432037354,
      "learning_rate": 3.61894120771576e-05,
      "loss": 4.1927,
      "step": 3866
    },
    {
      "epoch": 0.35237834882449426,
      "grad_norm": 1.6107505559921265,
      "learning_rate": 3.6183011592354507e-05,
      "loss": 2.8432,
      "step": 3867
    },
    {
      "epoch": 0.3524694733005285,
      "grad_norm": 1.551740288734436,
      "learning_rate": 3.6176610191059566e-05,
      "loss": 2.5898,
      "step": 3868
    },
    {
      "epoch": 0.3525605977765628,
      "grad_norm": 1.2871124744415283,
      "learning_rate": 3.617020787379739e-05,
      "loss": 2.8868,
      "step": 3869
    },
    {
      "epoch": 0.35265172225259706,
      "grad_norm": 3.1349716186523438,
      "learning_rate": 3.6163804641092694e-05,
      "loss": 2.7647,
      "step": 3870
    },
    {
      "epoch": 0.3527428467286313,
      "grad_norm": 1.344675898551941,
      "learning_rate": 3.615740049347023e-05,
      "loss": 2.8394,
      "step": 3871
    },
    {
      "epoch": 0.3528339712046656,
      "grad_norm": 2.187875986099243,
      "learning_rate": 3.6150995431454846e-05,
      "loss": 3.3025,
      "step": 3872
    },
    {
      "epoch": 0.35292509568069985,
      "grad_norm": 2.0291526317596436,
      "learning_rate": 3.6144589455571476e-05,
      "loss": 3.0297,
      "step": 3873
    },
    {
      "epoch": 0.3530162201567341,
      "grad_norm": 2.03884220123291,
      "learning_rate": 3.61381825663451e-05,
      "loss": 2.7662,
      "step": 3874
    },
    {
      "epoch": 0.3531073446327684,
      "grad_norm": 3.056154489517212,
      "learning_rate": 3.613177476430079e-05,
      "loss": 4.2678,
      "step": 3875
    },
    {
      "epoch": 0.35319846910880265,
      "grad_norm": 1.9085500240325928,
      "learning_rate": 3.612536604996369e-05,
      "loss": 2.9349,
      "step": 3876
    },
    {
      "epoch": 0.3532895935848369,
      "grad_norm": 1.8348232507705688,
      "learning_rate": 3.6118956423859025e-05,
      "loss": 2.7181,
      "step": 3877
    },
    {
      "epoch": 0.3533807180608711,
      "grad_norm": 1.475743055343628,
      "learning_rate": 3.6112545886512096e-05,
      "loss": 2.7736,
      "step": 3878
    },
    {
      "epoch": 0.3534718425369054,
      "grad_norm": 1.344373106956482,
      "learning_rate": 3.6106134438448244e-05,
      "loss": 2.8952,
      "step": 3879
    },
    {
      "epoch": 0.35356296701293966,
      "grad_norm": 2.126816749572754,
      "learning_rate": 3.609972208019295e-05,
      "loss": 2.9517,
      "step": 3880
    },
    {
      "epoch": 0.3536540914889739,
      "grad_norm": 2.4195187091827393,
      "learning_rate": 3.60933088122717e-05,
      "loss": 2.6887,
      "step": 3881
    },
    {
      "epoch": 0.3537452159650082,
      "grad_norm": 1.8879560232162476,
      "learning_rate": 3.6086894635210104e-05,
      "loss": 2.983,
      "step": 3882
    },
    {
      "epoch": 0.35383634044104245,
      "grad_norm": 2.3171489238739014,
      "learning_rate": 3.6080479549533825e-05,
      "loss": 2.9234,
      "step": 3883
    },
    {
      "epoch": 0.3539274649170767,
      "grad_norm": 1.8685672283172607,
      "learning_rate": 3.60740635557686e-05,
      "loss": 2.9561,
      "step": 3884
    },
    {
      "epoch": 0.354018589393111,
      "grad_norm": 2.9131546020507812,
      "learning_rate": 3.606764665444025e-05,
      "loss": 2.4934,
      "step": 3885
    },
    {
      "epoch": 0.35410971386914525,
      "grad_norm": 1.8487013578414917,
      "learning_rate": 3.606122884607467e-05,
      "loss": 2.7111,
      "step": 3886
    },
    {
      "epoch": 0.3542008383451795,
      "grad_norm": 3.133497953414917,
      "learning_rate": 3.6054810131197814e-05,
      "loss": 2.9239,
      "step": 3887
    },
    {
      "epoch": 0.3542919628212138,
      "grad_norm": 1.950701117515564,
      "learning_rate": 3.604839051033573e-05,
      "loss": 2.9701,
      "step": 3888
    },
    {
      "epoch": 0.35438308729724805,
      "grad_norm": 1.8519337177276611,
      "learning_rate": 3.604196998401452e-05,
      "loss": 2.9794,
      "step": 3889
    },
    {
      "epoch": 0.3544742117732823,
      "grad_norm": 1.8768532276153564,
      "learning_rate": 3.603554855276038e-05,
      "loss": 2.7599,
      "step": 3890
    },
    {
      "epoch": 0.3545653362493166,
      "grad_norm": 2.038742780685425,
      "learning_rate": 3.6029126217099574e-05,
      "loss": 2.5097,
      "step": 3891
    },
    {
      "epoch": 0.35465646072535084,
      "grad_norm": 1.1209274530410767,
      "learning_rate": 3.602270297755843e-05,
      "loss": 2.7807,
      "step": 3892
    },
    {
      "epoch": 0.3547475852013851,
      "grad_norm": 2.064605712890625,
      "learning_rate": 3.601627883466336e-05,
      "loss": 2.7451,
      "step": 3893
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 2.346763849258423,
      "learning_rate": 3.600985378894086e-05,
      "loss": 3.0488,
      "step": 3894
    },
    {
      "epoch": 0.35492983415345364,
      "grad_norm": 1.1897599697113037,
      "learning_rate": 3.6003427840917466e-05,
      "loss": 2.8332,
      "step": 3895
    },
    {
      "epoch": 0.3550209586294879,
      "grad_norm": 2.458848476409912,
      "learning_rate": 3.5997000991119826e-05,
      "loss": 2.9268,
      "step": 3896
    },
    {
      "epoch": 0.35511208310552217,
      "grad_norm": 1.9555765390396118,
      "learning_rate": 3.599057324007464e-05,
      "loss": 2.9573,
      "step": 3897
    },
    {
      "epoch": 0.3552032075815564,
      "grad_norm": 1.1245551109313965,
      "learning_rate": 3.59841445883087e-05,
      "loss": 2.8712,
      "step": 3898
    },
    {
      "epoch": 0.35529433205759064,
      "grad_norm": 2.105640172958374,
      "learning_rate": 3.597771503634883e-05,
      "loss": 2.8881,
      "step": 3899
    },
    {
      "epoch": 0.3553854565336249,
      "grad_norm": 2.525937795639038,
      "learning_rate": 3.5971284584721984e-05,
      "loss": 2.9959,
      "step": 3900
    },
    {
      "epoch": 0.3554765810096592,
      "grad_norm": 1.7026175260543823,
      "learning_rate": 3.5964853233955145e-05,
      "loss": 2.75,
      "step": 3901
    },
    {
      "epoch": 0.35556770548569344,
      "grad_norm": 2.129108428955078,
      "learning_rate": 3.5958420984575396e-05,
      "loss": 3.0411,
      "step": 3902
    },
    {
      "epoch": 0.3556588299617277,
      "grad_norm": 2.5817039012908936,
      "learning_rate": 3.595198783710989e-05,
      "loss": 1.3274,
      "step": 3903
    },
    {
      "epoch": 0.355749954437762,
      "grad_norm": 1.4782212972640991,
      "learning_rate": 3.5945553792085854e-05,
      "loss": 2.9477,
      "step": 3904
    },
    {
      "epoch": 0.35584107891379624,
      "grad_norm": 2.4119555950164795,
      "learning_rate": 3.5939118850030555e-05,
      "loss": 4.2836,
      "step": 3905
    },
    {
      "epoch": 0.3559322033898305,
      "grad_norm": 2.5613865852355957,
      "learning_rate": 3.593268301147139e-05,
      "loss": 2.773,
      "step": 3906
    },
    {
      "epoch": 0.35602332786586477,
      "grad_norm": 1.1394490003585815,
      "learning_rate": 3.592624627693579e-05,
      "loss": 2.7679,
      "step": 3907
    },
    {
      "epoch": 0.35611445234189903,
      "grad_norm": 2.2571609020233154,
      "learning_rate": 3.591980864695127e-05,
      "loss": 4.049,
      "step": 3908
    },
    {
      "epoch": 0.3562055768179333,
      "grad_norm": 2.095533609390259,
      "learning_rate": 3.591337012204542e-05,
      "loss": 3.0892,
      "step": 3909
    },
    {
      "epoch": 0.35629670129396757,
      "grad_norm": 2.1383309364318848,
      "learning_rate": 3.590693070274591e-05,
      "loss": 2.8717,
      "step": 3910
    },
    {
      "epoch": 0.35638782577000183,
      "grad_norm": 2.1540374755859375,
      "learning_rate": 3.5900490389580463e-05,
      "loss": 2.9696,
      "step": 3911
    },
    {
      "epoch": 0.3564789502460361,
      "grad_norm": 1.3364108800888062,
      "learning_rate": 3.5894049183076894e-05,
      "loss": 2.7868,
      "step": 3912
    },
    {
      "epoch": 0.35657007472207036,
      "grad_norm": 1.741766333580017,
      "learning_rate": 3.5887607083763086e-05,
      "loss": 2.8542,
      "step": 3913
    },
    {
      "epoch": 0.35666119919810463,
      "grad_norm": 2.2316343784332275,
      "learning_rate": 3.5881164092166996e-05,
      "loss": 3.1463,
      "step": 3914
    },
    {
      "epoch": 0.3567523236741389,
      "grad_norm": 2.008216619491577,
      "learning_rate": 3.587472020881666e-05,
      "loss": 2.9188,
      "step": 3915
    },
    {
      "epoch": 0.35684344815017316,
      "grad_norm": 3.517347574234009,
      "learning_rate": 3.586827543424016e-05,
      "loss": 2.642,
      "step": 3916
    },
    {
      "epoch": 0.3569345726262074,
      "grad_norm": 1.0667271614074707,
      "learning_rate": 3.5861829768965683e-05,
      "loss": 2.699,
      "step": 3917
    },
    {
      "epoch": 0.35702569710224163,
      "grad_norm": 2.14027738571167,
      "learning_rate": 3.585538321352148e-05,
      "loss": 3.0209,
      "step": 3918
    },
    {
      "epoch": 0.3571168215782759,
      "grad_norm": 1.3108826875686646,
      "learning_rate": 3.584893576843586e-05,
      "loss": 2.8505,
      "step": 3919
    },
    {
      "epoch": 0.35720794605431017,
      "grad_norm": 2.512115955352783,
      "learning_rate": 3.584248743423722e-05,
      "loss": 2.8856,
      "step": 3920
    },
    {
      "epoch": 0.35729907053034443,
      "grad_norm": 2.499490976333618,
      "learning_rate": 3.5836038211454034e-05,
      "loss": 2.8906,
      "step": 3921
    },
    {
      "epoch": 0.3573901950063787,
      "grad_norm": 1.4825475215911865,
      "learning_rate": 3.582958810061484e-05,
      "loss": 2.8461,
      "step": 3922
    },
    {
      "epoch": 0.35748131948241296,
      "grad_norm": 1.9871701002120972,
      "learning_rate": 3.582313710224824e-05,
      "loss": 3.1353,
      "step": 3923
    },
    {
      "epoch": 0.35757244395844723,
      "grad_norm": 2.1141164302825928,
      "learning_rate": 3.5816685216882926e-05,
      "loss": 2.6944,
      "step": 3924
    },
    {
      "epoch": 0.3576635684344815,
      "grad_norm": 1.7908185720443726,
      "learning_rate": 3.581023244504765e-05,
      "loss": 2.8794,
      "step": 3925
    },
    {
      "epoch": 0.35775469291051576,
      "grad_norm": 3.361798048019409,
      "learning_rate": 3.580377878727126e-05,
      "loss": 2.655,
      "step": 3926
    },
    {
      "epoch": 0.35784581738655,
      "grad_norm": 2.451209783554077,
      "learning_rate": 3.579732424408263e-05,
      "loss": 2.6731,
      "step": 3927
    },
    {
      "epoch": 0.3579369418625843,
      "grad_norm": 2.1984715461730957,
      "learning_rate": 3.579086881601076e-05,
      "loss": 2.8526,
      "step": 3928
    },
    {
      "epoch": 0.35802806633861856,
      "grad_norm": 2.1241250038146973,
      "learning_rate": 3.578441250358469e-05,
      "loss": 3.0359,
      "step": 3929
    },
    {
      "epoch": 0.3581191908146528,
      "grad_norm": 1.6053024530410767,
      "learning_rate": 3.577795530733353e-05,
      "loss": 2.9609,
      "step": 3930
    },
    {
      "epoch": 0.3582103152906871,
      "grad_norm": 1.9642552137374878,
      "learning_rate": 3.5771497227786485e-05,
      "loss": 2.9483,
      "step": 3931
    },
    {
      "epoch": 0.35830143976672135,
      "grad_norm": 1.4832853078842163,
      "learning_rate": 3.5765038265472815e-05,
      "loss": 2.9368,
      "step": 3932
    },
    {
      "epoch": 0.3583925642427556,
      "grad_norm": 2.202075719833374,
      "learning_rate": 3.575857842092186e-05,
      "loss": 3.0551,
      "step": 3933
    },
    {
      "epoch": 0.3584836887187899,
      "grad_norm": 2.516075372695923,
      "learning_rate": 3.5752117694663036e-05,
      "loss": 2.6607,
      "step": 3934
    },
    {
      "epoch": 0.35857481319482415,
      "grad_norm": 1.3236451148986816,
      "learning_rate": 3.574565608722581e-05,
      "loss": 3.0076,
      "step": 3935
    },
    {
      "epoch": 0.3586659376708584,
      "grad_norm": 2.1664178371429443,
      "learning_rate": 3.573919359913974e-05,
      "loss": 2.8718,
      "step": 3936
    },
    {
      "epoch": 0.3587570621468927,
      "grad_norm": 1.2623785734176636,
      "learning_rate": 3.5732730230934466e-05,
      "loss": 2.8297,
      "step": 3937
    },
    {
      "epoch": 0.35884818662292695,
      "grad_norm": 1.0817207098007202,
      "learning_rate": 3.5726265983139664e-05,
      "loss": 2.784,
      "step": 3938
    },
    {
      "epoch": 0.35893931109896116,
      "grad_norm": 1.7634341716766357,
      "learning_rate": 3.5719800856285125e-05,
      "loss": 2.7179,
      "step": 3939
    },
    {
      "epoch": 0.3590304355749954,
      "grad_norm": 3.3187787532806396,
      "learning_rate": 3.5713334850900685e-05,
      "loss": 2.9153,
      "step": 3940
    },
    {
      "epoch": 0.3591215600510297,
      "grad_norm": 1.3507903814315796,
      "learning_rate": 3.5706867967516254e-05,
      "loss": 2.84,
      "step": 3941
    },
    {
      "epoch": 0.35921268452706395,
      "grad_norm": 1.897520661354065,
      "learning_rate": 3.5700400206661824e-05,
      "loss": 2.6995,
      "step": 3942
    },
    {
      "epoch": 0.3593038090030982,
      "grad_norm": 2.5816197395324707,
      "learning_rate": 3.569393156886745e-05,
      "loss": 3.3279,
      "step": 3943
    },
    {
      "epoch": 0.3593949334791325,
      "grad_norm": 1.518595576286316,
      "learning_rate": 3.5687462054663254e-05,
      "loss": 2.9317,
      "step": 3944
    },
    {
      "epoch": 0.35948605795516675,
      "grad_norm": 2.560371160507202,
      "learning_rate": 3.5680991664579457e-05,
      "loss": 2.6313,
      "step": 3945
    },
    {
      "epoch": 0.359577182431201,
      "grad_norm": 2.088453531265259,
      "learning_rate": 3.567452039914632e-05,
      "loss": 3.0017,
      "step": 3946
    },
    {
      "epoch": 0.3596683069072353,
      "grad_norm": 2.3888070583343506,
      "learning_rate": 3.566804825889419e-05,
      "loss": 2.8355,
      "step": 3947
    },
    {
      "epoch": 0.35975943138326955,
      "grad_norm": 1.832493543624878,
      "learning_rate": 3.56615752443535e-05,
      "loss": 2.8189,
      "step": 3948
    },
    {
      "epoch": 0.3598505558593038,
      "grad_norm": 2.6397695541381836,
      "learning_rate": 3.565510135605471e-05,
      "loss": 2.6874,
      "step": 3949
    },
    {
      "epoch": 0.3599416803353381,
      "grad_norm": 1.8270232677459717,
      "learning_rate": 3.56486265945284e-05,
      "loss": 2.7982,
      "step": 3950
    },
    {
      "epoch": 0.36003280481137234,
      "grad_norm": 2.2680673599243164,
      "learning_rate": 3.5642150960305205e-05,
      "loss": 3.2147,
      "step": 3951
    },
    {
      "epoch": 0.3601239292874066,
      "grad_norm": 1.7601685523986816,
      "learning_rate": 3.5635674453915826e-05,
      "loss": 2.9083,
      "step": 3952
    },
    {
      "epoch": 0.3602150537634409,
      "grad_norm": 2.2197868824005127,
      "learning_rate": 3.562919707589102e-05,
      "loss": 2.7018,
      "step": 3953
    },
    {
      "epoch": 0.36030617823947514,
      "grad_norm": 2.0692265033721924,
      "learning_rate": 3.5622718826761656e-05,
      "loss": 2.9213,
      "step": 3954
    },
    {
      "epoch": 0.3603973027155094,
      "grad_norm": 1.7834818363189697,
      "learning_rate": 3.561623970705864e-05,
      "loss": 3.0289,
      "step": 3955
    },
    {
      "epoch": 0.36048842719154367,
      "grad_norm": 2.7679619789123535,
      "learning_rate": 3.560975971731297e-05,
      "loss": 2.6009,
      "step": 3956
    },
    {
      "epoch": 0.36057955166757794,
      "grad_norm": 1.1463043689727783,
      "learning_rate": 3.56032788580557e-05,
      "loss": 2.7361,
      "step": 3957
    },
    {
      "epoch": 0.3606706761436122,
      "grad_norm": 1.2137130498886108,
      "learning_rate": 3.559679712981797e-05,
      "loss": 2.7895,
      "step": 3958
    },
    {
      "epoch": 0.3607618006196464,
      "grad_norm": 3.3498077392578125,
      "learning_rate": 3.5590314533130976e-05,
      "loss": 3.1713,
      "step": 3959
    },
    {
      "epoch": 0.3608529250956807,
      "grad_norm": 1.9445748329162598,
      "learning_rate": 3.558383106852599e-05,
      "loss": 2.9748,
      "step": 3960
    },
    {
      "epoch": 0.36094404957171494,
      "grad_norm": 1.3004463911056519,
      "learning_rate": 3.5577346736534363e-05,
      "loss": 2.7943,
      "step": 3961
    },
    {
      "epoch": 0.3610351740477492,
      "grad_norm": 2.646358013153076,
      "learning_rate": 3.5570861537687504e-05,
      "loss": 4.1946,
      "step": 3962
    },
    {
      "epoch": 0.3611262985237835,
      "grad_norm": 1.8542988300323486,
      "learning_rate": 3.556437547251691e-05,
      "loss": 2.8508,
      "step": 3963
    },
    {
      "epoch": 0.36121742299981774,
      "grad_norm": 2.2286038398742676,
      "learning_rate": 3.5557888541554134e-05,
      "loss": 3.1898,
      "step": 3964
    },
    {
      "epoch": 0.361308547475852,
      "grad_norm": 2.0307719707489014,
      "learning_rate": 3.555140074533081e-05,
      "loss": 2.7496,
      "step": 3965
    },
    {
      "epoch": 0.36139967195188627,
      "grad_norm": 1.9570577144622803,
      "learning_rate": 3.554491208437864e-05,
      "loss": 2.7106,
      "step": 3966
    },
    {
      "epoch": 0.36149079642792054,
      "grad_norm": 2.1777496337890625,
      "learning_rate": 3.5538422559229385e-05,
      "loss": 2.836,
      "step": 3967
    },
    {
      "epoch": 0.3615819209039548,
      "grad_norm": 2.136327028274536,
      "learning_rate": 3.5531932170414896e-05,
      "loss": 2.8436,
      "step": 3968
    },
    {
      "epoch": 0.36167304537998907,
      "grad_norm": 1.2629234790802002,
      "learning_rate": 3.552544091846708e-05,
      "loss": 2.8648,
      "step": 3969
    },
    {
      "epoch": 0.36176416985602333,
      "grad_norm": 1.6470919847488403,
      "learning_rate": 3.551894880391793e-05,
      "loss": 2.8708,
      "step": 3970
    },
    {
      "epoch": 0.3618552943320576,
      "grad_norm": 2.0512735843658447,
      "learning_rate": 3.551245582729949e-05,
      "loss": 3.0524,
      "step": 3971
    },
    {
      "epoch": 0.36194641880809186,
      "grad_norm": 2.4797143936157227,
      "learning_rate": 3.550596198914389e-05,
      "loss": 2.8003,
      "step": 3972
    },
    {
      "epoch": 0.36203754328412613,
      "grad_norm": 1.5980339050292969,
      "learning_rate": 3.5499467289983326e-05,
      "loss": 3.0296,
      "step": 3973
    },
    {
      "epoch": 0.3621286677601604,
      "grad_norm": 2.2540714740753174,
      "learning_rate": 3.5492971730350054e-05,
      "loss": 4.2951,
      "step": 3974
    },
    {
      "epoch": 0.36221979223619466,
      "grad_norm": 2.0385961532592773,
      "learning_rate": 3.548647531077643e-05,
      "loss": 3.0279,
      "step": 3975
    },
    {
      "epoch": 0.3623109167122289,
      "grad_norm": 3.2300469875335693,
      "learning_rate": 3.547997803179485e-05,
      "loss": 2.8846,
      "step": 3976
    },
    {
      "epoch": 0.3624020411882632,
      "grad_norm": 1.400357961654663,
      "learning_rate": 3.547347989393779e-05,
      "loss": 2.8643,
      "step": 3977
    },
    {
      "epoch": 0.36249316566429746,
      "grad_norm": 2.1787259578704834,
      "learning_rate": 3.54669808977378e-05,
      "loss": 2.9825,
      "step": 3978
    },
    {
      "epoch": 0.36258429014033167,
      "grad_norm": 2.3056082725524902,
      "learning_rate": 3.5460481043727505e-05,
      "loss": 2.9454,
      "step": 3979
    },
    {
      "epoch": 0.36267541461636593,
      "grad_norm": 2.539608955383301,
      "learning_rate": 3.545398033243958e-05,
      "loss": 4.0545,
      "step": 3980
    },
    {
      "epoch": 0.3627665390924002,
      "grad_norm": 2.0951218605041504,
      "learning_rate": 3.5447478764406796e-05,
      "loss": 2.9488,
      "step": 3981
    },
    {
      "epoch": 0.36285766356843446,
      "grad_norm": 1.9108444452285767,
      "learning_rate": 3.5440976340161977e-05,
      "loss": 3.0369,
      "step": 3982
    },
    {
      "epoch": 0.36294878804446873,
      "grad_norm": 2.3561267852783203,
      "learning_rate": 3.543447306023802e-05,
      "loss": 2.6652,
      "step": 3983
    },
    {
      "epoch": 0.363039912520503,
      "grad_norm": 1.9491862058639526,
      "learning_rate": 3.54279689251679e-05,
      "loss": 2.9413,
      "step": 3984
    },
    {
      "epoch": 0.36313103699653726,
      "grad_norm": 1.8359100818634033,
      "learning_rate": 3.542146393548465e-05,
      "loss": 2.9326,
      "step": 3985
    },
    {
      "epoch": 0.3632221614725715,
      "grad_norm": 1.7808035612106323,
      "learning_rate": 3.541495809172139e-05,
      "loss": 3.2458,
      "step": 3986
    },
    {
      "epoch": 0.3633132859486058,
      "grad_norm": 3.141991138458252,
      "learning_rate": 3.540845139441129e-05,
      "loss": 2.8932,
      "step": 3987
    },
    {
      "epoch": 0.36340441042464006,
      "grad_norm": 1.4199646711349487,
      "learning_rate": 3.540194384408759e-05,
      "loss": 2.8592,
      "step": 3988
    },
    {
      "epoch": 0.3634955349006743,
      "grad_norm": 1.8217799663543701,
      "learning_rate": 3.539543544128364e-05,
      "loss": 2.8592,
      "step": 3989
    },
    {
      "epoch": 0.3635866593767086,
      "grad_norm": 1.3853861093521118,
      "learning_rate": 3.53889261865328e-05,
      "loss": 2.9349,
      "step": 3990
    },
    {
      "epoch": 0.36367778385274285,
      "grad_norm": 2.5066919326782227,
      "learning_rate": 3.538241608036854e-05,
      "loss": 2.9438,
      "step": 3991
    },
    {
      "epoch": 0.3637689083287771,
      "grad_norm": 1.313435673713684,
      "learning_rate": 3.5375905123324394e-05,
      "loss": 2.8891,
      "step": 3992
    },
    {
      "epoch": 0.3638600328048114,
      "grad_norm": 1.5595859289169312,
      "learning_rate": 3.536939331593395e-05,
      "loss": 2.6569,
      "step": 3993
    },
    {
      "epoch": 0.36395115728084565,
      "grad_norm": 1.233115553855896,
      "learning_rate": 3.536288065873089e-05,
      "loss": 2.8128,
      "step": 3994
    },
    {
      "epoch": 0.3640422817568799,
      "grad_norm": 2.040397882461548,
      "learning_rate": 3.5356367152248936e-05,
      "loss": 3.0897,
      "step": 3995
    },
    {
      "epoch": 0.3641334062329142,
      "grad_norm": 2.1081857681274414,
      "learning_rate": 3.534985279702191e-05,
      "loss": 3.1162,
      "step": 3996
    },
    {
      "epoch": 0.36422453070894845,
      "grad_norm": 1.9988462924957275,
      "learning_rate": 3.534333759358367e-05,
      "loss": 3.1878,
      "step": 3997
    },
    {
      "epoch": 0.3643156551849827,
      "grad_norm": 1.8169348239898682,
      "learning_rate": 3.533682154246818e-05,
      "loss": 2.707,
      "step": 3998
    },
    {
      "epoch": 0.3644067796610169,
      "grad_norm": 3.2198081016540527,
      "learning_rate": 3.533030464420946e-05,
      "loss": 3.086,
      "step": 3999
    },
    {
      "epoch": 0.3644979041370512,
      "grad_norm": 2.970412015914917,
      "learning_rate": 3.532378689934158e-05,
      "loss": 2.6541,
      "step": 4000
    },
    {
      "epoch": 0.36458902861308545,
      "grad_norm": 1.314903974533081,
      "learning_rate": 3.53172683083987e-05,
      "loss": 2.7542,
      "step": 4001
    },
    {
      "epoch": 0.3646801530891197,
      "grad_norm": 1.2221870422363281,
      "learning_rate": 3.531074887191505e-05,
      "loss": 2.8538,
      "step": 4002
    },
    {
      "epoch": 0.364771277565154,
      "grad_norm": 1.0248119831085205,
      "learning_rate": 3.53042285904249e-05,
      "loss": 2.7602,
      "step": 4003
    },
    {
      "epoch": 0.36486240204118825,
      "grad_norm": 2.396329879760742,
      "learning_rate": 3.529770746446265e-05,
      "loss": 2.8552,
      "step": 4004
    },
    {
      "epoch": 0.3649535265172225,
      "grad_norm": 3.1615164279937744,
      "learning_rate": 3.529118549456271e-05,
      "loss": 2.899,
      "step": 4005
    },
    {
      "epoch": 0.3650446509932568,
      "grad_norm": 2.654784917831421,
      "learning_rate": 3.528466268125958e-05,
      "loss": 2.6357,
      "step": 4006
    },
    {
      "epoch": 0.36513577546929105,
      "grad_norm": 1.8390332460403442,
      "learning_rate": 3.527813902508784e-05,
      "loss": 2.8981,
      "step": 4007
    },
    {
      "epoch": 0.3652268999453253,
      "grad_norm": 2.498948097229004,
      "learning_rate": 3.527161452658212e-05,
      "loss": 3.0534,
      "step": 4008
    },
    {
      "epoch": 0.3653180244213596,
      "grad_norm": 1.1839433908462524,
      "learning_rate": 3.526508918627713e-05,
      "loss": 2.788,
      "step": 4009
    },
    {
      "epoch": 0.36540914889739384,
      "grad_norm": 1.773061990737915,
      "learning_rate": 3.5258563004707654e-05,
      "loss": 3.0711,
      "step": 4010
    },
    {
      "epoch": 0.3655002733734281,
      "grad_norm": 2.1521997451782227,
      "learning_rate": 3.525203598240854e-05,
      "loss": 3.1547,
      "step": 4011
    },
    {
      "epoch": 0.3655913978494624,
      "grad_norm": 2.022233009338379,
      "learning_rate": 3.5245508119914687e-05,
      "loss": 2.9852,
      "step": 4012
    },
    {
      "epoch": 0.36568252232549664,
      "grad_norm": 1.2834147214889526,
      "learning_rate": 3.52389794177611e-05,
      "loss": 2.8851,
      "step": 4013
    },
    {
      "epoch": 0.3657736468015309,
      "grad_norm": 1.0654261112213135,
      "learning_rate": 3.523244987648281e-05,
      "loss": 2.7701,
      "step": 4014
    },
    {
      "epoch": 0.36586477127756517,
      "grad_norm": 1.2237108945846558,
      "learning_rate": 3.522591949661496e-05,
      "loss": 2.8871,
      "step": 4015
    },
    {
      "epoch": 0.36595589575359944,
      "grad_norm": 1.249617576599121,
      "learning_rate": 3.521938827869272e-05,
      "loss": 2.787,
      "step": 4016
    },
    {
      "epoch": 0.3660470202296337,
      "grad_norm": 2.047687292098999,
      "learning_rate": 3.5212856223251365e-05,
      "loss": 3.0996,
      "step": 4017
    },
    {
      "epoch": 0.36613814470566797,
      "grad_norm": 1.830929160118103,
      "learning_rate": 3.520632333082622e-05,
      "loss": 3.0087,
      "step": 4018
    },
    {
      "epoch": 0.36622926918170223,
      "grad_norm": 1.4000260829925537,
      "learning_rate": 3.519978960195267e-05,
      "loss": 2.8471,
      "step": 4019
    },
    {
      "epoch": 0.36632039365773644,
      "grad_norm": 2.1108145713806152,
      "learning_rate": 3.5193255037166204e-05,
      "loss": 2.9612,
      "step": 4020
    },
    {
      "epoch": 0.3664115181337707,
      "grad_norm": 1.9866982698440552,
      "learning_rate": 3.518671963700232e-05,
      "loss": 2.9079,
      "step": 4021
    },
    {
      "epoch": 0.366502642609805,
      "grad_norm": 1.9427390098571777,
      "learning_rate": 3.5180183401996645e-05,
      "loss": 2.8227,
      "step": 4022
    },
    {
      "epoch": 0.36659376708583924,
      "grad_norm": 2.211303949356079,
      "learning_rate": 3.5173646332684845e-05,
      "loss": 3.0705,
      "step": 4023
    },
    {
      "epoch": 0.3666848915618735,
      "grad_norm": 1.9407202005386353,
      "learning_rate": 3.5167108429602655e-05,
      "loss": 3.0343,
      "step": 4024
    },
    {
      "epoch": 0.36677601603790777,
      "grad_norm": 2.1224513053894043,
      "learning_rate": 3.516056969328589e-05,
      "loss": 2.6539,
      "step": 4025
    },
    {
      "epoch": 0.36686714051394204,
      "grad_norm": 1.1464595794677734,
      "learning_rate": 3.515403012427042e-05,
      "loss": 2.8628,
      "step": 4026
    },
    {
      "epoch": 0.3669582649899763,
      "grad_norm": 1.1502721309661865,
      "learning_rate": 3.514748972309218e-05,
      "loss": 2.8446,
      "step": 4027
    },
    {
      "epoch": 0.36704938946601057,
      "grad_norm": 2.269054889678955,
      "learning_rate": 3.5140948490287185e-05,
      "loss": 2.3973,
      "step": 4028
    },
    {
      "epoch": 0.36714051394204483,
      "grad_norm": 1.7927583456039429,
      "learning_rate": 3.513440642639152e-05,
      "loss": 2.9178,
      "step": 4029
    },
    {
      "epoch": 0.3672316384180791,
      "grad_norm": 2.767011880874634,
      "learning_rate": 3.512786353194134e-05,
      "loss": 2.6334,
      "step": 4030
    },
    {
      "epoch": 0.36732276289411336,
      "grad_norm": 2.0807368755340576,
      "learning_rate": 3.5121319807472853e-05,
      "loss": 2.8802,
      "step": 4031
    },
    {
      "epoch": 0.36741388737014763,
      "grad_norm": 2.7715446949005127,
      "learning_rate": 3.511477525352234e-05,
      "loss": 2.7732,
      "step": 4032
    },
    {
      "epoch": 0.3675050118461819,
      "grad_norm": 1.7267597913742065,
      "learning_rate": 3.5108229870626145e-05,
      "loss": 2.7715,
      "step": 4033
    },
    {
      "epoch": 0.36759613632221616,
      "grad_norm": 1.8751791715621948,
      "learning_rate": 3.510168365932071e-05,
      "loss": 2.7042,
      "step": 4034
    },
    {
      "epoch": 0.3676872607982504,
      "grad_norm": 1.795358657836914,
      "learning_rate": 3.509513662014251e-05,
      "loss": 3.018,
      "step": 4035
    },
    {
      "epoch": 0.3677783852742847,
      "grad_norm": 2.1549646854400635,
      "learning_rate": 3.50885887536281e-05,
      "loss": 3.1558,
      "step": 4036
    },
    {
      "epoch": 0.36786950975031896,
      "grad_norm": 5.208871841430664,
      "learning_rate": 3.508204006031411e-05,
      "loss": 3.0588,
      "step": 4037
    },
    {
      "epoch": 0.3679606342263532,
      "grad_norm": 1.1273174285888672,
      "learning_rate": 3.507549054073722e-05,
      "loss": 2.7974,
      "step": 4038
    },
    {
      "epoch": 0.3680517587023875,
      "grad_norm": 2.106046676635742,
      "learning_rate": 3.50689401954342e-05,
      "loss": 2.8909,
      "step": 4039
    },
    {
      "epoch": 0.3681428831784217,
      "grad_norm": 1.4817744493484497,
      "learning_rate": 3.5062389024941876e-05,
      "loss": 2.8608,
      "step": 4040
    },
    {
      "epoch": 0.36823400765445596,
      "grad_norm": 1.7251712083816528,
      "learning_rate": 3.5055837029797134e-05,
      "loss": 2.6718,
      "step": 4041
    },
    {
      "epoch": 0.36832513213049023,
      "grad_norm": 2.185094118118286,
      "learning_rate": 3.504928421053694e-05,
      "loss": 2.9731,
      "step": 4042
    },
    {
      "epoch": 0.3684162566065245,
      "grad_norm": 1.9892330169677734,
      "learning_rate": 3.5042730567698325e-05,
      "loss": 2.7347,
      "step": 4043
    },
    {
      "epoch": 0.36850738108255876,
      "grad_norm": 1.966184139251709,
      "learning_rate": 3.503617610181838e-05,
      "loss": 2.8545,
      "step": 4044
    },
    {
      "epoch": 0.368598505558593,
      "grad_norm": 2.0882437229156494,
      "learning_rate": 3.5029620813434276e-05,
      "loss": 2.9748,
      "step": 4045
    },
    {
      "epoch": 0.3686896300346273,
      "grad_norm": 1.992665410041809,
      "learning_rate": 3.5023064703083244e-05,
      "loss": 2.6228,
      "step": 4046
    },
    {
      "epoch": 0.36878075451066156,
      "grad_norm": 1.2144496440887451,
      "learning_rate": 3.5016507771302576e-05,
      "loss": 2.7661,
      "step": 4047
    },
    {
      "epoch": 0.3688718789866958,
      "grad_norm": 1.6925848722457886,
      "learning_rate": 3.500995001862965e-05,
      "loss": 2.9524,
      "step": 4048
    },
    {
      "epoch": 0.3689630034627301,
      "grad_norm": 2.659648895263672,
      "learning_rate": 3.5003391445601895e-05,
      "loss": 2.7323,
      "step": 4049
    },
    {
      "epoch": 0.36905412793876435,
      "grad_norm": 3.1457362174987793,
      "learning_rate": 3.499683205275681e-05,
      "loss": 4.4932,
      "step": 4050
    },
    {
      "epoch": 0.3691452524147986,
      "grad_norm": 2.8206567764282227,
      "learning_rate": 3.4990271840631957e-05,
      "loss": 2.684,
      "step": 4051
    },
    {
      "epoch": 0.3692363768908329,
      "grad_norm": 2.766204357147217,
      "learning_rate": 3.498371080976498e-05,
      "loss": 4.4379,
      "step": 4052
    },
    {
      "epoch": 0.36932750136686715,
      "grad_norm": 2.65632700920105,
      "learning_rate": 3.497714896069358e-05,
      "loss": 2.6426,
      "step": 4053
    },
    {
      "epoch": 0.3694186258429014,
      "grad_norm": 2.2508997917175293,
      "learning_rate": 3.497058629395552e-05,
      "loss": 2.8736,
      "step": 4054
    },
    {
      "epoch": 0.3695097503189357,
      "grad_norm": 2.3049795627593994,
      "learning_rate": 3.4964022810088656e-05,
      "loss": 2.6536,
      "step": 4055
    },
    {
      "epoch": 0.36960087479496995,
      "grad_norm": 1.7051366567611694,
      "learning_rate": 3.495745850963087e-05,
      "loss": 2.786,
      "step": 4056
    },
    {
      "epoch": 0.3696919992710042,
      "grad_norm": 2.069533109664917,
      "learning_rate": 3.495089339312013e-05,
      "loss": 3.9152,
      "step": 4057
    },
    {
      "epoch": 0.3697831237470385,
      "grad_norm": 2.0576367378234863,
      "learning_rate": 3.49443274610945e-05,
      "loss": 2.6739,
      "step": 4058
    },
    {
      "epoch": 0.36987424822307274,
      "grad_norm": 1.1116952896118164,
      "learning_rate": 3.493776071409205e-05,
      "loss": 2.7295,
      "step": 4059
    },
    {
      "epoch": 0.36996537269910695,
      "grad_norm": 1.6721360683441162,
      "learning_rate": 3.493119315265098e-05,
      "loss": 2.8957,
      "step": 4060
    },
    {
      "epoch": 0.3700564971751412,
      "grad_norm": 2.4993205070495605,
      "learning_rate": 3.4924624777309505e-05,
      "loss": 2.8824,
      "step": 4061
    },
    {
      "epoch": 0.3701476216511755,
      "grad_norm": 2.042153835296631,
      "learning_rate": 3.491805558860595e-05,
      "loss": 3.0502,
      "step": 4062
    },
    {
      "epoch": 0.37023874612720975,
      "grad_norm": 2.4793429374694824,
      "learning_rate": 3.491148558707867e-05,
      "loss": 2.8291,
      "step": 4063
    },
    {
      "epoch": 0.370329870603244,
      "grad_norm": 1.7722716331481934,
      "learning_rate": 3.4904914773266105e-05,
      "loss": 2.8647,
      "step": 4064
    },
    {
      "epoch": 0.3704209950792783,
      "grad_norm": 1.731912612915039,
      "learning_rate": 3.489834314770676e-05,
      "loss": 2.8444,
      "step": 4065
    },
    {
      "epoch": 0.37051211955531255,
      "grad_norm": 3.5742599964141846,
      "learning_rate": 3.489177071093921e-05,
      "loss": 2.4146,
      "step": 4066
    },
    {
      "epoch": 0.3706032440313468,
      "grad_norm": 1.1243046522140503,
      "learning_rate": 3.4885197463502095e-05,
      "loss": 2.9614,
      "step": 4067
    },
    {
      "epoch": 0.3706943685073811,
      "grad_norm": 1.3671035766601562,
      "learning_rate": 3.487862340593411e-05,
      "loss": 2.8685,
      "step": 4068
    },
    {
      "epoch": 0.37078549298341534,
      "grad_norm": 1.5952450037002563,
      "learning_rate": 3.4872048538774014e-05,
      "loss": 2.7792,
      "step": 4069
    },
    {
      "epoch": 0.3708766174594496,
      "grad_norm": 1.040452003479004,
      "learning_rate": 3.486547286256067e-05,
      "loss": 2.7843,
      "step": 4070
    },
    {
      "epoch": 0.3709677419354839,
      "grad_norm": 1.8967554569244385,
      "learning_rate": 3.4858896377832966e-05,
      "loss": 2.9094,
      "step": 4071
    },
    {
      "epoch": 0.37105886641151814,
      "grad_norm": 1.7596524953842163,
      "learning_rate": 3.485231908512987e-05,
      "loss": 2.823,
      "step": 4072
    },
    {
      "epoch": 0.3711499908875524,
      "grad_norm": 2.002730369567871,
      "learning_rate": 3.484574098499043e-05,
      "loss": 3.0072,
      "step": 4073
    },
    {
      "epoch": 0.37124111536358667,
      "grad_norm": 2.4404754638671875,
      "learning_rate": 3.483916207795372e-05,
      "loss": 4.0314,
      "step": 4074
    },
    {
      "epoch": 0.37133223983962094,
      "grad_norm": 2.499943494796753,
      "learning_rate": 3.483258236455893e-05,
      "loss": 2.8232,
      "step": 4075
    },
    {
      "epoch": 0.3714233643156552,
      "grad_norm": 2.7155990600585938,
      "learning_rate": 3.482600184534529e-05,
      "loss": 3.0242,
      "step": 4076
    },
    {
      "epoch": 0.37151448879168947,
      "grad_norm": 2.496497631072998,
      "learning_rate": 3.4819420520852095e-05,
      "loss": 2.5975,
      "step": 4077
    },
    {
      "epoch": 0.37160561326772373,
      "grad_norm": 1.6446696519851685,
      "learning_rate": 3.4812838391618714e-05,
      "loss": 2.7163,
      "step": 4078
    },
    {
      "epoch": 0.371696737743758,
      "grad_norm": 2.095543146133423,
      "learning_rate": 3.480625545818458e-05,
      "loss": 2.813,
      "step": 4079
    },
    {
      "epoch": 0.3717878622197922,
      "grad_norm": 1.2769831418991089,
      "learning_rate": 3.479967172108919e-05,
      "loss": 2.8737,
      "step": 4080
    },
    {
      "epoch": 0.3718789866958265,
      "grad_norm": 1.8378889560699463,
      "learning_rate": 3.479308718087209e-05,
      "loss": 2.9456,
      "step": 4081
    },
    {
      "epoch": 0.37197011117186074,
      "grad_norm": 2.2929582595825195,
      "learning_rate": 3.478650183807293e-05,
      "loss": 2.9254,
      "step": 4082
    },
    {
      "epoch": 0.372061235647895,
      "grad_norm": 1.2210321426391602,
      "learning_rate": 3.4779915693231405e-05,
      "loss": 2.9573,
      "step": 4083
    },
    {
      "epoch": 0.37215236012392927,
      "grad_norm": 1.8166511058807373,
      "learning_rate": 3.477332874688726e-05,
      "loss": 2.6635,
      "step": 4084
    },
    {
      "epoch": 0.37224348459996354,
      "grad_norm": 2.147169351577759,
      "learning_rate": 3.4766740999580325e-05,
      "loss": 2.834,
      "step": 4085
    },
    {
      "epoch": 0.3723346090759978,
      "grad_norm": 1.827354073524475,
      "learning_rate": 3.476015245185051e-05,
      "loss": 2.78,
      "step": 4086
    },
    {
      "epoch": 0.37242573355203207,
      "grad_norm": 2.5748417377471924,
      "learning_rate": 3.475356310423775e-05,
      "loss": 3.109,
      "step": 4087
    },
    {
      "epoch": 0.37251685802806633,
      "grad_norm": 1.9170514345169067,
      "learning_rate": 3.474697295728208e-05,
      "loss": 2.9882,
      "step": 4088
    },
    {
      "epoch": 0.3726079825041006,
      "grad_norm": 2.1357595920562744,
      "learning_rate": 3.4740382011523586e-05,
      "loss": 3.1946,
      "step": 4089
    },
    {
      "epoch": 0.37269910698013486,
      "grad_norm": 2.025557041168213,
      "learning_rate": 3.473379026750242e-05,
      "loss": 2.7868,
      "step": 4090
    },
    {
      "epoch": 0.37279023145616913,
      "grad_norm": 2.765110492706299,
      "learning_rate": 3.47271977257588e-05,
      "loss": 3.016,
      "step": 4091
    },
    {
      "epoch": 0.3728813559322034,
      "grad_norm": 1.8704719543457031,
      "learning_rate": 3.472060438683302e-05,
      "loss": 2.9377,
      "step": 4092
    },
    {
      "epoch": 0.37297248040823766,
      "grad_norm": 2.644057035446167,
      "learning_rate": 3.471401025126542e-05,
      "loss": 3.9747,
      "step": 4093
    },
    {
      "epoch": 0.3730636048842719,
      "grad_norm": 1.4239095449447632,
      "learning_rate": 3.470741531959642e-05,
      "loss": 2.8683,
      "step": 4094
    },
    {
      "epoch": 0.3731547293603062,
      "grad_norm": 2.439014434814453,
      "learning_rate": 3.47008195923665e-05,
      "loss": 3.0452,
      "step": 4095
    },
    {
      "epoch": 0.37324585383634046,
      "grad_norm": 1.1885889768600464,
      "learning_rate": 3.46942230701162e-05,
      "loss": 2.8186,
      "step": 4096
    },
    {
      "epoch": 0.3733369783123747,
      "grad_norm": 2.5863568782806396,
      "learning_rate": 3.468762575338614e-05,
      "loss": 2.5458,
      "step": 4097
    },
    {
      "epoch": 0.373428102788409,
      "grad_norm": 2.7627546787261963,
      "learning_rate": 3.4681027642717e-05,
      "loss": 3.245,
      "step": 4098
    },
    {
      "epoch": 0.37351922726444325,
      "grad_norm": 1.5321674346923828,
      "learning_rate": 3.46744287386495e-05,
      "loss": 3.142,
      "step": 4099
    },
    {
      "epoch": 0.3736103517404775,
      "grad_norm": 1.6103945970535278,
      "learning_rate": 3.466782904172448e-05,
      "loss": 2.9619,
      "step": 4100
    },
    {
      "epoch": 0.37370147621651173,
      "grad_norm": 2.4311273097991943,
      "learning_rate": 3.466122855248278e-05,
      "loss": 2.7748,
      "step": 4101
    },
    {
      "epoch": 0.373792600692546,
      "grad_norm": 1.8217551708221436,
      "learning_rate": 3.465462727146534e-05,
      "loss": 2.9766,
      "step": 4102
    },
    {
      "epoch": 0.37388372516858026,
      "grad_norm": 1.5154308080673218,
      "learning_rate": 3.464802519921318e-05,
      "loss": 2.9229,
      "step": 4103
    },
    {
      "epoch": 0.3739748496446145,
      "grad_norm": 1.7806023359298706,
      "learning_rate": 3.464142233626735e-05,
      "loss": 2.8425,
      "step": 4104
    },
    {
      "epoch": 0.3740659741206488,
      "grad_norm": 2.833099842071533,
      "learning_rate": 3.463481868316898e-05,
      "loss": 2.9216,
      "step": 4105
    },
    {
      "epoch": 0.37415709859668306,
      "grad_norm": 2.205800771713257,
      "learning_rate": 3.4628214240459264e-05,
      "loss": 2.9176,
      "step": 4106
    },
    {
      "epoch": 0.3742482230727173,
      "grad_norm": 1.7765841484069824,
      "learning_rate": 3.462160900867948e-05,
      "loss": 2.9006,
      "step": 4107
    },
    {
      "epoch": 0.3743393475487516,
      "grad_norm": 2.06925106048584,
      "learning_rate": 3.461500298837094e-05,
      "loss": 2.9761,
      "step": 4108
    },
    {
      "epoch": 0.37443047202478585,
      "grad_norm": 3.6971864700317383,
      "learning_rate": 3.460839618007503e-05,
      "loss": 2.819,
      "step": 4109
    },
    {
      "epoch": 0.3745215965008201,
      "grad_norm": 1.6620944738388062,
      "learning_rate": 3.460178858433322e-05,
      "loss": 2.9781,
      "step": 4110
    },
    {
      "epoch": 0.3746127209768544,
      "grad_norm": 2.0255467891693115,
      "learning_rate": 3.4595180201687e-05,
      "loss": 2.8969,
      "step": 4111
    },
    {
      "epoch": 0.37470384545288865,
      "grad_norm": 2.7446296215057373,
      "learning_rate": 3.458857103267797e-05,
      "loss": 3.0776,
      "step": 4112
    },
    {
      "epoch": 0.3747949699289229,
      "grad_norm": 1.3519412279129028,
      "learning_rate": 3.458196107784779e-05,
      "loss": 2.8386,
      "step": 4113
    },
    {
      "epoch": 0.3748860944049572,
      "grad_norm": 2.0908806324005127,
      "learning_rate": 3.457535033773815e-05,
      "loss": 2.5016,
      "step": 4114
    },
    {
      "epoch": 0.37497721888099145,
      "grad_norm": 1.2436522245407104,
      "learning_rate": 3.456873881289084e-05,
      "loss": 2.9284,
      "step": 4115
    },
    {
      "epoch": 0.3750683433570257,
      "grad_norm": 2.0627927780151367,
      "learning_rate": 3.45621265038477e-05,
      "loss": 2.9382,
      "step": 4116
    },
    {
      "epoch": 0.37515946783306,
      "grad_norm": 2.1016409397125244,
      "learning_rate": 3.455551341115062e-05,
      "loss": 2.6851,
      "step": 4117
    },
    {
      "epoch": 0.37525059230909424,
      "grad_norm": 2.442094326019287,
      "learning_rate": 3.454889953534158e-05,
      "loss": 2.9792,
      "step": 4118
    },
    {
      "epoch": 0.3753417167851285,
      "grad_norm": 1.299414873123169,
      "learning_rate": 3.454228487696262e-05,
      "loss": 2.8253,
      "step": 4119
    },
    {
      "epoch": 0.3754328412611628,
      "grad_norm": 1.8722634315490723,
      "learning_rate": 3.453566943655582e-05,
      "loss": 3.1063,
      "step": 4120
    },
    {
      "epoch": 0.375523965737197,
      "grad_norm": 1.3567423820495605,
      "learning_rate": 3.452905321466336e-05,
      "loss": 3.021,
      "step": 4121
    },
    {
      "epoch": 0.37561509021323125,
      "grad_norm": 1.7505642175674438,
      "learning_rate": 3.4522436211827455e-05,
      "loss": 2.9605,
      "step": 4122
    },
    {
      "epoch": 0.3757062146892655,
      "grad_norm": 1.0575664043426514,
      "learning_rate": 3.45158184285904e-05,
      "loss": 2.6987,
      "step": 4123
    },
    {
      "epoch": 0.3757973391652998,
      "grad_norm": 1.7600878477096558,
      "learning_rate": 3.4509199865494534e-05,
      "loss": 2.8643,
      "step": 4124
    },
    {
      "epoch": 0.37588846364133405,
      "grad_norm": 2.4899380207061768,
      "learning_rate": 3.450258052308229e-05,
      "loss": 2.6964,
      "step": 4125
    },
    {
      "epoch": 0.3759795881173683,
      "grad_norm": 3.2307300567626953,
      "learning_rate": 3.449596040189614e-05,
      "loss": 2.7823,
      "step": 4126
    },
    {
      "epoch": 0.3760707125934026,
      "grad_norm": 2.3406102657318115,
      "learning_rate": 3.448933950247865e-05,
      "loss": 2.9959,
      "step": 4127
    },
    {
      "epoch": 0.37616183706943684,
      "grad_norm": 1.1284135580062866,
      "learning_rate": 3.44827178253724e-05,
      "loss": 2.8507,
      "step": 4128
    },
    {
      "epoch": 0.3762529615454711,
      "grad_norm": 1.404544472694397,
      "learning_rate": 3.447609537112007e-05,
      "loss": 2.7774,
      "step": 4129
    },
    {
      "epoch": 0.3763440860215054,
      "grad_norm": 2.0779612064361572,
      "learning_rate": 3.44694721402644e-05,
      "loss": 2.7807,
      "step": 4130
    },
    {
      "epoch": 0.37643521049753964,
      "grad_norm": 1.0089433193206787,
      "learning_rate": 3.4462848133348196e-05,
      "loss": 2.7735,
      "step": 4131
    },
    {
      "epoch": 0.3765263349735739,
      "grad_norm": 1.2792896032333374,
      "learning_rate": 3.445622335091432e-05,
      "loss": 2.9638,
      "step": 4132
    },
    {
      "epoch": 0.37661745944960817,
      "grad_norm": 2.7967028617858887,
      "learning_rate": 3.4449597793505684e-05,
      "loss": 2.6247,
      "step": 4133
    },
    {
      "epoch": 0.37670858392564244,
      "grad_norm": 1.0178301334381104,
      "learning_rate": 3.44429714616653e-05,
      "loss": 2.8258,
      "step": 4134
    },
    {
      "epoch": 0.3767997084016767,
      "grad_norm": 1.7861324548721313,
      "learning_rate": 3.443634435593621e-05,
      "loss": 2.8018,
      "step": 4135
    },
    {
      "epoch": 0.37689083287771097,
      "grad_norm": 1.23408842086792,
      "learning_rate": 3.442971647686152e-05,
      "loss": 2.8004,
      "step": 4136
    },
    {
      "epoch": 0.37698195735374523,
      "grad_norm": 1.1951161623001099,
      "learning_rate": 3.442308782498443e-05,
      "loss": 2.9171,
      "step": 4137
    },
    {
      "epoch": 0.3770730818297795,
      "grad_norm": 1.6528165340423584,
      "learning_rate": 3.441645840084818e-05,
      "loss": 2.7883,
      "step": 4138
    },
    {
      "epoch": 0.37716420630581377,
      "grad_norm": 1.7744090557098389,
      "learning_rate": 3.440982820499609e-05,
      "loss": 2.8246,
      "step": 4139
    },
    {
      "epoch": 0.37725533078184803,
      "grad_norm": 1.8641780614852905,
      "learning_rate": 3.4403197237971496e-05,
      "loss": 2.94,
      "step": 4140
    },
    {
      "epoch": 0.37734645525788224,
      "grad_norm": 2.3618950843811035,
      "learning_rate": 3.4396565500317865e-05,
      "loss": 4.131,
      "step": 4141
    },
    {
      "epoch": 0.3774375797339165,
      "grad_norm": 1.2556582689285278,
      "learning_rate": 3.4389932992578675e-05,
      "loss": 2.7899,
      "step": 4142
    },
    {
      "epoch": 0.37752870420995077,
      "grad_norm": 1.6574651002883911,
      "learning_rate": 3.438329971529749e-05,
      "loss": 2.9883,
      "step": 4143
    },
    {
      "epoch": 0.37761982868598504,
      "grad_norm": 4.04176139831543,
      "learning_rate": 3.437666566901793e-05,
      "loss": 3.0416,
      "step": 4144
    },
    {
      "epoch": 0.3777109531620193,
      "grad_norm": 1.6325371265411377,
      "learning_rate": 3.437003085428369e-05,
      "loss": 2.6454,
      "step": 4145
    },
    {
      "epoch": 0.37780207763805357,
      "grad_norm": 1.7220995426177979,
      "learning_rate": 3.436339527163852e-05,
      "loss": 2.9266,
      "step": 4146
    },
    {
      "epoch": 0.37789320211408783,
      "grad_norm": 1.703921914100647,
      "learning_rate": 3.4356758921626234e-05,
      "loss": 2.7954,
      "step": 4147
    },
    {
      "epoch": 0.3779843265901221,
      "grad_norm": 2.379129648208618,
      "learning_rate": 3.435012180479069e-05,
      "loss": 2.826,
      "step": 4148
    },
    {
      "epoch": 0.37807545106615636,
      "grad_norm": 2.304898738861084,
      "learning_rate": 3.4343483921675836e-05,
      "loss": 2.9139,
      "step": 4149
    },
    {
      "epoch": 0.37816657554219063,
      "grad_norm": 1.7130361795425415,
      "learning_rate": 3.433684527282567e-05,
      "loss": 2.8424,
      "step": 4150
    },
    {
      "epoch": 0.3782577000182249,
      "grad_norm": 2.0056562423706055,
      "learning_rate": 3.433020585878426e-05,
      "loss": 2.9831,
      "step": 4151
    },
    {
      "epoch": 0.37834882449425916,
      "grad_norm": 1.5037654638290405,
      "learning_rate": 3.4323565680095746e-05,
      "loss": 2.7694,
      "step": 4152
    },
    {
      "epoch": 0.3784399489702934,
      "grad_norm": 2.3252577781677246,
      "learning_rate": 3.431692473730428e-05,
      "loss": 2.757,
      "step": 4153
    },
    {
      "epoch": 0.3785310734463277,
      "grad_norm": 1.4820101261138916,
      "learning_rate": 3.431028303095415e-05,
      "loss": 2.8359,
      "step": 4154
    },
    {
      "epoch": 0.37862219792236196,
      "grad_norm": 1.652717113494873,
      "learning_rate": 3.430364056158965e-05,
      "loss": 2.9239,
      "step": 4155
    },
    {
      "epoch": 0.3787133223983962,
      "grad_norm": 1.9454329013824463,
      "learning_rate": 3.4296997329755157e-05,
      "loss": 3.0929,
      "step": 4156
    },
    {
      "epoch": 0.3788044468744305,
      "grad_norm": 1.1464757919311523,
      "learning_rate": 3.4290353335995125e-05,
      "loss": 2.8641,
      "step": 4157
    },
    {
      "epoch": 0.37889557135046475,
      "grad_norm": 1.2712149620056152,
      "learning_rate": 3.428370858085404e-05,
      "loss": 2.7583,
      "step": 4158
    },
    {
      "epoch": 0.378986695826499,
      "grad_norm": 1.8796969652175903,
      "learning_rate": 3.427706306487647e-05,
      "loss": 3.1049,
      "step": 4159
    },
    {
      "epoch": 0.3790778203025333,
      "grad_norm": 2.439962863922119,
      "learning_rate": 3.427041678860704e-05,
      "loss": 2.8188,
      "step": 4160
    },
    {
      "epoch": 0.3791689447785675,
      "grad_norm": 2.2781715393066406,
      "learning_rate": 3.4263769752590445e-05,
      "loss": 2.9318,
      "step": 4161
    },
    {
      "epoch": 0.37926006925460176,
      "grad_norm": 1.1783050298690796,
      "learning_rate": 3.425712195737143e-05,
      "loss": 2.7495,
      "step": 4162
    },
    {
      "epoch": 0.379351193730636,
      "grad_norm": 2.021010160446167,
      "learning_rate": 3.425047340349481e-05,
      "loss": 2.9098,
      "step": 4163
    },
    {
      "epoch": 0.3794423182066703,
      "grad_norm": 1.9897592067718506,
      "learning_rate": 3.4243824091505455e-05,
      "loss": 2.7142,
      "step": 4164
    },
    {
      "epoch": 0.37953344268270456,
      "grad_norm": 2.986851453781128,
      "learning_rate": 3.423717402194832e-05,
      "loss": 2.7326,
      "step": 4165
    },
    {
      "epoch": 0.3796245671587388,
      "grad_norm": 2.3054416179656982,
      "learning_rate": 3.423052319536837e-05,
      "loss": 2.9543,
      "step": 4166
    },
    {
      "epoch": 0.3797156916347731,
      "grad_norm": 2.4709482192993164,
      "learning_rate": 3.4223871612310695e-05,
      "loss": 2.8406,
      "step": 4167
    },
    {
      "epoch": 0.37980681611080735,
      "grad_norm": 1.275244116783142,
      "learning_rate": 3.421721927332042e-05,
      "loss": 2.7616,
      "step": 4168
    },
    {
      "epoch": 0.3798979405868416,
      "grad_norm": 2.0112144947052,
      "learning_rate": 3.421056617894272e-05,
      "loss": 2.8657,
      "step": 4169
    },
    {
      "epoch": 0.3799890650628759,
      "grad_norm": 2.556831121444702,
      "learning_rate": 3.420391232972284e-05,
      "loss": 3.1722,
      "step": 4170
    },
    {
      "epoch": 0.38008018953891015,
      "grad_norm": 1.1606031656265259,
      "learning_rate": 3.419725772620609e-05,
      "loss": 2.7476,
      "step": 4171
    },
    {
      "epoch": 0.3801713140149444,
      "grad_norm": 1.899490237236023,
      "learning_rate": 3.419060236893785e-05,
      "loss": 2.8447,
      "step": 4172
    },
    {
      "epoch": 0.3802624384909787,
      "grad_norm": 2.2114531993865967,
      "learning_rate": 3.418394625846354e-05,
      "loss": 2.7659,
      "step": 4173
    },
    {
      "epoch": 0.38035356296701295,
      "grad_norm": 2.282189130783081,
      "learning_rate": 3.417728939532866e-05,
      "loss": 3.0587,
      "step": 4174
    },
    {
      "epoch": 0.3804446874430472,
      "grad_norm": 1.6870964765548706,
      "learning_rate": 3.417063178007877e-05,
      "loss": 3.007,
      "step": 4175
    },
    {
      "epoch": 0.3805358119190815,
      "grad_norm": 2.2074756622314453,
      "learning_rate": 3.4163973413259494e-05,
      "loss": 3.1778,
      "step": 4176
    },
    {
      "epoch": 0.38062693639511574,
      "grad_norm": 1.7985002994537354,
      "learning_rate": 3.4157314295416493e-05,
      "loss": 2.5898,
      "step": 4177
    },
    {
      "epoch": 0.38071806087115,
      "grad_norm": 3.1245524883270264,
      "learning_rate": 3.415065442709552e-05,
      "loss": 4.0224,
      "step": 4178
    },
    {
      "epoch": 0.3808091853471843,
      "grad_norm": 3.275292158126831,
      "learning_rate": 3.414399380884237e-05,
      "loss": 3.0245,
      "step": 4179
    },
    {
      "epoch": 0.38090030982321854,
      "grad_norm": 2.4659037590026855,
      "learning_rate": 3.413733244120292e-05,
      "loss": 2.752,
      "step": 4180
    },
    {
      "epoch": 0.3809914342992528,
      "grad_norm": 1.986199140548706,
      "learning_rate": 3.413067032472308e-05,
      "loss": 3.0155,
      "step": 4181
    },
    {
      "epoch": 0.381082558775287,
      "grad_norm": 2.4537720680236816,
      "learning_rate": 3.412400745994885e-05,
      "loss": 2.6609,
      "step": 4182
    },
    {
      "epoch": 0.3811736832513213,
      "grad_norm": 2.0721962451934814,
      "learning_rate": 3.411734384742627e-05,
      "loss": 3.0887,
      "step": 4183
    },
    {
      "epoch": 0.38126480772735555,
      "grad_norm": 2.119662284851074,
      "learning_rate": 3.411067948770145e-05,
      "loss": 2.8415,
      "step": 4184
    },
    {
      "epoch": 0.3813559322033898,
      "grad_norm": 2.0754473209381104,
      "learning_rate": 3.410401438132056e-05,
      "loss": 3.052,
      "step": 4185
    },
    {
      "epoch": 0.3814470566794241,
      "grad_norm": 2.078939437866211,
      "learning_rate": 3.409734852882983e-05,
      "loss": 2.9295,
      "step": 4186
    },
    {
      "epoch": 0.38153818115545834,
      "grad_norm": 2.749246835708618,
      "learning_rate": 3.409068193077556e-05,
      "loss": 3.2956,
      "step": 4187
    },
    {
      "epoch": 0.3816293056314926,
      "grad_norm": 2.1682963371276855,
      "learning_rate": 3.40840145877041e-05,
      "loss": 3.0412,
      "step": 4188
    },
    {
      "epoch": 0.3817204301075269,
      "grad_norm": 2.55059552192688,
      "learning_rate": 3.407734650016187e-05,
      "loss": 3.2763,
      "step": 4189
    },
    {
      "epoch": 0.38181155458356114,
      "grad_norm": 2.0001368522644043,
      "learning_rate": 3.407067766869533e-05,
      "loss": 2.7592,
      "step": 4190
    },
    {
      "epoch": 0.3819026790595954,
      "grad_norm": 3.2308475971221924,
      "learning_rate": 3.406400809385104e-05,
      "loss": 1.9433,
      "step": 4191
    },
    {
      "epoch": 0.3819938035356297,
      "grad_norm": 1.8999704122543335,
      "learning_rate": 3.405733777617558e-05,
      "loss": 3.0112,
      "step": 4192
    },
    {
      "epoch": 0.38208492801166394,
      "grad_norm": 2.6240406036376953,
      "learning_rate": 3.405066671621561e-05,
      "loss": 3.0163,
      "step": 4193
    },
    {
      "epoch": 0.3821760524876982,
      "grad_norm": 1.9133130311965942,
      "learning_rate": 3.404399491451786e-05,
      "loss": 2.6075,
      "step": 4194
    },
    {
      "epoch": 0.38226717696373247,
      "grad_norm": 1.256852388381958,
      "learning_rate": 3.403732237162911e-05,
      "loss": 2.8427,
      "step": 4195
    },
    {
      "epoch": 0.38235830143976673,
      "grad_norm": 1.1847515106201172,
      "learning_rate": 3.403064908809619e-05,
      "loss": 2.6475,
      "step": 4196
    },
    {
      "epoch": 0.382449425915801,
      "grad_norm": 2.2173714637756348,
      "learning_rate": 3.402397506446601e-05,
      "loss": 2.8204,
      "step": 4197
    },
    {
      "epoch": 0.38254055039183527,
      "grad_norm": 2.0657732486724854,
      "learning_rate": 3.4017300301285523e-05,
      "loss": 2.9728,
      "step": 4198
    },
    {
      "epoch": 0.38263167486786953,
      "grad_norm": 2.048556089401245,
      "learning_rate": 3.401062479910177e-05,
      "loss": 2.9558,
      "step": 4199
    },
    {
      "epoch": 0.3827227993439038,
      "grad_norm": 1.6779792308807373,
      "learning_rate": 3.4003948558461825e-05,
      "loss": 3.0049,
      "step": 4200
    },
    {
      "epoch": 0.38281392381993806,
      "grad_norm": 2.5007333755493164,
      "learning_rate": 3.399727157991283e-05,
      "loss": 2.9348,
      "step": 4201
    },
    {
      "epoch": 0.3829050482959723,
      "grad_norm": 1.823770523071289,
      "learning_rate": 3.3990593864001986e-05,
      "loss": 2.9164,
      "step": 4202
    },
    {
      "epoch": 0.38299617277200654,
      "grad_norm": 1.5018728971481323,
      "learning_rate": 3.398391541127657e-05,
      "loss": 2.7829,
      "step": 4203
    },
    {
      "epoch": 0.3830872972480408,
      "grad_norm": 2.4136838912963867,
      "learning_rate": 3.397723622228389e-05,
      "loss": 2.8048,
      "step": 4204
    },
    {
      "epoch": 0.38317842172407507,
      "grad_norm": 2.0759356021881104,
      "learning_rate": 3.397055629757135e-05,
      "loss": 3.0083,
      "step": 4205
    },
    {
      "epoch": 0.38326954620010933,
      "grad_norm": 2.029360771179199,
      "learning_rate": 3.396387563768638e-05,
      "loss": 3.0666,
      "step": 4206
    },
    {
      "epoch": 0.3833606706761436,
      "grad_norm": 2.3367111682891846,
      "learning_rate": 3.3957194243176516e-05,
      "loss": 3.0711,
      "step": 4207
    },
    {
      "epoch": 0.38345179515217787,
      "grad_norm": 1.6189872026443481,
      "learning_rate": 3.3950512114589286e-05,
      "loss": 2.9735,
      "step": 4208
    },
    {
      "epoch": 0.38354291962821213,
      "grad_norm": 1.2657705545425415,
      "learning_rate": 3.394382925247234e-05,
      "loss": 2.9048,
      "step": 4209
    },
    {
      "epoch": 0.3836340441042464,
      "grad_norm": 1.9889881610870361,
      "learning_rate": 3.393714565737336e-05,
      "loss": 2.435,
      "step": 4210
    },
    {
      "epoch": 0.38372516858028066,
      "grad_norm": 1.3429951667785645,
      "learning_rate": 3.393046132984009e-05,
      "loss": 2.8926,
      "step": 4211
    },
    {
      "epoch": 0.38381629305631493,
      "grad_norm": 2.322265148162842,
      "learning_rate": 3.392377627042034e-05,
      "loss": 3.0992,
      "step": 4212
    },
    {
      "epoch": 0.3839074175323492,
      "grad_norm": 1.146443486213684,
      "learning_rate": 3.391709047966198e-05,
      "loss": 2.7765,
      "step": 4213
    },
    {
      "epoch": 0.38399854200838346,
      "grad_norm": 2.024702310562134,
      "learning_rate": 3.391040395811293e-05,
      "loss": 2.7704,
      "step": 4214
    },
    {
      "epoch": 0.3840896664844177,
      "grad_norm": 2.68880295753479,
      "learning_rate": 3.3903716706321173e-05,
      "loss": 2.624,
      "step": 4215
    },
    {
      "epoch": 0.384180790960452,
      "grad_norm": 1.6819884777069092,
      "learning_rate": 3.389702872483477e-05,
      "loss": 3.0312,
      "step": 4216
    },
    {
      "epoch": 0.38427191543648626,
      "grad_norm": 2.098794937133789,
      "learning_rate": 3.389034001420183e-05,
      "loss": 2.631,
      "step": 4217
    },
    {
      "epoch": 0.3843630399125205,
      "grad_norm": 1.624861240386963,
      "learning_rate": 3.38836505749705e-05,
      "loss": 2.9423,
      "step": 4218
    },
    {
      "epoch": 0.3844541643885548,
      "grad_norm": 2.2856056690216064,
      "learning_rate": 3.387696040768901e-05,
      "loss": 2.9971,
      "step": 4219
    },
    {
      "epoch": 0.38454528886458905,
      "grad_norm": 2.240643262863159,
      "learning_rate": 3.387026951290566e-05,
      "loss": 3.0408,
      "step": 4220
    },
    {
      "epoch": 0.3846364133406233,
      "grad_norm": 1.0858722925186157,
      "learning_rate": 3.3863577891168774e-05,
      "loss": 2.8506,
      "step": 4221
    },
    {
      "epoch": 0.3847275378166575,
      "grad_norm": 1.4940340518951416,
      "learning_rate": 3.385688554302678e-05,
      "loss": 2.8445,
      "step": 4222
    },
    {
      "epoch": 0.3848186622926918,
      "grad_norm": 1.144880771636963,
      "learning_rate": 3.385019246902813e-05,
      "loss": 2.8213,
      "step": 4223
    },
    {
      "epoch": 0.38490978676872606,
      "grad_norm": 1.0414236783981323,
      "learning_rate": 3.384349866972134e-05,
      "loss": 2.7937,
      "step": 4224
    },
    {
      "epoch": 0.3850009112447603,
      "grad_norm": 1.2710025310516357,
      "learning_rate": 3.383680414565501e-05,
      "loss": 2.8514,
      "step": 4225
    },
    {
      "epoch": 0.3850920357207946,
      "grad_norm": 2.161470651626587,
      "learning_rate": 3.3830108897377775e-05,
      "loss": 2.6643,
      "step": 4226
    },
    {
      "epoch": 0.38518316019682886,
      "grad_norm": 1.4335719347000122,
      "learning_rate": 3.3823412925438333e-05,
      "loss": 2.8956,
      "step": 4227
    },
    {
      "epoch": 0.3852742846728631,
      "grad_norm": 2.818394422531128,
      "learning_rate": 3.381671623038545e-05,
      "loss": 2.6416,
      "step": 4228
    },
    {
      "epoch": 0.3853654091488974,
      "grad_norm": 1.3687599897384644,
      "learning_rate": 3.381001881276794e-05,
      "loss": 2.8143,
      "step": 4229
    },
    {
      "epoch": 0.38545653362493165,
      "grad_norm": 2.265561819076538,
      "learning_rate": 3.3803320673134695e-05,
      "loss": 3.0132,
      "step": 4230
    },
    {
      "epoch": 0.3855476581009659,
      "grad_norm": 1.2492432594299316,
      "learning_rate": 3.379662181203465e-05,
      "loss": 2.8567,
      "step": 4231
    },
    {
      "epoch": 0.3856387825770002,
      "grad_norm": 2.069157600402832,
      "learning_rate": 3.378992223001679e-05,
      "loss": 2.8395,
      "step": 4232
    },
    {
      "epoch": 0.38572990705303445,
      "grad_norm": 2.366887331008911,
      "learning_rate": 3.378322192763018e-05,
      "loss": 2.6251,
      "step": 4233
    },
    {
      "epoch": 0.3858210315290687,
      "grad_norm": 1.3577516078948975,
      "learning_rate": 3.377652090542395e-05,
      "loss": 2.8356,
      "step": 4234
    },
    {
      "epoch": 0.385912156005103,
      "grad_norm": 1.536522626876831,
      "learning_rate": 3.3769819163947254e-05,
      "loss": 2.8043,
      "step": 4235
    },
    {
      "epoch": 0.38600328048113725,
      "grad_norm": 1.0610957145690918,
      "learning_rate": 3.3763116703749334e-05,
      "loss": 2.6839,
      "step": 4236
    },
    {
      "epoch": 0.3860944049571715,
      "grad_norm": 2.998645067214966,
      "learning_rate": 3.37564135253795e-05,
      "loss": 2.7045,
      "step": 4237
    },
    {
      "epoch": 0.3861855294332058,
      "grad_norm": 1.801048755645752,
      "learning_rate": 3.374970962938708e-05,
      "loss": 2.5778,
      "step": 4238
    },
    {
      "epoch": 0.38627665390924004,
      "grad_norm": 2.298513889312744,
      "learning_rate": 3.374300501632149e-05,
      "loss": 3.9502,
      "step": 4239
    },
    {
      "epoch": 0.3863677783852743,
      "grad_norm": 1.727810263633728,
      "learning_rate": 3.37362996867322e-05,
      "loss": 3.0594,
      "step": 4240
    },
    {
      "epoch": 0.3864589028613086,
      "grad_norm": 2.07499361038208,
      "learning_rate": 3.372959364116874e-05,
      "loss": 4.1238,
      "step": 4241
    },
    {
      "epoch": 0.3865500273373428,
      "grad_norm": 1.2313474416732788,
      "learning_rate": 3.372288688018072e-05,
      "loss": 2.9299,
      "step": 4242
    },
    {
      "epoch": 0.38664115181337705,
      "grad_norm": 2.3732800483703613,
      "learning_rate": 3.371617940431774e-05,
      "loss": 3.261,
      "step": 4243
    },
    {
      "epoch": 0.3867322762894113,
      "grad_norm": 2.1866507530212402,
      "learning_rate": 3.370947121412954e-05,
      "loss": 2.9687,
      "step": 4244
    },
    {
      "epoch": 0.3868234007654456,
      "grad_norm": 1.855997920036316,
      "learning_rate": 3.3702762310165855e-05,
      "loss": 2.9127,
      "step": 4245
    },
    {
      "epoch": 0.38691452524147985,
      "grad_norm": 2.1293985843658447,
      "learning_rate": 3.369605269297653e-05,
      "loss": 3.0522,
      "step": 4246
    },
    {
      "epoch": 0.3870056497175141,
      "grad_norm": 2.461085319519043,
      "learning_rate": 3.3689342363111435e-05,
      "loss": 3.9874,
      "step": 4247
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 1.077233076095581,
      "learning_rate": 3.3682631321120504e-05,
      "loss": 2.7806,
      "step": 4248
    },
    {
      "epoch": 0.38718789866958264,
      "grad_norm": 2.0236458778381348,
      "learning_rate": 3.367591956755375e-05,
      "loss": 3.0726,
      "step": 4249
    },
    {
      "epoch": 0.3872790231456169,
      "grad_norm": 1.9083157777786255,
      "learning_rate": 3.366920710296121e-05,
      "loss": 2.9799,
      "step": 4250
    },
    {
      "epoch": 0.3873701476216512,
      "grad_norm": 2.2697513103485107,
      "learning_rate": 3.3662493927892995e-05,
      "loss": 2.5751,
      "step": 4251
    },
    {
      "epoch": 0.38746127209768544,
      "grad_norm": 2.0585193634033203,
      "learning_rate": 3.365578004289929e-05,
      "loss": 2.6989,
      "step": 4252
    },
    {
      "epoch": 0.3875523965737197,
      "grad_norm": 1.5001369714736938,
      "learning_rate": 3.364906544853031e-05,
      "loss": 2.7434,
      "step": 4253
    },
    {
      "epoch": 0.38764352104975397,
      "grad_norm": 2.296638011932373,
      "learning_rate": 3.364235014533636e-05,
      "loss": 2.8908,
      "step": 4254
    },
    {
      "epoch": 0.38773464552578824,
      "grad_norm": 1.8784656524658203,
      "learning_rate": 3.363563413386777e-05,
      "loss": 2.6903,
      "step": 4255
    },
    {
      "epoch": 0.3878257700018225,
      "grad_norm": 3.225198268890381,
      "learning_rate": 3.362891741467495e-05,
      "loss": 2.5186,
      "step": 4256
    },
    {
      "epoch": 0.38791689447785677,
      "grad_norm": 2.0329434871673584,
      "learning_rate": 3.362219998830836e-05,
      "loss": 1.9759,
      "step": 4257
    },
    {
      "epoch": 0.38800801895389103,
      "grad_norm": 1.6679686307907104,
      "learning_rate": 3.361548185531853e-05,
      "loss": 2.7766,
      "step": 4258
    },
    {
      "epoch": 0.3880991434299253,
      "grad_norm": 1.7468844652175903,
      "learning_rate": 3.3608763016256014e-05,
      "loss": 2.8846,
      "step": 4259
    },
    {
      "epoch": 0.38819026790595956,
      "grad_norm": 2.055598258972168,
      "learning_rate": 3.360204347167146e-05,
      "loss": 2.7307,
      "step": 4260
    },
    {
      "epoch": 0.38828139238199383,
      "grad_norm": 3.095827102661133,
      "learning_rate": 3.359532322211558e-05,
      "loss": 2.9634,
      "step": 4261
    },
    {
      "epoch": 0.3883725168580281,
      "grad_norm": 1.2687351703643799,
      "learning_rate": 3.35886022681391e-05,
      "loss": 2.7861,
      "step": 4262
    },
    {
      "epoch": 0.3884636413340623,
      "grad_norm": 1.3467941284179688,
      "learning_rate": 3.358188061029283e-05,
      "loss": 2.8769,
      "step": 4263
    },
    {
      "epoch": 0.38855476581009657,
      "grad_norm": 1.147680401802063,
      "learning_rate": 3.3575158249127645e-05,
      "loss": 2.7851,
      "step": 4264
    },
    {
      "epoch": 0.38864589028613084,
      "grad_norm": 2.3483688831329346,
      "learning_rate": 3.356843518519447e-05,
      "loss": 2.823,
      "step": 4265
    },
    {
      "epoch": 0.3887370147621651,
      "grad_norm": 2.1379716396331787,
      "learning_rate": 3.356171141904428e-05,
      "loss": 2.8657,
      "step": 4266
    },
    {
      "epoch": 0.38882813923819937,
      "grad_norm": 2.722900867462158,
      "learning_rate": 3.355498695122812e-05,
      "loss": 3.0311,
      "step": 4267
    },
    {
      "epoch": 0.38891926371423363,
      "grad_norm": 2.3523757457733154,
      "learning_rate": 3.354826178229708e-05,
      "loss": 4.1591,
      "step": 4268
    },
    {
      "epoch": 0.3890103881902679,
      "grad_norm": 1.3672984838485718,
      "learning_rate": 3.354153591280231e-05,
      "loss": 2.8067,
      "step": 4269
    },
    {
      "epoch": 0.38910151266630216,
      "grad_norm": 1.2910019159317017,
      "learning_rate": 3.353480934329504e-05,
      "loss": 2.7548,
      "step": 4270
    },
    {
      "epoch": 0.38919263714233643,
      "grad_norm": 2.4902775287628174,
      "learning_rate": 3.352808207432653e-05,
      "loss": 2.8104,
      "step": 4271
    },
    {
      "epoch": 0.3892837616183707,
      "grad_norm": 2.833726644515991,
      "learning_rate": 3.3521354106448096e-05,
      "loss": 2.755,
      "step": 4272
    },
    {
      "epoch": 0.38937488609440496,
      "grad_norm": 1.8954635858535767,
      "learning_rate": 3.3514625440211135e-05,
      "loss": 2.811,
      "step": 4273
    },
    {
      "epoch": 0.3894660105704392,
      "grad_norm": 2.2011353969573975,
      "learning_rate": 3.350789607616708e-05,
      "loss": 3.1399,
      "step": 4274
    },
    {
      "epoch": 0.3895571350464735,
      "grad_norm": 2.487058162689209,
      "learning_rate": 3.350116601486743e-05,
      "loss": 2.9175,
      "step": 4275
    },
    {
      "epoch": 0.38964825952250776,
      "grad_norm": 4.795505523681641,
      "learning_rate": 3.349443525686374e-05,
      "loss": 2.9089,
      "step": 4276
    },
    {
      "epoch": 0.389739383998542,
      "grad_norm": 2.1616175174713135,
      "learning_rate": 3.348770380270763e-05,
      "loss": 2.9424,
      "step": 4277
    },
    {
      "epoch": 0.3898305084745763,
      "grad_norm": 1.2748678922653198,
      "learning_rate": 3.348097165295076e-05,
      "loss": 2.8536,
      "step": 4278
    },
    {
      "epoch": 0.38992163295061055,
      "grad_norm": 2.319888114929199,
      "learning_rate": 3.347423880814486e-05,
      "loss": 2.9395,
      "step": 4279
    },
    {
      "epoch": 0.3900127574266448,
      "grad_norm": 1.380151391029358,
      "learning_rate": 3.346750526884171e-05,
      "loss": 2.751,
      "step": 4280
    },
    {
      "epoch": 0.3901038819026791,
      "grad_norm": 2.4729177951812744,
      "learning_rate": 3.346077103559315e-05,
      "loss": 2.6823,
      "step": 4281
    },
    {
      "epoch": 0.39019500637871335,
      "grad_norm": 2.2468976974487305,
      "learning_rate": 3.3454036108951084e-05,
      "loss": 2.8829,
      "step": 4282
    },
    {
      "epoch": 0.39028613085474756,
      "grad_norm": 1.721650242805481,
      "learning_rate": 3.3447300489467456e-05,
      "loss": 2.9926,
      "step": 4283
    },
    {
      "epoch": 0.3903772553307818,
      "grad_norm": 1.2114652395248413,
      "learning_rate": 3.3440564177694286e-05,
      "loss": 2.9401,
      "step": 4284
    },
    {
      "epoch": 0.3904683798068161,
      "grad_norm": 2.134742259979248,
      "learning_rate": 3.343382717418363e-05,
      "loss": 2.831,
      "step": 4285
    },
    {
      "epoch": 0.39055950428285036,
      "grad_norm": 3.0684754848480225,
      "learning_rate": 3.3427089479487625e-05,
      "loss": 3.031,
      "step": 4286
    },
    {
      "epoch": 0.3906506287588846,
      "grad_norm": 1.3037548065185547,
      "learning_rate": 3.342035109415844e-05,
      "loss": 2.7848,
      "step": 4287
    },
    {
      "epoch": 0.3907417532349189,
      "grad_norm": 1.451306939125061,
      "learning_rate": 3.3413612018748316e-05,
      "loss": 2.6825,
      "step": 4288
    },
    {
      "epoch": 0.39083287771095315,
      "grad_norm": 1.8713335990905762,
      "learning_rate": 3.340687225380956e-05,
      "loss": 2.8644,
      "step": 4289
    },
    {
      "epoch": 0.3909240021869874,
      "grad_norm": 2.493887424468994,
      "learning_rate": 3.34001317998945e-05,
      "loss": 3.0251,
      "step": 4290
    },
    {
      "epoch": 0.3910151266630217,
      "grad_norm": 1.1594059467315674,
      "learning_rate": 3.339339065755556e-05,
      "loss": 2.8049,
      "step": 4291
    },
    {
      "epoch": 0.39110625113905595,
      "grad_norm": 1.2584283351898193,
      "learning_rate": 3.338664882734519e-05,
      "loss": 2.7395,
      "step": 4292
    },
    {
      "epoch": 0.3911973756150902,
      "grad_norm": 1.5875940322875977,
      "learning_rate": 3.337990630981591e-05,
      "loss": 2.6939,
      "step": 4293
    },
    {
      "epoch": 0.3912885000911245,
      "grad_norm": 1.9941436052322388,
      "learning_rate": 3.337316310552031e-05,
      "loss": 3.0694,
      "step": 4294
    },
    {
      "epoch": 0.39137962456715875,
      "grad_norm": 2.142404317855835,
      "learning_rate": 3.336641921501101e-05,
      "loss": 2.8627,
      "step": 4295
    },
    {
      "epoch": 0.391470749043193,
      "grad_norm": 2.685666799545288,
      "learning_rate": 3.335967463884071e-05,
      "loss": 2.9717,
      "step": 4296
    },
    {
      "epoch": 0.3915618735192273,
      "grad_norm": 2.0626375675201416,
      "learning_rate": 3.335292937756215e-05,
      "loss": 3.0222,
      "step": 4297
    },
    {
      "epoch": 0.39165299799526154,
      "grad_norm": 2.0693519115448,
      "learning_rate": 3.3346183431728115e-05,
      "loss": 2.9323,
      "step": 4298
    },
    {
      "epoch": 0.3917441224712958,
      "grad_norm": 1.6387494802474976,
      "learning_rate": 3.333943680189148e-05,
      "loss": 2.8351,
      "step": 4299
    },
    {
      "epoch": 0.3918352469473301,
      "grad_norm": 1.9031468629837036,
      "learning_rate": 3.333268948860515e-05,
      "loss": 2.4677,
      "step": 4300
    },
    {
      "epoch": 0.39192637142336434,
      "grad_norm": 3.470432996749878,
      "learning_rate": 3.33259414924221e-05,
      "loss": 3.1321,
      "step": 4301
    },
    {
      "epoch": 0.3920174958993986,
      "grad_norm": 2.2685863971710205,
      "learning_rate": 3.331919281389535e-05,
      "loss": 2.8918,
      "step": 4302
    },
    {
      "epoch": 0.3921086203754328,
      "grad_norm": 1.2296284437179565,
      "learning_rate": 3.331244345357798e-05,
      "loss": 2.8812,
      "step": 4303
    },
    {
      "epoch": 0.3921997448514671,
      "grad_norm": 2.130908250808716,
      "learning_rate": 3.3305693412023134e-05,
      "loss": 3.0668,
      "step": 4304
    },
    {
      "epoch": 0.39229086932750135,
      "grad_norm": 2.0666961669921875,
      "learning_rate": 3.329894268978399e-05,
      "loss": 2.8964,
      "step": 4305
    },
    {
      "epoch": 0.3923819938035356,
      "grad_norm": 2.3169262409210205,
      "learning_rate": 3.329219128741382e-05,
      "loss": 2.9066,
      "step": 4306
    },
    {
      "epoch": 0.3924731182795699,
      "grad_norm": 1.8417303562164307,
      "learning_rate": 3.32854392054659e-05,
      "loss": 2.8802,
      "step": 4307
    },
    {
      "epoch": 0.39256424275560414,
      "grad_norm": 2.102884292602539,
      "learning_rate": 3.32786864444936e-05,
      "loss": 2.8798,
      "step": 4308
    },
    {
      "epoch": 0.3926553672316384,
      "grad_norm": 1.325048804283142,
      "learning_rate": 3.327193300505035e-05,
      "loss": 2.9291,
      "step": 4309
    },
    {
      "epoch": 0.3927464917076727,
      "grad_norm": 2.426746129989624,
      "learning_rate": 3.326517888768961e-05,
      "loss": 2.606,
      "step": 4310
    },
    {
      "epoch": 0.39283761618370694,
      "grad_norm": 1.894679069519043,
      "learning_rate": 3.3258424092964894e-05,
      "loss": 2.5896,
      "step": 4311
    },
    {
      "epoch": 0.3929287406597412,
      "grad_norm": 2.052297592163086,
      "learning_rate": 3.32516686214298e-05,
      "loss": 2.8738,
      "step": 4312
    },
    {
      "epoch": 0.39301986513577547,
      "grad_norm": 1.8670403957366943,
      "learning_rate": 3.3244912473637965e-05,
      "loss": 2.88,
      "step": 4313
    },
    {
      "epoch": 0.39311098961180974,
      "grad_norm": 2.7187044620513916,
      "learning_rate": 3.323815565014308e-05,
      "loss": 3.0874,
      "step": 4314
    },
    {
      "epoch": 0.393202114087844,
      "grad_norm": 1.4732177257537842,
      "learning_rate": 3.3231398151498886e-05,
      "loss": 2.9712,
      "step": 4315
    },
    {
      "epoch": 0.39329323856387827,
      "grad_norm": 2.3827435970306396,
      "learning_rate": 3.3224639978259195e-05,
      "loss": 3.2639,
      "step": 4316
    },
    {
      "epoch": 0.39338436303991253,
      "grad_norm": 1.7608435153961182,
      "learning_rate": 3.3217881130977864e-05,
      "loss": 2.9826,
      "step": 4317
    },
    {
      "epoch": 0.3934754875159468,
      "grad_norm": 4.645968914031982,
      "learning_rate": 3.3211121610208805e-05,
      "loss": 2.8524,
      "step": 4318
    },
    {
      "epoch": 0.39356661199198106,
      "grad_norm": 1.8105016946792603,
      "learning_rate": 3.3204361416505985e-05,
      "loss": 2.4935,
      "step": 4319
    },
    {
      "epoch": 0.39365773646801533,
      "grad_norm": 2.248232364654541,
      "learning_rate": 3.319760055042344e-05,
      "loss": 3.128,
      "step": 4320
    },
    {
      "epoch": 0.3937488609440496,
      "grad_norm": 1.8415868282318115,
      "learning_rate": 3.319083901251524e-05,
      "loss": 2.9891,
      "step": 4321
    },
    {
      "epoch": 0.39383998542008386,
      "grad_norm": 1.2224351167678833,
      "learning_rate": 3.3184076803335514e-05,
      "loss": 2.8457,
      "step": 4322
    },
    {
      "epoch": 0.39393110989611807,
      "grad_norm": 2.3212549686431885,
      "learning_rate": 3.317731392343847e-05,
      "loss": 2.8333,
      "step": 4323
    },
    {
      "epoch": 0.39402223437215234,
      "grad_norm": 2.3624818325042725,
      "learning_rate": 3.317055037337834e-05,
      "loss": 3.0207,
      "step": 4324
    },
    {
      "epoch": 0.3941133588481866,
      "grad_norm": 1.2816665172576904,
      "learning_rate": 3.316378615370942e-05,
      "loss": 2.7063,
      "step": 4325
    },
    {
      "epoch": 0.39420448332422087,
      "grad_norm": 1.1228337287902832,
      "learning_rate": 3.3157021264986064e-05,
      "loss": 2.7213,
      "step": 4326
    },
    {
      "epoch": 0.39429560780025513,
      "grad_norm": 3.2542853355407715,
      "learning_rate": 3.31502557077627e-05,
      "loss": 2.9982,
      "step": 4327
    },
    {
      "epoch": 0.3943867322762894,
      "grad_norm": 1.7231829166412354,
      "learning_rate": 3.314348948259378e-05,
      "loss": 2.7593,
      "step": 4328
    },
    {
      "epoch": 0.39447785675232366,
      "grad_norm": 1.2360827922821045,
      "learning_rate": 3.313672259003381e-05,
      "loss": 2.7539,
      "step": 4329
    },
    {
      "epoch": 0.39456898122835793,
      "grad_norm": 2.971083879470825,
      "learning_rate": 3.3129955030637385e-05,
      "loss": 2.6367,
      "step": 4330
    },
    {
      "epoch": 0.3946601057043922,
      "grad_norm": 1.9886386394500732,
      "learning_rate": 3.3123186804959116e-05,
      "loss": 2.8373,
      "step": 4331
    },
    {
      "epoch": 0.39475123018042646,
      "grad_norm": 2.4619314670562744,
      "learning_rate": 3.311641791355369e-05,
      "loss": 4.2126,
      "step": 4332
    },
    {
      "epoch": 0.3948423546564607,
      "grad_norm": 1.9613878726959229,
      "learning_rate": 3.310964835697586e-05,
      "loss": 2.8815,
      "step": 4333
    },
    {
      "epoch": 0.394933479132495,
      "grad_norm": 1.1786651611328125,
      "learning_rate": 3.31028781357804e-05,
      "loss": 2.7479,
      "step": 4334
    },
    {
      "epoch": 0.39502460360852926,
      "grad_norm": 2.7005157470703125,
      "learning_rate": 3.309610725052216e-05,
      "loss": 2.6343,
      "step": 4335
    },
    {
      "epoch": 0.3951157280845635,
      "grad_norm": 2.365050792694092,
      "learning_rate": 3.308933570175605e-05,
      "loss": 3.2159,
      "step": 4336
    },
    {
      "epoch": 0.3952068525605978,
      "grad_norm": 2.0960474014282227,
      "learning_rate": 3.3082563490037006e-05,
      "loss": 2.9791,
      "step": 4337
    },
    {
      "epoch": 0.39529797703663205,
      "grad_norm": 1.3719370365142822,
      "learning_rate": 3.3075790615920054e-05,
      "loss": 2.8531,
      "step": 4338
    },
    {
      "epoch": 0.3953891015126663,
      "grad_norm": 2.012529134750366,
      "learning_rate": 3.306901707996026e-05,
      "loss": 2.9787,
      "step": 4339
    },
    {
      "epoch": 0.3954802259887006,
      "grad_norm": 1.1985915899276733,
      "learning_rate": 3.3062242882712724e-05,
      "loss": 2.8341,
      "step": 4340
    },
    {
      "epoch": 0.39557135046473485,
      "grad_norm": 1.65743088722229,
      "learning_rate": 3.305546802473263e-05,
      "loss": 3.0249,
      "step": 4341
    },
    {
      "epoch": 0.3956624749407691,
      "grad_norm": 1.9842802286148071,
      "learning_rate": 3.304869250657521e-05,
      "loss": 2.6948,
      "step": 4342
    },
    {
      "epoch": 0.3957535994168034,
      "grad_norm": 1.1486029624938965,
      "learning_rate": 3.304191632879573e-05,
      "loss": 2.7721,
      "step": 4343
    },
    {
      "epoch": 0.3958447238928376,
      "grad_norm": 1.9137284755706787,
      "learning_rate": 3.303513949194954e-05,
      "loss": 2.4157,
      "step": 4344
    },
    {
      "epoch": 0.39593584836887186,
      "grad_norm": 1.0785857439041138,
      "learning_rate": 3.3028361996592014e-05,
      "loss": 2.7075,
      "step": 4345
    },
    {
      "epoch": 0.3960269728449061,
      "grad_norm": 2.083606004714966,
      "learning_rate": 3.302158384327861e-05,
      "loss": 2.8424,
      "step": 4346
    },
    {
      "epoch": 0.3961180973209404,
      "grad_norm": 1.4267306327819824,
      "learning_rate": 3.3014805032564807e-05,
      "loss": 2.7744,
      "step": 4347
    },
    {
      "epoch": 0.39620922179697465,
      "grad_norm": 1.8474345207214355,
      "learning_rate": 3.300802556500616e-05,
      "loss": 2.9583,
      "step": 4348
    },
    {
      "epoch": 0.3963003462730089,
      "grad_norm": 1.4951956272125244,
      "learning_rate": 3.300124544115828e-05,
      "loss": 2.8661,
      "step": 4349
    },
    {
      "epoch": 0.3963914707490432,
      "grad_norm": 3.185985565185547,
      "learning_rate": 3.2994464661576824e-05,
      "loss": 2.4481,
      "step": 4350
    },
    {
      "epoch": 0.39648259522507745,
      "grad_norm": 2.1537766456604004,
      "learning_rate": 3.29876832268175e-05,
      "loss": 3.0235,
      "step": 4351
    },
    {
      "epoch": 0.3965737197011117,
      "grad_norm": 2.4585399627685547,
      "learning_rate": 3.2980901137436074e-05,
      "loss": 2.7231,
      "step": 4352
    },
    {
      "epoch": 0.396664844177146,
      "grad_norm": 1.205686092376709,
      "learning_rate": 3.2974118393988366e-05,
      "loss": 2.7777,
      "step": 4353
    },
    {
      "epoch": 0.39675596865318025,
      "grad_norm": 1.3509880304336548,
      "learning_rate": 3.296733499703025e-05,
      "loss": 2.8168,
      "step": 4354
    },
    {
      "epoch": 0.3968470931292145,
      "grad_norm": 1.8954664468765259,
      "learning_rate": 3.296055094711764e-05,
      "loss": 2.9554,
      "step": 4355
    },
    {
      "epoch": 0.3969382176052488,
      "grad_norm": 2.103806734085083,
      "learning_rate": 3.295376624480653e-05,
      "loss": 2.7641,
      "step": 4356
    },
    {
      "epoch": 0.39702934208128304,
      "grad_norm": 1.1327093839645386,
      "learning_rate": 3.294698089065295e-05,
      "loss": 2.8389,
      "step": 4357
    },
    {
      "epoch": 0.3971204665573173,
      "grad_norm": 1.1658673286437988,
      "learning_rate": 3.2940194885212995e-05,
      "loss": 2.9173,
      "step": 4358
    },
    {
      "epoch": 0.3972115910333516,
      "grad_norm": 4.052643775939941,
      "learning_rate": 3.2933408229042785e-05,
      "loss": 2.6345,
      "step": 4359
    },
    {
      "epoch": 0.39730271550938584,
      "grad_norm": 2.609099864959717,
      "learning_rate": 3.292662092269852e-05,
      "loss": 2.8668,
      "step": 4360
    },
    {
      "epoch": 0.3973938399854201,
      "grad_norm": 2.2896339893341064,
      "learning_rate": 3.2919832966736453e-05,
      "loss": 2.9833,
      "step": 4361
    },
    {
      "epoch": 0.39748496446145437,
      "grad_norm": 1.806899070739746,
      "learning_rate": 3.291304436171287e-05,
      "loss": 3.0806,
      "step": 4362
    },
    {
      "epoch": 0.39757608893748864,
      "grad_norm": 1.135959506034851,
      "learning_rate": 3.2906255108184157e-05,
      "loss": 2.8721,
      "step": 4363
    },
    {
      "epoch": 0.39766721341352285,
      "grad_norm": 1.8248028755187988,
      "learning_rate": 3.289946520670668e-05,
      "loss": 2.3318,
      "step": 4364
    },
    {
      "epoch": 0.3977583378895571,
      "grad_norm": 2.1132678985595703,
      "learning_rate": 3.289267465783693e-05,
      "loss": 2.9245,
      "step": 4365
    },
    {
      "epoch": 0.3978494623655914,
      "grad_norm": 1.584977626800537,
      "learning_rate": 3.2885883462131394e-05,
      "loss": 2.9067,
      "step": 4366
    },
    {
      "epoch": 0.39794058684162564,
      "grad_norm": 1.6566323041915894,
      "learning_rate": 3.2879091620146645e-05,
      "loss": 2.8745,
      "step": 4367
    },
    {
      "epoch": 0.3980317113176599,
      "grad_norm": 2.5217409133911133,
      "learning_rate": 3.287229913243931e-05,
      "loss": 2.4871,
      "step": 4368
    },
    {
      "epoch": 0.3981228357936942,
      "grad_norm": 3.132434844970703,
      "learning_rate": 3.286550599956605e-05,
      "loss": 2.9405,
      "step": 4369
    },
    {
      "epoch": 0.39821396026972844,
      "grad_norm": 1.8794810771942139,
      "learning_rate": 3.28587122220836e-05,
      "loss": 2.9239,
      "step": 4370
    },
    {
      "epoch": 0.3983050847457627,
      "grad_norm": 1.9061259031295776,
      "learning_rate": 3.2851917800548724e-05,
      "loss": 2.845,
      "step": 4371
    },
    {
      "epoch": 0.39839620922179697,
      "grad_norm": 1.9400830268859863,
      "learning_rate": 3.2845122735518256e-05,
      "loss": 2.8944,
      "step": 4372
    },
    {
      "epoch": 0.39848733369783124,
      "grad_norm": 1.9702918529510498,
      "learning_rate": 3.283832702754909e-05,
      "loss": 3.0366,
      "step": 4373
    },
    {
      "epoch": 0.3985784581738655,
      "grad_norm": 2.8022499084472656,
      "learning_rate": 3.283153067719814e-05,
      "loss": 2.5294,
      "step": 4374
    },
    {
      "epoch": 0.39866958264989977,
      "grad_norm": 1.0661141872406006,
      "learning_rate": 3.282473368502241e-05,
      "loss": 2.755,
      "step": 4375
    },
    {
      "epoch": 0.39876070712593403,
      "grad_norm": 2.361636161804199,
      "learning_rate": 3.2817936051578934e-05,
      "loss": 2.979,
      "step": 4376
    },
    {
      "epoch": 0.3988518316019683,
      "grad_norm": 1.8393009901046753,
      "learning_rate": 3.281113777742481e-05,
      "loss": 2.8588,
      "step": 4377
    },
    {
      "epoch": 0.39894295607800256,
      "grad_norm": 2.3095669746398926,
      "learning_rate": 3.280433886311717e-05,
      "loss": 4.1212,
      "step": 4378
    },
    {
      "epoch": 0.39903408055403683,
      "grad_norm": 1.7479196786880493,
      "learning_rate": 3.279753930921323e-05,
      "loss": 3.0738,
      "step": 4379
    },
    {
      "epoch": 0.3991252050300711,
      "grad_norm": 1.6373766660690308,
      "learning_rate": 3.2790739116270225e-05,
      "loss": 2.8267,
      "step": 4380
    },
    {
      "epoch": 0.39921632950610536,
      "grad_norm": 1.9771066904067993,
      "learning_rate": 3.2783938284845464e-05,
      "loss": 2.8022,
      "step": 4381
    },
    {
      "epoch": 0.3993074539821396,
      "grad_norm": 2.413102626800537,
      "learning_rate": 3.277713681549631e-05,
      "loss": 4.4526,
      "step": 4382
    },
    {
      "epoch": 0.3993985784581739,
      "grad_norm": 2.8796029090881348,
      "learning_rate": 3.277033470878016e-05,
      "loss": 3.9437,
      "step": 4383
    },
    {
      "epoch": 0.3994897029342081,
      "grad_norm": 2.0225090980529785,
      "learning_rate": 3.276353196525447e-05,
      "loss": 2.7655,
      "step": 4384
    },
    {
      "epoch": 0.39958082741024237,
      "grad_norm": 1.67082679271698,
      "learning_rate": 3.275672858547676e-05,
      "loss": 2.9582,
      "step": 4385
    },
    {
      "epoch": 0.39967195188627663,
      "grad_norm": 2.3526885509490967,
      "learning_rate": 3.274992457000459e-05,
      "loss": 2.9731,
      "step": 4386
    },
    {
      "epoch": 0.3997630763623109,
      "grad_norm": 1.25216805934906,
      "learning_rate": 3.2743119919395575e-05,
      "loss": 2.9272,
      "step": 4387
    },
    {
      "epoch": 0.39985420083834516,
      "grad_norm": 3.6884124279022217,
      "learning_rate": 3.2736314634207387e-05,
      "loss": 3.4803,
      "step": 4388
    },
    {
      "epoch": 0.39994532531437943,
      "grad_norm": 1.7297048568725586,
      "learning_rate": 3.272950871499775e-05,
      "loss": 2.5795,
      "step": 4389
    },
    {
      "epoch": 0.4000364497904137,
      "grad_norm": 1.9093859195709229,
      "learning_rate": 3.2722702162324415e-05,
      "loss": 2.9224,
      "step": 4390
    },
    {
      "epoch": 0.40012757426644796,
      "grad_norm": 1.2085903882980347,
      "learning_rate": 3.271589497674523e-05,
      "loss": 2.7768,
      "step": 4391
    },
    {
      "epoch": 0.4002186987424822,
      "grad_norm": 1.5012298822402954,
      "learning_rate": 3.2709087158818054e-05,
      "loss": 2.8245,
      "step": 4392
    },
    {
      "epoch": 0.4003098232185165,
      "grad_norm": 2.0188727378845215,
      "learning_rate": 3.270227870910083e-05,
      "loss": 2.7462,
      "step": 4393
    },
    {
      "epoch": 0.40040094769455076,
      "grad_norm": 2.706233024597168,
      "learning_rate": 3.269546962815152e-05,
      "loss": 2.6839,
      "step": 4394
    },
    {
      "epoch": 0.400492072170585,
      "grad_norm": 2.6421377658843994,
      "learning_rate": 3.268865991652817e-05,
      "loss": 2.5787,
      "step": 4395
    },
    {
      "epoch": 0.4005831966466193,
      "grad_norm": 1.5512365102767944,
      "learning_rate": 3.268184957478885e-05,
      "loss": 2.6775,
      "step": 4396
    },
    {
      "epoch": 0.40067432112265355,
      "grad_norm": 2.2817323207855225,
      "learning_rate": 3.26750386034917e-05,
      "loss": 2.9501,
      "step": 4397
    },
    {
      "epoch": 0.4007654455986878,
      "grad_norm": 1.805039882659912,
      "learning_rate": 3.266822700319492e-05,
      "loss": 2.7609,
      "step": 4398
    },
    {
      "epoch": 0.4008565700747221,
      "grad_norm": 1.8230581283569336,
      "learning_rate": 3.266141477445672e-05,
      "loss": 2.9147,
      "step": 4399
    },
    {
      "epoch": 0.40094769455075635,
      "grad_norm": 1.8851603269577026,
      "learning_rate": 3.2654601917835415e-05,
      "loss": 2.9569,
      "step": 4400
    },
    {
      "epoch": 0.4010388190267906,
      "grad_norm": 1.5952259302139282,
      "learning_rate": 3.2647788433889324e-05,
      "loss": 2.9186,
      "step": 4401
    },
    {
      "epoch": 0.4011299435028249,
      "grad_norm": 4.641587734222412,
      "learning_rate": 3.2640974323176846e-05,
      "loss": 4.1273,
      "step": 4402
    },
    {
      "epoch": 0.40122106797885915,
      "grad_norm": 1.1636343002319336,
      "learning_rate": 3.2634159586256434e-05,
      "loss": 2.8288,
      "step": 4403
    },
    {
      "epoch": 0.40131219245489336,
      "grad_norm": 2.2497575283050537,
      "learning_rate": 3.2627344223686574e-05,
      "loss": 2.9929,
      "step": 4404
    },
    {
      "epoch": 0.4014033169309276,
      "grad_norm": 2.6345934867858887,
      "learning_rate": 3.262052823602581e-05,
      "loss": 2.9656,
      "step": 4405
    },
    {
      "epoch": 0.4014944414069619,
      "grad_norm": 2.1121745109558105,
      "learning_rate": 3.2613711623832745e-05,
      "loss": 3.0488,
      "step": 4406
    },
    {
      "epoch": 0.40158556588299615,
      "grad_norm": 1.3148226737976074,
      "learning_rate": 3.2606894387666026e-05,
      "loss": 2.777,
      "step": 4407
    },
    {
      "epoch": 0.4016766903590304,
      "grad_norm": 2.3494622707366943,
      "learning_rate": 3.2600076528084345e-05,
      "loss": 2.9852,
      "step": 4408
    },
    {
      "epoch": 0.4017678148350647,
      "grad_norm": 1.4944117069244385,
      "learning_rate": 3.259325804564646e-05,
      "loss": 2.8425,
      "step": 4409
    },
    {
      "epoch": 0.40185893931109895,
      "grad_norm": 1.3485617637634277,
      "learning_rate": 3.258643894091117e-05,
      "loss": 2.9207,
      "step": 4410
    },
    {
      "epoch": 0.4019500637871332,
      "grad_norm": 5.799379348754883,
      "learning_rate": 3.257961921443733e-05,
      "loss": 2.7909,
      "step": 4411
    },
    {
      "epoch": 0.4020411882631675,
      "grad_norm": 2.2868027687072754,
      "learning_rate": 3.257279886678383e-05,
      "loss": 2.6756,
      "step": 4412
    },
    {
      "epoch": 0.40213231273920175,
      "grad_norm": 2.308295965194702,
      "learning_rate": 3.256597789850965e-05,
      "loss": 2.761,
      "step": 4413
    },
    {
      "epoch": 0.402223437215236,
      "grad_norm": 2.098550319671631,
      "learning_rate": 3.255915631017378e-05,
      "loss": 2.7285,
      "step": 4414
    },
    {
      "epoch": 0.4023145616912703,
      "grad_norm": 2.424180746078491,
      "learning_rate": 3.255233410233527e-05,
      "loss": 4.0718,
      "step": 4415
    },
    {
      "epoch": 0.40240568616730454,
      "grad_norm": 4.364797592163086,
      "learning_rate": 3.2545511275553234e-05,
      "loss": 2.8614,
      "step": 4416
    },
    {
      "epoch": 0.4024968106433388,
      "grad_norm": 1.0432000160217285,
      "learning_rate": 3.253868783038683e-05,
      "loss": 2.6593,
      "step": 4417
    },
    {
      "epoch": 0.4025879351193731,
      "grad_norm": 2.2880938053131104,
      "learning_rate": 3.2531863767395274e-05,
      "loss": 3.1349,
      "step": 4418
    },
    {
      "epoch": 0.40267905959540734,
      "grad_norm": 1.1159415245056152,
      "learning_rate": 3.252503908713781e-05,
      "loss": 2.7978,
      "step": 4419
    },
    {
      "epoch": 0.4027701840714416,
      "grad_norm": 2.286468744277954,
      "learning_rate": 3.251821379017375e-05,
      "loss": 2.6583,
      "step": 4420
    },
    {
      "epoch": 0.40286130854747587,
      "grad_norm": 1.852563738822937,
      "learning_rate": 3.251138787706246e-05,
      "loss": 2.8652,
      "step": 4421
    },
    {
      "epoch": 0.40295243302351014,
      "grad_norm": 2.39939022064209,
      "learning_rate": 3.250456134836335e-05,
      "loss": 2.7111,
      "step": 4422
    },
    {
      "epoch": 0.4030435574995444,
      "grad_norm": 1.3026500940322876,
      "learning_rate": 3.2497734204635874e-05,
      "loss": 2.8005,
      "step": 4423
    },
    {
      "epoch": 0.40313468197557867,
      "grad_norm": 2.1428260803222656,
      "learning_rate": 3.2490906446439564e-05,
      "loss": 2.6517,
      "step": 4424
    },
    {
      "epoch": 0.4032258064516129,
      "grad_norm": 1.8353937864303589,
      "learning_rate": 3.2484078074333954e-05,
      "loss": 2.668,
      "step": 4425
    },
    {
      "epoch": 0.40331693092764714,
      "grad_norm": 1.727136492729187,
      "learning_rate": 3.247724908887867e-05,
      "loss": 2.8786,
      "step": 4426
    },
    {
      "epoch": 0.4034080554036814,
      "grad_norm": 2.2049927711486816,
      "learning_rate": 3.247041949063337e-05,
      "loss": 2.5332,
      "step": 4427
    },
    {
      "epoch": 0.4034991798797157,
      "grad_norm": 2.3069138526916504,
      "learning_rate": 3.246358928015777e-05,
      "loss": 2.6512,
      "step": 4428
    },
    {
      "epoch": 0.40359030435574994,
      "grad_norm": 1.7783092260360718,
      "learning_rate": 3.245675845801163e-05,
      "loss": 2.7163,
      "step": 4429
    },
    {
      "epoch": 0.4036814288317842,
      "grad_norm": 1.4670650959014893,
      "learning_rate": 3.2449927024754765e-05,
      "loss": 2.8857,
      "step": 4430
    },
    {
      "epoch": 0.40377255330781847,
      "grad_norm": 2.514350175857544,
      "learning_rate": 3.2443094980947045e-05,
      "loss": 2.6972,
      "step": 4431
    },
    {
      "epoch": 0.40386367778385274,
      "grad_norm": 2.5272343158721924,
      "learning_rate": 3.2436262327148365e-05,
      "loss": 2.7909,
      "step": 4432
    },
    {
      "epoch": 0.403954802259887,
      "grad_norm": 1.985368013381958,
      "learning_rate": 3.2429429063918696e-05,
      "loss": 2.8473,
      "step": 4433
    },
    {
      "epoch": 0.40404592673592127,
      "grad_norm": 4.089684009552002,
      "learning_rate": 3.242259519181806e-05,
      "loss": 3.1239,
      "step": 4434
    },
    {
      "epoch": 0.40413705121195553,
      "grad_norm": 1.9141079187393188,
      "learning_rate": 3.2415760711406506e-05,
      "loss": 3.136,
      "step": 4435
    },
    {
      "epoch": 0.4042281756879898,
      "grad_norm": 1.857908010482788,
      "learning_rate": 3.2408925623244155e-05,
      "loss": 3.0514,
      "step": 4436
    },
    {
      "epoch": 0.40431930016402406,
      "grad_norm": 4.9560747146606445,
      "learning_rate": 3.2402089927891176e-05,
      "loss": 3.049,
      "step": 4437
    },
    {
      "epoch": 0.40441042464005833,
      "grad_norm": 1.9221911430358887,
      "learning_rate": 3.239525362590777e-05,
      "loss": 3.0009,
      "step": 4438
    },
    {
      "epoch": 0.4045015491160926,
      "grad_norm": 1.8919742107391357,
      "learning_rate": 3.238841671785419e-05,
      "loss": 2.8946,
      "step": 4439
    },
    {
      "epoch": 0.40459267359212686,
      "grad_norm": 1.4872406721115112,
      "learning_rate": 3.238157920429077e-05,
      "loss": 3.0688,
      "step": 4440
    },
    {
      "epoch": 0.4046837980681611,
      "grad_norm": 2.3813068866729736,
      "learning_rate": 3.237474108577785e-05,
      "loss": 2.9606,
      "step": 4441
    },
    {
      "epoch": 0.4047749225441954,
      "grad_norm": 1.3793668746948242,
      "learning_rate": 3.236790236287587e-05,
      "loss": 2.8101,
      "step": 4442
    },
    {
      "epoch": 0.40486604702022966,
      "grad_norm": 1.2744842767715454,
      "learning_rate": 3.236106303614526e-05,
      "loss": 2.8166,
      "step": 4443
    },
    {
      "epoch": 0.4049571714962639,
      "grad_norm": 1.936646580696106,
      "learning_rate": 3.2354223106146536e-05,
      "loss": 2.8595,
      "step": 4444
    },
    {
      "epoch": 0.40504829597229813,
      "grad_norm": 1.3184117078781128,
      "learning_rate": 3.234738257344027e-05,
      "loss": 2.8135,
      "step": 4445
    },
    {
      "epoch": 0.4051394204483324,
      "grad_norm": 1.3127785921096802,
      "learning_rate": 3.234054143858706e-05,
      "loss": 2.8848,
      "step": 4446
    },
    {
      "epoch": 0.40523054492436666,
      "grad_norm": 3.6906988620758057,
      "learning_rate": 3.2333699702147564e-05,
      "loss": 3.12,
      "step": 4447
    },
    {
      "epoch": 0.40532166940040093,
      "grad_norm": 1.7647302150726318,
      "learning_rate": 3.2326857364682486e-05,
      "loss": 2.7964,
      "step": 4448
    },
    {
      "epoch": 0.4054127938764352,
      "grad_norm": 1.186546802520752,
      "learning_rate": 3.23200144267526e-05,
      "loss": 2.8803,
      "step": 4449
    },
    {
      "epoch": 0.40550391835246946,
      "grad_norm": 1.7396302223205566,
      "learning_rate": 3.23131708889187e-05,
      "loss": 2.7151,
      "step": 4450
    },
    {
      "epoch": 0.4055950428285037,
      "grad_norm": 1.6440458297729492,
      "learning_rate": 3.2306326751741636e-05,
      "loss": 2.7791,
      "step": 4451
    },
    {
      "epoch": 0.405686167304538,
      "grad_norm": 1.8522855043411255,
      "learning_rate": 3.229948201578232e-05,
      "loss": 2.7659,
      "step": 4452
    },
    {
      "epoch": 0.40577729178057226,
      "grad_norm": 1.192237377166748,
      "learning_rate": 3.2292636681601695e-05,
      "loss": 2.7728,
      "step": 4453
    },
    {
      "epoch": 0.4058684162566065,
      "grad_norm": 1.0489412546157837,
      "learning_rate": 3.228579074976078e-05,
      "loss": 2.7927,
      "step": 4454
    },
    {
      "epoch": 0.4059595407326408,
      "grad_norm": 2.1448814868927,
      "learning_rate": 3.2278944220820616e-05,
      "loss": 2.9884,
      "step": 4455
    },
    {
      "epoch": 0.40605066520867505,
      "grad_norm": 1.534974217414856,
      "learning_rate": 3.22720970953423e-05,
      "loss": 2.8205,
      "step": 4456
    },
    {
      "epoch": 0.4061417896847093,
      "grad_norm": 1.369764804840088,
      "learning_rate": 3.226524937388698e-05,
      "loss": 2.8066,
      "step": 4457
    },
    {
      "epoch": 0.4062329141607436,
      "grad_norm": 2.239865779876709,
      "learning_rate": 3.225840105701587e-05,
      "loss": 2.7398,
      "step": 4458
    },
    {
      "epoch": 0.40632403863677785,
      "grad_norm": 2.112170934677124,
      "learning_rate": 3.225155214529019e-05,
      "loss": 3.1131,
      "step": 4459
    },
    {
      "epoch": 0.4064151631128121,
      "grad_norm": 2.8207359313964844,
      "learning_rate": 3.2244702639271254e-05,
      "loss": 2.9188,
      "step": 4460
    },
    {
      "epoch": 0.4065062875888464,
      "grad_norm": 2.6152524948120117,
      "learning_rate": 3.223785253952041e-05,
      "loss": 2.7861,
      "step": 4461
    },
    {
      "epoch": 0.40659741206488065,
      "grad_norm": 2.6737942695617676,
      "learning_rate": 3.223100184659903e-05,
      "loss": 2.9854,
      "step": 4462
    },
    {
      "epoch": 0.4066885365409149,
      "grad_norm": 1.9513267278671265,
      "learning_rate": 3.222415056106857e-05,
      "loss": 2.943,
      "step": 4463
    },
    {
      "epoch": 0.4067796610169492,
      "grad_norm": 1.3829379081726074,
      "learning_rate": 3.2217298683490525e-05,
      "loss": 2.8262,
      "step": 4464
    },
    {
      "epoch": 0.4068707854929834,
      "grad_norm": 1.976520299911499,
      "learning_rate": 3.221044621442643e-05,
      "loss": 2.8414,
      "step": 4465
    },
    {
      "epoch": 0.40696190996901765,
      "grad_norm": 2.3137247562408447,
      "learning_rate": 3.2203593154437856e-05,
      "loss": 2.7224,
      "step": 4466
    },
    {
      "epoch": 0.4070530344450519,
      "grad_norm": 2.160423994064331,
      "learning_rate": 3.2196739504086454e-05,
      "loss": 2.9763,
      "step": 4467
    },
    {
      "epoch": 0.4071441589210862,
      "grad_norm": 1.6264245510101318,
      "learning_rate": 3.218988526393391e-05,
      "loss": 2.9002,
      "step": 4468
    },
    {
      "epoch": 0.40723528339712045,
      "grad_norm": 1.1554653644561768,
      "learning_rate": 3.218303043454194e-05,
      "loss": 2.7822,
      "step": 4469
    },
    {
      "epoch": 0.4073264078731547,
      "grad_norm": 2.8829667568206787,
      "learning_rate": 3.2176175016472335e-05,
      "loss": 4.2981,
      "step": 4470
    },
    {
      "epoch": 0.407417532349189,
      "grad_norm": 1.8908452987670898,
      "learning_rate": 3.2169319010286926e-05,
      "loss": 2.9185,
      "step": 4471
    },
    {
      "epoch": 0.40750865682522325,
      "grad_norm": 2.1926820278167725,
      "learning_rate": 3.2162462416547586e-05,
      "loss": 2.6298,
      "step": 4472
    },
    {
      "epoch": 0.4075997813012575,
      "grad_norm": 2.5904009342193604,
      "learning_rate": 3.2155605235816244e-05,
      "loss": 2.7351,
      "step": 4473
    },
    {
      "epoch": 0.4076909057772918,
      "grad_norm": 2.1665830612182617,
      "learning_rate": 3.214874746865486e-05,
      "loss": 2.4203,
      "step": 4474
    },
    {
      "epoch": 0.40778203025332604,
      "grad_norm": 1.0418517589569092,
      "learning_rate": 3.214188911562547e-05,
      "loss": 2.7712,
      "step": 4475
    },
    {
      "epoch": 0.4078731547293603,
      "grad_norm": 1.126806616783142,
      "learning_rate": 3.2135030177290134e-05,
      "loss": 2.7533,
      "step": 4476
    },
    {
      "epoch": 0.4079642792053946,
      "grad_norm": 2.1458215713500977,
      "learning_rate": 3.212817065421097e-05,
      "loss": 2.6355,
      "step": 4477
    },
    {
      "epoch": 0.40805540368142884,
      "grad_norm": 1.577311635017395,
      "learning_rate": 3.212131054695015e-05,
      "loss": 2.5151,
      "step": 4478
    },
    {
      "epoch": 0.4081465281574631,
      "grad_norm": 2.128000020980835,
      "learning_rate": 3.211444985606989e-05,
      "loss": 2.8862,
      "step": 4479
    },
    {
      "epoch": 0.40823765263349737,
      "grad_norm": 1.8151744604110718,
      "learning_rate": 3.2107588582132424e-05,
      "loss": 2.9314,
      "step": 4480
    },
    {
      "epoch": 0.40832877710953164,
      "grad_norm": 2.455010175704956,
      "learning_rate": 3.2100726725700086e-05,
      "loss": 4.1721,
      "step": 4481
    },
    {
      "epoch": 0.4084199015855659,
      "grad_norm": 1.8307721614837646,
      "learning_rate": 3.209386428733523e-05,
      "loss": 2.9426,
      "step": 4482
    },
    {
      "epoch": 0.40851102606160017,
      "grad_norm": 3.117290735244751,
      "learning_rate": 3.208700126760025e-05,
      "loss": 2.7759,
      "step": 4483
    },
    {
      "epoch": 0.40860215053763443,
      "grad_norm": 2.702833652496338,
      "learning_rate": 3.2080137667057595e-05,
      "loss": 2.9405,
      "step": 4484
    },
    {
      "epoch": 0.40869327501366864,
      "grad_norm": 3.6786367893218994,
      "learning_rate": 3.2073273486269787e-05,
      "loss": 3.0893,
      "step": 4485
    },
    {
      "epoch": 0.4087843994897029,
      "grad_norm": 2.0820255279541016,
      "learning_rate": 3.2066408725799355e-05,
      "loss": 3.0297,
      "step": 4486
    },
    {
      "epoch": 0.4088755239657372,
      "grad_norm": 2.224792718887329,
      "learning_rate": 3.205954338620889e-05,
      "loss": 2.4835,
      "step": 4487
    },
    {
      "epoch": 0.40896664844177144,
      "grad_norm": 2.201183795928955,
      "learning_rate": 3.205267746806104e-05,
      "loss": 3.1903,
      "step": 4488
    },
    {
      "epoch": 0.4090577729178057,
      "grad_norm": 1.2779878377914429,
      "learning_rate": 3.204581097191849e-05,
      "loss": 2.7517,
      "step": 4489
    },
    {
      "epoch": 0.40914889739383997,
      "grad_norm": 2.08199405670166,
      "learning_rate": 3.203894389834398e-05,
      "loss": 2.9577,
      "step": 4490
    },
    {
      "epoch": 0.40924002186987424,
      "grad_norm": 1.554548978805542,
      "learning_rate": 3.20320762479003e-05,
      "loss": 2.8705,
      "step": 4491
    },
    {
      "epoch": 0.4093311463459085,
      "grad_norm": 1.3128666877746582,
      "learning_rate": 3.2025208021150265e-05,
      "loss": 2.9257,
      "step": 4492
    },
    {
      "epoch": 0.40942227082194277,
      "grad_norm": 1.9608087539672852,
      "learning_rate": 3.201833921865677e-05,
      "loss": 2.8411,
      "step": 4493
    },
    {
      "epoch": 0.40951339529797703,
      "grad_norm": 2.038268804550171,
      "learning_rate": 3.2011469840982725e-05,
      "loss": 2.8486,
      "step": 4494
    },
    {
      "epoch": 0.4096045197740113,
      "grad_norm": 2.1823034286499023,
      "learning_rate": 3.200459988869111e-05,
      "loss": 3.0369,
      "step": 4495
    },
    {
      "epoch": 0.40969564425004557,
      "grad_norm": 2.4450623989105225,
      "learning_rate": 3.199772936234495e-05,
      "loss": 3.0696,
      "step": 4496
    },
    {
      "epoch": 0.40978676872607983,
      "grad_norm": 2.1355745792388916,
      "learning_rate": 3.199085826250731e-05,
      "loss": 2.7756,
      "step": 4497
    },
    {
      "epoch": 0.4098778932021141,
      "grad_norm": 3.498073101043701,
      "learning_rate": 3.19839865897413e-05,
      "loss": 2.4934,
      "step": 4498
    },
    {
      "epoch": 0.40996901767814836,
      "grad_norm": 2.177896022796631,
      "learning_rate": 3.197711434461007e-05,
      "loss": 2.9129,
      "step": 4499
    },
    {
      "epoch": 0.4100601421541826,
      "grad_norm": 2.8647849559783936,
      "learning_rate": 3.197024152767685e-05,
      "loss": 2.1755,
      "step": 4500
    },
    {
      "epoch": 0.4101512666302169,
      "grad_norm": 1.1693389415740967,
      "learning_rate": 3.1963368139504875e-05,
      "loss": 2.7643,
      "step": 4501
    },
    {
      "epoch": 0.41024239110625116,
      "grad_norm": 1.8377412557601929,
      "learning_rate": 3.195649418065746e-05,
      "loss": 2.9447,
      "step": 4502
    },
    {
      "epoch": 0.4103335155822854,
      "grad_norm": 2.280611753463745,
      "learning_rate": 3.194961965169794e-05,
      "loss": 2.927,
      "step": 4503
    },
    {
      "epoch": 0.4104246400583197,
      "grad_norm": 2.8796348571777344,
      "learning_rate": 3.1942744553189725e-05,
      "loss": 3.183,
      "step": 4504
    },
    {
      "epoch": 0.41051576453435396,
      "grad_norm": 1.1690211296081543,
      "learning_rate": 3.193586888569625e-05,
      "loss": 2.7576,
      "step": 4505
    },
    {
      "epoch": 0.41060688901038817,
      "grad_norm": 2.1534738540649414,
      "learning_rate": 3.1928992649781e-05,
      "loss": 3.3935,
      "step": 4506
    },
    {
      "epoch": 0.41069801348642243,
      "grad_norm": 1.8495118618011475,
      "learning_rate": 3.19221158460075e-05,
      "loss": 2.8795,
      "step": 4507
    },
    {
      "epoch": 0.4107891379624567,
      "grad_norm": 2.0038235187530518,
      "learning_rate": 3.191523847493935e-05,
      "loss": 2.7737,
      "step": 4508
    },
    {
      "epoch": 0.41088026243849096,
      "grad_norm": 1.5462512969970703,
      "learning_rate": 3.190836053714017e-05,
      "loss": 2.8547,
      "step": 4509
    },
    {
      "epoch": 0.4109713869145252,
      "grad_norm": 1.071761965751648,
      "learning_rate": 3.1901482033173636e-05,
      "loss": 2.6974,
      "step": 4510
    },
    {
      "epoch": 0.4110625113905595,
      "grad_norm": 1.6600842475891113,
      "learning_rate": 3.189460296360346e-05,
      "loss": 2.7601,
      "step": 4511
    },
    {
      "epoch": 0.41115363586659376,
      "grad_norm": 2.3412249088287354,
      "learning_rate": 3.188772332899342e-05,
      "loss": 2.9457,
      "step": 4512
    },
    {
      "epoch": 0.411244760342628,
      "grad_norm": 1.0086255073547363,
      "learning_rate": 3.188084312990732e-05,
      "loss": 2.7503,
      "step": 4513
    },
    {
      "epoch": 0.4113358848186623,
      "grad_norm": 1.7373006343841553,
      "learning_rate": 3.187396236690903e-05,
      "loss": 2.8293,
      "step": 4514
    },
    {
      "epoch": 0.41142700929469656,
      "grad_norm": 2.282508611679077,
      "learning_rate": 3.186708104056245e-05,
      "loss": 2.6871,
      "step": 4515
    },
    {
      "epoch": 0.4115181337707308,
      "grad_norm": 1.9477885961532593,
      "learning_rate": 3.186019915143152e-05,
      "loss": 2.9617,
      "step": 4516
    },
    {
      "epoch": 0.4116092582467651,
      "grad_norm": 2.788564920425415,
      "learning_rate": 3.185331670008026e-05,
      "loss": 4.0408,
      "step": 4517
    },
    {
      "epoch": 0.41170038272279935,
      "grad_norm": 3.1527233123779297,
      "learning_rate": 3.18464336870727e-05,
      "loss": 2.7471,
      "step": 4518
    },
    {
      "epoch": 0.4117915071988336,
      "grad_norm": 1.5687060356140137,
      "learning_rate": 3.183955011297293e-05,
      "loss": 2.9212,
      "step": 4519
    },
    {
      "epoch": 0.4118826316748679,
      "grad_norm": 2.474637746810913,
      "learning_rate": 3.1832665978345086e-05,
      "loss": 3.0837,
      "step": 4520
    },
    {
      "epoch": 0.41197375615090215,
      "grad_norm": 1.754908561706543,
      "learning_rate": 3.1825781283753365e-05,
      "loss": 2.7054,
      "step": 4521
    },
    {
      "epoch": 0.4120648806269364,
      "grad_norm": 2.8179144859313965,
      "learning_rate": 3.181889602976198e-05,
      "loss": 2.6962,
      "step": 4522
    },
    {
      "epoch": 0.4121560051029707,
      "grad_norm": 2.8330836296081543,
      "learning_rate": 3.1812010216935194e-05,
      "loss": 2.834,
      "step": 4523
    },
    {
      "epoch": 0.41224712957900495,
      "grad_norm": 1.6644024848937988,
      "learning_rate": 3.180512384583735e-05,
      "loss": 2.9137,
      "step": 4524
    },
    {
      "epoch": 0.4123382540550392,
      "grad_norm": 2.9086010456085205,
      "learning_rate": 3.17982369170328e-05,
      "loss": 3.0624,
      "step": 4525
    },
    {
      "epoch": 0.4124293785310734,
      "grad_norm": 1.1048904657363892,
      "learning_rate": 3.179134943108597e-05,
      "loss": 2.714,
      "step": 4526
    },
    {
      "epoch": 0.4125205030071077,
      "grad_norm": 2.089932918548584,
      "learning_rate": 3.17844613885613e-05,
      "loss": 2.9938,
      "step": 4527
    },
    {
      "epoch": 0.41261162748314195,
      "grad_norm": 2.32454514503479,
      "learning_rate": 3.1777572790023295e-05,
      "loss": 2.544,
      "step": 4528
    },
    {
      "epoch": 0.4127027519591762,
      "grad_norm": 1.1051408052444458,
      "learning_rate": 3.17706836360365e-05,
      "loss": 2.7575,
      "step": 4529
    },
    {
      "epoch": 0.4127938764352105,
      "grad_norm": 2.997105360031128,
      "learning_rate": 3.176379392716552e-05,
      "loss": 2.8883,
      "step": 4530
    },
    {
      "epoch": 0.41288500091124475,
      "grad_norm": 2.79752779006958,
      "learning_rate": 3.175690366397498e-05,
      "loss": 2.7789,
      "step": 4531
    },
    {
      "epoch": 0.412976125387279,
      "grad_norm": 1.5945277214050293,
      "learning_rate": 3.175001284702958e-05,
      "loss": 2.8983,
      "step": 4532
    },
    {
      "epoch": 0.4130672498633133,
      "grad_norm": 2.0927162170410156,
      "learning_rate": 3.1743121476894025e-05,
      "loss": 2.9372,
      "step": 4533
    },
    {
      "epoch": 0.41315837433934754,
      "grad_norm": 2.5365045070648193,
      "learning_rate": 3.1736229554133124e-05,
      "loss": 2.5126,
      "step": 4534
    },
    {
      "epoch": 0.4132494988153818,
      "grad_norm": 1.2347599267959595,
      "learning_rate": 3.1729337079311675e-05,
      "loss": 2.7527,
      "step": 4535
    },
    {
      "epoch": 0.4133406232914161,
      "grad_norm": 2.2072315216064453,
      "learning_rate": 3.1722444052994535e-05,
      "loss": 3.0286,
      "step": 4536
    },
    {
      "epoch": 0.41343174776745034,
      "grad_norm": 2.255784273147583,
      "learning_rate": 3.1715550475746634e-05,
      "loss": 2.5061,
      "step": 4537
    },
    {
      "epoch": 0.4135228722434846,
      "grad_norm": 3.299997568130493,
      "learning_rate": 3.170865634813292e-05,
      "loss": 2.9705,
      "step": 4538
    },
    {
      "epoch": 0.4136139967195189,
      "grad_norm": 1.9700467586517334,
      "learning_rate": 3.1701761670718406e-05,
      "loss": 2.6186,
      "step": 4539
    },
    {
      "epoch": 0.41370512119555314,
      "grad_norm": 1.8142222166061401,
      "learning_rate": 3.1694866444068116e-05,
      "loss": 2.7495,
      "step": 4540
    },
    {
      "epoch": 0.4137962456715874,
      "grad_norm": 1.8268600702285767,
      "learning_rate": 3.168797066874715e-05,
      "loss": 2.6537,
      "step": 4541
    },
    {
      "epoch": 0.41388737014762167,
      "grad_norm": 2.058959484100342,
      "learning_rate": 3.168107434532065e-05,
      "loss": 2.9575,
      "step": 4542
    },
    {
      "epoch": 0.41397849462365593,
      "grad_norm": 2.1766836643218994,
      "learning_rate": 3.167417747435379e-05,
      "loss": 2.8253,
      "step": 4543
    },
    {
      "epoch": 0.4140696190996902,
      "grad_norm": 2.2068872451782227,
      "learning_rate": 3.1667280056411804e-05,
      "loss": 2.7326,
      "step": 4544
    },
    {
      "epoch": 0.41416074357572447,
      "grad_norm": 3.057389736175537,
      "learning_rate": 3.1660382092059956e-05,
      "loss": 2.9606,
      "step": 4545
    },
    {
      "epoch": 0.4142518680517587,
      "grad_norm": 1.6699577569961548,
      "learning_rate": 3.165348358186356e-05,
      "loss": 2.8725,
      "step": 4546
    },
    {
      "epoch": 0.41434299252779294,
      "grad_norm": 1.284726858139038,
      "learning_rate": 3.164658452638798e-05,
      "loss": 2.8772,
      "step": 4547
    },
    {
      "epoch": 0.4144341170038272,
      "grad_norm": 2.9582924842834473,
      "learning_rate": 3.163968492619861e-05,
      "loss": 2.6839,
      "step": 4548
    },
    {
      "epoch": 0.4145252414798615,
      "grad_norm": 1.4855778217315674,
      "learning_rate": 3.1632784781860914e-05,
      "loss": 2.7526,
      "step": 4549
    },
    {
      "epoch": 0.41461636595589574,
      "grad_norm": 1.7183122634887695,
      "learning_rate": 3.162588409394038e-05,
      "loss": 2.7219,
      "step": 4550
    },
    {
      "epoch": 0.41470749043193,
      "grad_norm": 2.118049383163452,
      "learning_rate": 3.161898286300254e-05,
      "loss": 2.9009,
      "step": 4551
    },
    {
      "epoch": 0.41479861490796427,
      "grad_norm": 3.133176803588867,
      "learning_rate": 3.1612081089613e-05,
      "loss": 2.8757,
      "step": 4552
    },
    {
      "epoch": 0.41488973938399853,
      "grad_norm": 1.841657042503357,
      "learning_rate": 3.160517877433736e-05,
      "loss": 2.9001,
      "step": 4553
    },
    {
      "epoch": 0.4149808638600328,
      "grad_norm": 1.3739827871322632,
      "learning_rate": 3.159827591774131e-05,
      "loss": 2.805,
      "step": 4554
    },
    {
      "epoch": 0.41507198833606707,
      "grad_norm": 1.739086627960205,
      "learning_rate": 3.1591372520390555e-05,
      "loss": 2.8619,
      "step": 4555
    },
    {
      "epoch": 0.41516311281210133,
      "grad_norm": 2.3171238899230957,
      "learning_rate": 3.158446858285086e-05,
      "loss": 2.9566,
      "step": 4556
    },
    {
      "epoch": 0.4152542372881356,
      "grad_norm": 1.1659268140792847,
      "learning_rate": 3.157756410568803e-05,
      "loss": 2.8373,
      "step": 4557
    },
    {
      "epoch": 0.41534536176416986,
      "grad_norm": 2.0037057399749756,
      "learning_rate": 3.157065908946792e-05,
      "loss": 2.8541,
      "step": 4558
    },
    {
      "epoch": 0.41543648624020413,
      "grad_norm": 2.6407113075256348,
      "learning_rate": 3.156375353475641e-05,
      "loss": 2.8517,
      "step": 4559
    },
    {
      "epoch": 0.4155276107162384,
      "grad_norm": 2.3835625648498535,
      "learning_rate": 3.155684744211945e-05,
      "loss": 3.18,
      "step": 4560
    },
    {
      "epoch": 0.41561873519227266,
      "grad_norm": 1.6656790971755981,
      "learning_rate": 3.154994081212302e-05,
      "loss": 2.7166,
      "step": 4561
    },
    {
      "epoch": 0.4157098596683069,
      "grad_norm": 2.1433024406433105,
      "learning_rate": 3.154303364533313e-05,
      "loss": 2.9008,
      "step": 4562
    },
    {
      "epoch": 0.4158009841443412,
      "grad_norm": 1.7388198375701904,
      "learning_rate": 3.153612594231587e-05,
      "loss": 2.7296,
      "step": 4563
    },
    {
      "epoch": 0.41589210862037546,
      "grad_norm": 1.2556601762771606,
      "learning_rate": 3.1529217703637347e-05,
      "loss": 2.7922,
      "step": 4564
    },
    {
      "epoch": 0.4159832330964097,
      "grad_norm": 2.625145435333252,
      "learning_rate": 3.152230892986371e-05,
      "loss": 3.0068,
      "step": 4565
    },
    {
      "epoch": 0.41607435757244393,
      "grad_norm": 2.4203362464904785,
      "learning_rate": 3.151539962156118e-05,
      "loss": 2.9711,
      "step": 4566
    },
    {
      "epoch": 0.4161654820484782,
      "grad_norm": 2.499189615249634,
      "learning_rate": 3.1508489779295975e-05,
      "loss": 2.7412,
      "step": 4567
    },
    {
      "epoch": 0.41625660652451246,
      "grad_norm": 2.328958034515381,
      "learning_rate": 3.15015794036344e-05,
      "loss": 2.7415,
      "step": 4568
    },
    {
      "epoch": 0.41634773100054673,
      "grad_norm": 2.3216240406036377,
      "learning_rate": 3.149466849514279e-05,
      "loss": 2.2956,
      "step": 4569
    },
    {
      "epoch": 0.416438855476581,
      "grad_norm": 1.6929175853729248,
      "learning_rate": 3.1487757054387514e-05,
      "loss": 2.8717,
      "step": 4570
    },
    {
      "epoch": 0.41652997995261526,
      "grad_norm": 1.1297354698181152,
      "learning_rate": 3.148084508193499e-05,
      "loss": 2.8447,
      "step": 4571
    },
    {
      "epoch": 0.4166211044286495,
      "grad_norm": 2.3441104888916016,
      "learning_rate": 3.147393257835168e-05,
      "loss": 2.7091,
      "step": 4572
    },
    {
      "epoch": 0.4167122289046838,
      "grad_norm": 1.2615569829940796,
      "learning_rate": 3.146701954420411e-05,
      "loss": 2.8053,
      "step": 4573
    },
    {
      "epoch": 0.41680335338071806,
      "grad_norm": 1.066810131072998,
      "learning_rate": 3.146010598005881e-05,
      "loss": 2.7725,
      "step": 4574
    },
    {
      "epoch": 0.4168944778567523,
      "grad_norm": 1.7798120975494385,
      "learning_rate": 3.1453191886482376e-05,
      "loss": 3.0168,
      "step": 4575
    },
    {
      "epoch": 0.4169856023327866,
      "grad_norm": 1.843639850616455,
      "learning_rate": 3.1446277264041455e-05,
      "loss": 2.7541,
      "step": 4576
    },
    {
      "epoch": 0.41707672680882085,
      "grad_norm": 1.8339529037475586,
      "learning_rate": 3.1439362113302724e-05,
      "loss": 2.8671,
      "step": 4577
    },
    {
      "epoch": 0.4171678512848551,
      "grad_norm": 1.23455810546875,
      "learning_rate": 3.14324464348329e-05,
      "loss": 2.7823,
      "step": 4578
    },
    {
      "epoch": 0.4172589757608894,
      "grad_norm": 2.4697518348693848,
      "learning_rate": 3.1425530229198755e-05,
      "loss": 2.7754,
      "step": 4579
    },
    {
      "epoch": 0.41735010023692365,
      "grad_norm": 2.117480993270874,
      "learning_rate": 3.14186134969671e-05,
      "loss": 2.8065,
      "step": 4580
    },
    {
      "epoch": 0.4174412247129579,
      "grad_norm": 2.022076368331909,
      "learning_rate": 3.141169623870479e-05,
      "loss": 2.7251,
      "step": 4581
    },
    {
      "epoch": 0.4175323491889922,
      "grad_norm": 1.3527026176452637,
      "learning_rate": 3.140477845497872e-05,
      "loss": 2.9212,
      "step": 4582
    },
    {
      "epoch": 0.41762347366502645,
      "grad_norm": 2.2644402980804443,
      "learning_rate": 3.139786014635583e-05,
      "loss": 2.7175,
      "step": 4583
    },
    {
      "epoch": 0.4177145981410607,
      "grad_norm": 1.2161213159561157,
      "learning_rate": 3.1390941313403095e-05,
      "loss": 2.6127,
      "step": 4584
    },
    {
      "epoch": 0.417805722617095,
      "grad_norm": 2.00390362739563,
      "learning_rate": 3.138402195668755e-05,
      "loss": 2.7175,
      "step": 4585
    },
    {
      "epoch": 0.4178968470931292,
      "grad_norm": 1.754068374633789,
      "learning_rate": 3.1377102076776266e-05,
      "loss": 2.91,
      "step": 4586
    },
    {
      "epoch": 0.41798797156916345,
      "grad_norm": 2.160160541534424,
      "learning_rate": 3.1370181674236344e-05,
      "loss": 2.743,
      "step": 4587
    },
    {
      "epoch": 0.4180790960451977,
      "grad_norm": 1.6743322610855103,
      "learning_rate": 3.136326074963494e-05,
      "loss": 2.8534,
      "step": 4588
    },
    {
      "epoch": 0.418170220521232,
      "grad_norm": 2.787940263748169,
      "learning_rate": 3.135633930353927e-05,
      "loss": 3.9295,
      "step": 4589
    },
    {
      "epoch": 0.41826134499726625,
      "grad_norm": 1.8593132495880127,
      "learning_rate": 3.134941733651654e-05,
      "loss": 2.9128,
      "step": 4590
    },
    {
      "epoch": 0.4183524694733005,
      "grad_norm": 2.3649837970733643,
      "learning_rate": 3.134249484913406e-05,
      "loss": 2.9199,
      "step": 4591
    },
    {
      "epoch": 0.4184435939493348,
      "grad_norm": 1.9744008779525757,
      "learning_rate": 3.1335571841959145e-05,
      "loss": 3.1337,
      "step": 4592
    },
    {
      "epoch": 0.41853471842536905,
      "grad_norm": 2.0241858959198,
      "learning_rate": 3.132864831555916e-05,
      "loss": 2.195,
      "step": 4593
    },
    {
      "epoch": 0.4186258429014033,
      "grad_norm": 1.2098608016967773,
      "learning_rate": 3.1321724270501524e-05,
      "loss": 2.7823,
      "step": 4594
    },
    {
      "epoch": 0.4187169673774376,
      "grad_norm": 1.103244423866272,
      "learning_rate": 3.131479970735368e-05,
      "loss": 2.7889,
      "step": 4595
    },
    {
      "epoch": 0.41880809185347184,
      "grad_norm": 1.0106710195541382,
      "learning_rate": 3.130787462668313e-05,
      "loss": 2.7026,
      "step": 4596
    },
    {
      "epoch": 0.4188992163295061,
      "grad_norm": 1.7852424383163452,
      "learning_rate": 3.13009490290574e-05,
      "loss": 2.8592,
      "step": 4597
    },
    {
      "epoch": 0.4189903408055404,
      "grad_norm": 1.7532740831375122,
      "learning_rate": 3.129402291504409e-05,
      "loss": 2.7029,
      "step": 4598
    },
    {
      "epoch": 0.41908146528157464,
      "grad_norm": 1.5102341175079346,
      "learning_rate": 3.128709628521081e-05,
      "loss": 2.8612,
      "step": 4599
    },
    {
      "epoch": 0.4191725897576089,
      "grad_norm": 2.0256459712982178,
      "learning_rate": 3.1280169140125224e-05,
      "loss": 3.0715,
      "step": 4600
    },
    {
      "epoch": 0.41926371423364317,
      "grad_norm": 1.8489441871643066,
      "learning_rate": 3.1273241480355044e-05,
      "loss": 2.7621,
      "step": 4601
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 2.827669382095337,
      "learning_rate": 3.126631330646802e-05,
      "loss": 2.9456,
      "step": 4602
    },
    {
      "epoch": 0.4194459631857117,
      "grad_norm": 2.019564390182495,
      "learning_rate": 3.125938461903193e-05,
      "loss": 2.8253,
      "step": 4603
    },
    {
      "epoch": 0.41953708766174597,
      "grad_norm": 2.28914737701416,
      "learning_rate": 3.1252455418614616e-05,
      "loss": 3.0144,
      "step": 4604
    },
    {
      "epoch": 0.41962821213778023,
      "grad_norm": 1.2336108684539795,
      "learning_rate": 3.1245525705783955e-05,
      "loss": 2.8774,
      "step": 4605
    },
    {
      "epoch": 0.4197193366138145,
      "grad_norm": 2.353778839111328,
      "learning_rate": 3.123859548110787e-05,
      "loss": 2.9269,
      "step": 4606
    },
    {
      "epoch": 0.4198104610898487,
      "grad_norm": 1.2510467767715454,
      "learning_rate": 3.123166474515432e-05,
      "loss": 2.7797,
      "step": 4607
    },
    {
      "epoch": 0.419901585565883,
      "grad_norm": 1.2786785364151,
      "learning_rate": 3.1224733498491286e-05,
      "loss": 2.867,
      "step": 4608
    },
    {
      "epoch": 0.41999271004191724,
      "grad_norm": 2.382572889328003,
      "learning_rate": 3.121780174168682e-05,
      "loss": 2.6464,
      "step": 4609
    },
    {
      "epoch": 0.4200838345179515,
      "grad_norm": 2.3540284633636475,
      "learning_rate": 3.1210869475309026e-05,
      "loss": 2.4685,
      "step": 4610
    },
    {
      "epoch": 0.42017495899398577,
      "grad_norm": 2.6135125160217285,
      "learning_rate": 3.1203936699926004e-05,
      "loss": 2.6856,
      "step": 4611
    },
    {
      "epoch": 0.42026608347002004,
      "grad_norm": 1.9916917085647583,
      "learning_rate": 3.119700341610593e-05,
      "loss": 2.8112,
      "step": 4612
    },
    {
      "epoch": 0.4203572079460543,
      "grad_norm": 2.6804332733154297,
      "learning_rate": 3.1190069624417036e-05,
      "loss": 2.8169,
      "step": 4613
    },
    {
      "epoch": 0.42044833242208857,
      "grad_norm": 2.0490968227386475,
      "learning_rate": 3.118313532542754e-05,
      "loss": 2.966,
      "step": 4614
    },
    {
      "epoch": 0.42053945689812283,
      "grad_norm": 1.1913549900054932,
      "learning_rate": 3.1176200519705753e-05,
      "loss": 2.7967,
      "step": 4615
    },
    {
      "epoch": 0.4206305813741571,
      "grad_norm": 1.1152830123901367,
      "learning_rate": 3.1169265207820005e-05,
      "loss": 2.8344,
      "step": 4616
    },
    {
      "epoch": 0.42072170585019136,
      "grad_norm": 2.288219690322876,
      "learning_rate": 3.1162329390338685e-05,
      "loss": 2.6506,
      "step": 4617
    },
    {
      "epoch": 0.42081283032622563,
      "grad_norm": 1.1654590368270874,
      "learning_rate": 3.11553930678302e-05,
      "loss": 2.8741,
      "step": 4618
    },
    {
      "epoch": 0.4209039548022599,
      "grad_norm": 2.273160934448242,
      "learning_rate": 3.1148456240862995e-05,
      "loss": 2.9006,
      "step": 4619
    },
    {
      "epoch": 0.42099507927829416,
      "grad_norm": 2.3380298614501953,
      "learning_rate": 3.114151891000559e-05,
      "loss": 2.2502,
      "step": 4620
    },
    {
      "epoch": 0.4210862037543284,
      "grad_norm": 1.1176081895828247,
      "learning_rate": 3.113458107582652e-05,
      "loss": 2.7707,
      "step": 4621
    },
    {
      "epoch": 0.4211773282303627,
      "grad_norm": 1.3066140413284302,
      "learning_rate": 3.112764273889437e-05,
      "loss": 2.9785,
      "step": 4622
    },
    {
      "epoch": 0.42126845270639696,
      "grad_norm": 3.08982253074646,
      "learning_rate": 3.112070389977776e-05,
      "loss": 2.7006,
      "step": 4623
    },
    {
      "epoch": 0.4213595771824312,
      "grad_norm": 2.104027032852173,
      "learning_rate": 3.1113764559045365e-05,
      "loss": 2.6663,
      "step": 4624
    },
    {
      "epoch": 0.4214507016584655,
      "grad_norm": 1.0140502452850342,
      "learning_rate": 3.110682471726588e-05,
      "loss": 2.7656,
      "step": 4625
    },
    {
      "epoch": 0.42154182613449975,
      "grad_norm": 2.158660411834717,
      "learning_rate": 3.109988437500805e-05,
      "loss": 2.9183,
      "step": 4626
    },
    {
      "epoch": 0.42163295061053396,
      "grad_norm": 2.0166332721710205,
      "learning_rate": 3.109294353284068e-05,
      "loss": 2.8336,
      "step": 4627
    },
    {
      "epoch": 0.42172407508656823,
      "grad_norm": 2.5492897033691406,
      "learning_rate": 3.108600219133258e-05,
      "loss": 2.7788,
      "step": 4628
    },
    {
      "epoch": 0.4218151995626025,
      "grad_norm": 1.8595359325408936,
      "learning_rate": 3.107906035105264e-05,
      "loss": 2.7522,
      "step": 4629
    },
    {
      "epoch": 0.42190632403863676,
      "grad_norm": 1.3182097673416138,
      "learning_rate": 3.1072118012569745e-05,
      "loss": 2.7682,
      "step": 4630
    },
    {
      "epoch": 0.421997448514671,
      "grad_norm": 1.578131914138794,
      "learning_rate": 3.1065175176452874e-05,
      "loss": 2.8078,
      "step": 4631
    },
    {
      "epoch": 0.4220885729907053,
      "grad_norm": 1.0402295589447021,
      "learning_rate": 3.105823184327101e-05,
      "loss": 2.7443,
      "step": 4632
    },
    {
      "epoch": 0.42217969746673956,
      "grad_norm": 2.2800116539001465,
      "learning_rate": 3.105128801359318e-05,
      "loss": 2.8318,
      "step": 4633
    },
    {
      "epoch": 0.4222708219427738,
      "grad_norm": 1.6143537759780884,
      "learning_rate": 3.104434368798846e-05,
      "loss": 2.5641,
      "step": 4634
    },
    {
      "epoch": 0.4223619464188081,
      "grad_norm": 1.3232684135437012,
      "learning_rate": 3.103739886702597e-05,
      "loss": 2.9835,
      "step": 4635
    },
    {
      "epoch": 0.42245307089484235,
      "grad_norm": 1.5483814477920532,
      "learning_rate": 3.1030453551274865e-05,
      "loss": 2.672,
      "step": 4636
    },
    {
      "epoch": 0.4225441953708766,
      "grad_norm": 2.344534158706665,
      "learning_rate": 3.1023507741304334e-05,
      "loss": 2.964,
      "step": 4637
    },
    {
      "epoch": 0.4226353198469109,
      "grad_norm": 1.7696547508239746,
      "learning_rate": 3.1016561437683625e-05,
      "loss": 2.8373,
      "step": 4638
    },
    {
      "epoch": 0.42272644432294515,
      "grad_norm": 1.3616026639938354,
      "learning_rate": 3.1009614640982005e-05,
      "loss": 2.8279,
      "step": 4639
    },
    {
      "epoch": 0.4228175687989794,
      "grad_norm": 2.757293462753296,
      "learning_rate": 3.1002667351768795e-05,
      "loss": 3.0834,
      "step": 4640
    },
    {
      "epoch": 0.4229086932750137,
      "grad_norm": 2.2400894165039062,
      "learning_rate": 3.099571957061335e-05,
      "loss": 2.9036,
      "step": 4641
    },
    {
      "epoch": 0.42299981775104795,
      "grad_norm": 1.8044562339782715,
      "learning_rate": 3.098877129808508e-05,
      "loss": 2.9769,
      "step": 4642
    },
    {
      "epoch": 0.4230909422270822,
      "grad_norm": 1.9385782480239868,
      "learning_rate": 3.098182253475341e-05,
      "loss": 3.0623,
      "step": 4643
    },
    {
      "epoch": 0.4231820667031165,
      "grad_norm": 2.9786555767059326,
      "learning_rate": 3.097487328118782e-05,
      "loss": 2.7517,
      "step": 4644
    },
    {
      "epoch": 0.42327319117915074,
      "grad_norm": 1.1800529956817627,
      "learning_rate": 3.096792353795783e-05,
      "loss": 2.7744,
      "step": 4645
    },
    {
      "epoch": 0.423364315655185,
      "grad_norm": 1.9973397254943848,
      "learning_rate": 3.0960973305633e-05,
      "loss": 2.4302,
      "step": 4646
    },
    {
      "epoch": 0.4234554401312192,
      "grad_norm": 2.223601818084717,
      "learning_rate": 3.0954022584782936e-05,
      "loss": 2.9982,
      "step": 4647
    },
    {
      "epoch": 0.4235465646072535,
      "grad_norm": 1.9286527633666992,
      "learning_rate": 3.0947071375977266e-05,
      "loss": 3.014,
      "step": 4648
    },
    {
      "epoch": 0.42363768908328775,
      "grad_norm": 2.1274187564849854,
      "learning_rate": 3.094011967978567e-05,
      "loss": 2.6909,
      "step": 4649
    },
    {
      "epoch": 0.423728813559322,
      "grad_norm": 1.1837794780731201,
      "learning_rate": 3.093316749677788e-05,
      "loss": 2.816,
      "step": 4650
    },
    {
      "epoch": 0.4238199380353563,
      "grad_norm": 1.3666268587112427,
      "learning_rate": 3.092621482752363e-05,
      "loss": 2.8119,
      "step": 4651
    },
    {
      "epoch": 0.42391106251139055,
      "grad_norm": 2.952113151550293,
      "learning_rate": 3.091926167259274e-05,
      "loss": 2.8443,
      "step": 4652
    },
    {
      "epoch": 0.4240021869874248,
      "grad_norm": 2.2075612545013428,
      "learning_rate": 3.0912308032555034e-05,
      "loss": 2.9203,
      "step": 4653
    },
    {
      "epoch": 0.4240933114634591,
      "grad_norm": 1.108238935470581,
      "learning_rate": 3.090535390798041e-05,
      "loss": 2.7161,
      "step": 4654
    },
    {
      "epoch": 0.42418443593949334,
      "grad_norm": 2.5001142024993896,
      "learning_rate": 3.089839929943878e-05,
      "loss": 2.6774,
      "step": 4655
    },
    {
      "epoch": 0.4242755604155276,
      "grad_norm": 2.1996359825134277,
      "learning_rate": 3.0891444207500074e-05,
      "loss": 3.0443,
      "step": 4656
    },
    {
      "epoch": 0.4243666848915619,
      "grad_norm": 1.2600350379943848,
      "learning_rate": 3.088448863273432e-05,
      "loss": 2.7355,
      "step": 4657
    },
    {
      "epoch": 0.42445780936759614,
      "grad_norm": 3.3744263648986816,
      "learning_rate": 3.087753257571154e-05,
      "loss": 2.7616,
      "step": 4658
    },
    {
      "epoch": 0.4245489338436304,
      "grad_norm": 1.4782227277755737,
      "learning_rate": 3.0870576037001824e-05,
      "loss": 2.9125,
      "step": 4659
    },
    {
      "epoch": 0.42464005831966467,
      "grad_norm": 2.2311906814575195,
      "learning_rate": 3.086361901717528e-05,
      "loss": 2.9621,
      "step": 4660
    },
    {
      "epoch": 0.42473118279569894,
      "grad_norm": 2.2398884296417236,
      "learning_rate": 3.0856661516802054e-05,
      "loss": 2.7863,
      "step": 4661
    },
    {
      "epoch": 0.4248223072717332,
      "grad_norm": 1.828068494796753,
      "learning_rate": 3.084970353645236e-05,
      "loss": 2.8684,
      "step": 4662
    },
    {
      "epoch": 0.42491343174776747,
      "grad_norm": 1.6475368738174438,
      "learning_rate": 3.084274507669641e-05,
      "loss": 3.0145,
      "step": 4663
    },
    {
      "epoch": 0.42500455622380173,
      "grad_norm": 2.331514596939087,
      "learning_rate": 3.083578613810449e-05,
      "loss": 2.7526,
      "step": 4664
    },
    {
      "epoch": 0.425095680699836,
      "grad_norm": 1.536002278327942,
      "learning_rate": 3.082882672124691e-05,
      "loss": 3.0934,
      "step": 4665
    },
    {
      "epoch": 0.42518680517587026,
      "grad_norm": 1.8033182621002197,
      "learning_rate": 3.082186682669403e-05,
      "loss": 2.4355,
      "step": 4666
    },
    {
      "epoch": 0.4252779296519045,
      "grad_norm": 2.8311264514923096,
      "learning_rate": 3.0814906455016225e-05,
      "loss": 3.0453,
      "step": 4667
    },
    {
      "epoch": 0.42536905412793874,
      "grad_norm": 1.6262344121932983,
      "learning_rate": 3.080794560678394e-05,
      "loss": 2.9092,
      "step": 4668
    },
    {
      "epoch": 0.425460178603973,
      "grad_norm": 1.1289215087890625,
      "learning_rate": 3.080098428256763e-05,
      "loss": 2.8544,
      "step": 4669
    },
    {
      "epoch": 0.42555130308000727,
      "grad_norm": 1.558866262435913,
      "learning_rate": 3.079402248293781e-05,
      "loss": 2.6182,
      "step": 4670
    },
    {
      "epoch": 0.42564242755604154,
      "grad_norm": 1.3312584161758423,
      "learning_rate": 3.0787060208465026e-05,
      "loss": 2.9176,
      "step": 4671
    },
    {
      "epoch": 0.4257335520320758,
      "grad_norm": 1.315832257270813,
      "learning_rate": 3.078009745971987e-05,
      "loss": 2.783,
      "step": 4672
    },
    {
      "epoch": 0.42582467650811007,
      "grad_norm": 1.7949696779251099,
      "learning_rate": 3.077313423727296e-05,
      "loss": 2.842,
      "step": 4673
    },
    {
      "epoch": 0.42591580098414433,
      "grad_norm": 2.6259350776672363,
      "learning_rate": 3.076617054169495e-05,
      "loss": 3.0003,
      "step": 4674
    },
    {
      "epoch": 0.4260069254601786,
      "grad_norm": 2.1498239040374756,
      "learning_rate": 3.075920637355656e-05,
      "loss": 3.0707,
      "step": 4675
    },
    {
      "epoch": 0.42609804993621286,
      "grad_norm": 2.3275563716888428,
      "learning_rate": 3.075224173342853e-05,
      "loss": 2.7957,
      "step": 4676
    },
    {
      "epoch": 0.42618917441224713,
      "grad_norm": 2.193732261657715,
      "learning_rate": 3.074527662188162e-05,
      "loss": 2.9646,
      "step": 4677
    },
    {
      "epoch": 0.4262802988882814,
      "grad_norm": 2.091560125350952,
      "learning_rate": 3.0738311039486664e-05,
      "loss": 2.6502,
      "step": 4678
    },
    {
      "epoch": 0.42637142336431566,
      "grad_norm": 1.6881122589111328,
      "learning_rate": 3.073134498681453e-05,
      "loss": 2.649,
      "step": 4679
    },
    {
      "epoch": 0.4264625478403499,
      "grad_norm": 1.9299123287200928,
      "learning_rate": 3.072437846443609e-05,
      "loss": 2.819,
      "step": 4680
    },
    {
      "epoch": 0.4265536723163842,
      "grad_norm": 2.3648412227630615,
      "learning_rate": 3.0717411472922294e-05,
      "loss": 3.7393,
      "step": 4681
    },
    {
      "epoch": 0.42664479679241846,
      "grad_norm": 2.058213949203491,
      "learning_rate": 3.07104440128441e-05,
      "loss": 2.8868,
      "step": 4682
    },
    {
      "epoch": 0.4267359212684527,
      "grad_norm": 1.5442355871200562,
      "learning_rate": 3.070347608477253e-05,
      "loss": 2.8852,
      "step": 4683
    },
    {
      "epoch": 0.426827045744487,
      "grad_norm": 1.6395387649536133,
      "learning_rate": 3.069650768927863e-05,
      "loss": 2.7667,
      "step": 4684
    },
    {
      "epoch": 0.42691817022052125,
      "grad_norm": 1.3729428052902222,
      "learning_rate": 3.0689538826933495e-05,
      "loss": 2.8447,
      "step": 4685
    },
    {
      "epoch": 0.4270092946965555,
      "grad_norm": 2.214373826980591,
      "learning_rate": 3.068256949830824e-05,
      "loss": 3.9005,
      "step": 4686
    },
    {
      "epoch": 0.4271004191725898,
      "grad_norm": 1.3884356021881104,
      "learning_rate": 3.067559970397403e-05,
      "loss": 2.8438,
      "step": 4687
    },
    {
      "epoch": 0.427191543648624,
      "grad_norm": 2.8032007217407227,
      "learning_rate": 3.066862944450207e-05,
      "loss": 2.8612,
      "step": 4688
    },
    {
      "epoch": 0.42728266812465826,
      "grad_norm": 1.1474858522415161,
      "learning_rate": 3.06616587204636e-05,
      "loss": 2.7758,
      "step": 4689
    },
    {
      "epoch": 0.4273737926006925,
      "grad_norm": 2.9872078895568848,
      "learning_rate": 3.06546875324299e-05,
      "loss": 2.7959,
      "step": 4690
    },
    {
      "epoch": 0.4274649170767268,
      "grad_norm": 1.3450682163238525,
      "learning_rate": 3.0647715880972286e-05,
      "loss": 3.0229,
      "step": 4691
    },
    {
      "epoch": 0.42755604155276106,
      "grad_norm": 2.6238901615142822,
      "learning_rate": 3.064074376666211e-05,
      "loss": 3.9264,
      "step": 4692
    },
    {
      "epoch": 0.4276471660287953,
      "grad_norm": 1.7515500783920288,
      "learning_rate": 3.0633771190070756e-05,
      "loss": 3.0079,
      "step": 4693
    },
    {
      "epoch": 0.4277382905048296,
      "grad_norm": 1.2737873792648315,
      "learning_rate": 3.0626798151769674e-05,
      "loss": 2.6999,
      "step": 4694
    },
    {
      "epoch": 0.42782941498086385,
      "grad_norm": 2.549633264541626,
      "learning_rate": 3.0619824652330315e-05,
      "loss": 2.8532,
      "step": 4695
    },
    {
      "epoch": 0.4279205394568981,
      "grad_norm": 2.380354404449463,
      "learning_rate": 3.061285069232419e-05,
      "loss": 2.7915,
      "step": 4696
    },
    {
      "epoch": 0.4280116639329324,
      "grad_norm": 1.8545502424240112,
      "learning_rate": 3.060587627232285e-05,
      "loss": 3.0465,
      "step": 4697
    },
    {
      "epoch": 0.42810278840896665,
      "grad_norm": 1.2882804870605469,
      "learning_rate": 3.0598901392897864e-05,
      "loss": 2.7527,
      "step": 4698
    },
    {
      "epoch": 0.4281939128850009,
      "grad_norm": 1.9244800806045532,
      "learning_rate": 3.0591926054620864e-05,
      "loss": 2.865,
      "step": 4699
    },
    {
      "epoch": 0.4282850373610352,
      "grad_norm": 1.6406519412994385,
      "learning_rate": 3.058495025806349e-05,
      "loss": 2.8724,
      "step": 4700
    },
    {
      "epoch": 0.42837616183706945,
      "grad_norm": 1.202433705329895,
      "learning_rate": 3.0577974003797445e-05,
      "loss": 2.7541,
      "step": 4701
    },
    {
      "epoch": 0.4284672863131037,
      "grad_norm": 1.1615561246871948,
      "learning_rate": 3.057099729239446e-05,
      "loss": 2.8083,
      "step": 4702
    },
    {
      "epoch": 0.428558410789138,
      "grad_norm": 1.1613645553588867,
      "learning_rate": 3.056402012442631e-05,
      "loss": 2.6852,
      "step": 4703
    },
    {
      "epoch": 0.42864953526517224,
      "grad_norm": 1.215599775314331,
      "learning_rate": 3.0557042500464796e-05,
      "loss": 2.7041,
      "step": 4704
    },
    {
      "epoch": 0.4287406597412065,
      "grad_norm": 1.017043948173523,
      "learning_rate": 3.055006442108176e-05,
      "loss": 2.7864,
      "step": 4705
    },
    {
      "epoch": 0.4288317842172408,
      "grad_norm": 1.3886948823928833,
      "learning_rate": 3.0543085886849074e-05,
      "loss": 2.9137,
      "step": 4706
    },
    {
      "epoch": 0.42892290869327504,
      "grad_norm": 2.1811916828155518,
      "learning_rate": 3.053610689833868e-05,
      "loss": 2.8649,
      "step": 4707
    },
    {
      "epoch": 0.42901403316930925,
      "grad_norm": 1.7581355571746826,
      "learning_rate": 3.052912745612252e-05,
      "loss": 3.0831,
      "step": 4708
    },
    {
      "epoch": 0.4291051576453435,
      "grad_norm": 1.7793229818344116,
      "learning_rate": 3.052214756077258e-05,
      "loss": 2.9231,
      "step": 4709
    },
    {
      "epoch": 0.4291962821213778,
      "grad_norm": 1.6510096788406372,
      "learning_rate": 3.0515167212860906e-05,
      "loss": 2.8598,
      "step": 4710
    },
    {
      "epoch": 0.42928740659741205,
      "grad_norm": 2.119175434112549,
      "learning_rate": 3.050818641295955e-05,
      "loss": 2.712,
      "step": 4711
    },
    {
      "epoch": 0.4293785310734463,
      "grad_norm": 1.204708218574524,
      "learning_rate": 3.050120516164062e-05,
      "loss": 2.8158,
      "step": 4712
    },
    {
      "epoch": 0.4294696555494806,
      "grad_norm": 1.1528009176254272,
      "learning_rate": 3.049422345947626e-05,
      "loss": 2.7614,
      "step": 4713
    },
    {
      "epoch": 0.42956078002551484,
      "grad_norm": 1.1320804357528687,
      "learning_rate": 3.048724130703865e-05,
      "loss": 2.8703,
      "step": 4714
    },
    {
      "epoch": 0.4296519045015491,
      "grad_norm": 2.8243215084075928,
      "learning_rate": 3.0480258704900004e-05,
      "loss": 2.8532,
      "step": 4715
    },
    {
      "epoch": 0.4297430289775834,
      "grad_norm": 1.5269145965576172,
      "learning_rate": 3.047327565363257e-05,
      "loss": 2.8377,
      "step": 4716
    },
    {
      "epoch": 0.42983415345361764,
      "grad_norm": 2.22741961479187,
      "learning_rate": 3.046629215380864e-05,
      "loss": 2.9019,
      "step": 4717
    },
    {
      "epoch": 0.4299252779296519,
      "grad_norm": 1.1602956056594849,
      "learning_rate": 3.045930820600053e-05,
      "loss": 2.7372,
      "step": 4718
    },
    {
      "epoch": 0.43001640240568617,
      "grad_norm": 3.888667106628418,
      "learning_rate": 3.0452323810780614e-05,
      "loss": 2.6963,
      "step": 4719
    },
    {
      "epoch": 0.43010752688172044,
      "grad_norm": 2.033456802368164,
      "learning_rate": 3.0445338968721287e-05,
      "loss": 2.5572,
      "step": 4720
    },
    {
      "epoch": 0.4301986513577547,
      "grad_norm": 1.4933876991271973,
      "learning_rate": 3.0438353680394995e-05,
      "loss": 2.6821,
      "step": 4721
    },
    {
      "epoch": 0.43028977583378897,
      "grad_norm": 1.7489629983901978,
      "learning_rate": 3.0431367946374185e-05,
      "loss": 2.9813,
      "step": 4722
    },
    {
      "epoch": 0.43038090030982323,
      "grad_norm": 2.4618115425109863,
      "learning_rate": 3.042438176723138e-05,
      "loss": 4.4179,
      "step": 4723
    },
    {
      "epoch": 0.4304720247858575,
      "grad_norm": 1.697442650794983,
      "learning_rate": 3.0417395143539122e-05,
      "loss": 2.5364,
      "step": 4724
    },
    {
      "epoch": 0.43056314926189176,
      "grad_norm": 1.6448115110397339,
      "learning_rate": 3.041040807587e-05,
      "loss": 2.7694,
      "step": 4725
    },
    {
      "epoch": 0.43065427373792603,
      "grad_norm": 1.8555973768234253,
      "learning_rate": 3.0403420564796625e-05,
      "loss": 2.4332,
      "step": 4726
    },
    {
      "epoch": 0.4307453982139603,
      "grad_norm": 1.2157251834869385,
      "learning_rate": 3.0396432610891658e-05,
      "loss": 2.7347,
      "step": 4727
    },
    {
      "epoch": 0.4308365226899945,
      "grad_norm": 1.2389181852340698,
      "learning_rate": 3.038944421472778e-05,
      "loss": 2.7799,
      "step": 4728
    },
    {
      "epoch": 0.43092764716602877,
      "grad_norm": 1.407375693321228,
      "learning_rate": 3.0382455376877718e-05,
      "loss": 2.9923,
      "step": 4729
    },
    {
      "epoch": 0.43101877164206304,
      "grad_norm": 1.2477648258209229,
      "learning_rate": 3.0375466097914242e-05,
      "loss": 2.7911,
      "step": 4730
    },
    {
      "epoch": 0.4311098961180973,
      "grad_norm": 1.781338095664978,
      "learning_rate": 3.0368476378410136e-05,
      "loss": 3.0002,
      "step": 4731
    },
    {
      "epoch": 0.43120102059413157,
      "grad_norm": 2.1225738525390625,
      "learning_rate": 3.036148621893825e-05,
      "loss": 2.8994,
      "step": 4732
    },
    {
      "epoch": 0.43129214507016583,
      "grad_norm": 2.2463603019714355,
      "learning_rate": 3.0354495620071453e-05,
      "loss": 2.6385,
      "step": 4733
    },
    {
      "epoch": 0.4313832695462001,
      "grad_norm": 1.2159438133239746,
      "learning_rate": 3.0347504582382658e-05,
      "loss": 2.8118,
      "step": 4734
    },
    {
      "epoch": 0.43147439402223436,
      "grad_norm": 1.7712781429290771,
      "learning_rate": 3.0340513106444795e-05,
      "loss": 2.9038,
      "step": 4735
    },
    {
      "epoch": 0.43156551849826863,
      "grad_norm": 2.0760083198547363,
      "learning_rate": 3.0333521192830843e-05,
      "loss": 3.073,
      "step": 4736
    },
    {
      "epoch": 0.4316566429743029,
      "grad_norm": 2.259357452392578,
      "learning_rate": 3.032652884211383e-05,
      "loss": 2.7284,
      "step": 4737
    },
    {
      "epoch": 0.43174776745033716,
      "grad_norm": 2.0813887119293213,
      "learning_rate": 3.0319536054866788e-05,
      "loss": 2.6576,
      "step": 4738
    },
    {
      "epoch": 0.4318388919263714,
      "grad_norm": 2.93135142326355,
      "learning_rate": 3.0312542831662826e-05,
      "loss": 2.9429,
      "step": 4739
    },
    {
      "epoch": 0.4319300164024057,
      "grad_norm": 1.9467328786849976,
      "learning_rate": 3.0305549173075042e-05,
      "loss": 2.7593,
      "step": 4740
    },
    {
      "epoch": 0.43202114087843996,
      "grad_norm": 3.059394121170044,
      "learning_rate": 3.0298555079676617e-05,
      "loss": 3.034,
      "step": 4741
    },
    {
      "epoch": 0.4321122653544742,
      "grad_norm": 2.493899345397949,
      "learning_rate": 3.0291560552040727e-05,
      "loss": 2.804,
      "step": 4742
    },
    {
      "epoch": 0.4322033898305085,
      "grad_norm": 1.656376838684082,
      "learning_rate": 3.028456559074061e-05,
      "loss": 2.6677,
      "step": 4743
    },
    {
      "epoch": 0.43229451430654275,
      "grad_norm": 1.7796459197998047,
      "learning_rate": 3.0277570196349526e-05,
      "loss": 2.8673,
      "step": 4744
    },
    {
      "epoch": 0.432385638782577,
      "grad_norm": 2.5672478675842285,
      "learning_rate": 3.0270574369440786e-05,
      "loss": 2.9533,
      "step": 4745
    },
    {
      "epoch": 0.4324767632586113,
      "grad_norm": 2.1092751026153564,
      "learning_rate": 3.026357811058771e-05,
      "loss": 2.8127,
      "step": 4746
    },
    {
      "epoch": 0.43256788773464555,
      "grad_norm": 1.9806127548217773,
      "learning_rate": 3.0256581420363674e-05,
      "loss": 2.8213,
      "step": 4747
    },
    {
      "epoch": 0.43265901221067976,
      "grad_norm": 1.1219462156295776,
      "learning_rate": 3.0249584299342093e-05,
      "loss": 2.6555,
      "step": 4748
    },
    {
      "epoch": 0.432750136686714,
      "grad_norm": 1.8404849767684937,
      "learning_rate": 3.0242586748096398e-05,
      "loss": 2.8402,
      "step": 4749
    },
    {
      "epoch": 0.4328412611627483,
      "grad_norm": 1.6993242502212524,
      "learning_rate": 3.0235588767200068e-05,
      "loss": 3.0199,
      "step": 4750
    },
    {
      "epoch": 0.43293238563878256,
      "grad_norm": 2.558347702026367,
      "learning_rate": 3.022859035722662e-05,
      "loss": 2.9329,
      "step": 4751
    },
    {
      "epoch": 0.4330235101148168,
      "grad_norm": 2.5083115100860596,
      "learning_rate": 3.0221591518749605e-05,
      "loss": 2.3329,
      "step": 4752
    },
    {
      "epoch": 0.4331146345908511,
      "grad_norm": 1.944162130355835,
      "learning_rate": 3.021459225234259e-05,
      "loss": 2.7707,
      "step": 4753
    },
    {
      "epoch": 0.43320575906688535,
      "grad_norm": 1.8385224342346191,
      "learning_rate": 3.0207592558579194e-05,
      "loss": 3.0217,
      "step": 4754
    },
    {
      "epoch": 0.4332968835429196,
      "grad_norm": 1.6336921453475952,
      "learning_rate": 3.020059243803309e-05,
      "loss": 2.9009,
      "step": 4755
    },
    {
      "epoch": 0.4333880080189539,
      "grad_norm": 2.3132331371307373,
      "learning_rate": 3.0193591891277944e-05,
      "loss": 2.8544,
      "step": 4756
    },
    {
      "epoch": 0.43347913249498815,
      "grad_norm": 3.2172837257385254,
      "learning_rate": 3.0186590918887487e-05,
      "loss": 2.7981,
      "step": 4757
    },
    {
      "epoch": 0.4335702569710224,
      "grad_norm": 2.025907278060913,
      "learning_rate": 3.017958952143548e-05,
      "loss": 2.8541,
      "step": 4758
    },
    {
      "epoch": 0.4336613814470567,
      "grad_norm": 1.3030201196670532,
      "learning_rate": 3.017258769949571e-05,
      "loss": 2.8839,
      "step": 4759
    },
    {
      "epoch": 0.43375250592309095,
      "grad_norm": 1.3179248571395874,
      "learning_rate": 3.0165585453642004e-05,
      "loss": 2.7791,
      "step": 4760
    },
    {
      "epoch": 0.4338436303991252,
      "grad_norm": 1.743671178817749,
      "learning_rate": 3.015858278444822e-05,
      "loss": 2.7718,
      "step": 4761
    },
    {
      "epoch": 0.4339347548751595,
      "grad_norm": 1.3369323015213013,
      "learning_rate": 3.0151579692488263e-05,
      "loss": 2.8473,
      "step": 4762
    },
    {
      "epoch": 0.43402587935119374,
      "grad_norm": 2.0303432941436768,
      "learning_rate": 3.014457617833606e-05,
      "loss": 2.8052,
      "step": 4763
    },
    {
      "epoch": 0.434117003827228,
      "grad_norm": 2.373037338256836,
      "learning_rate": 3.0137572242565575e-05,
      "loss": 3.0152,
      "step": 4764
    },
    {
      "epoch": 0.4342081283032623,
      "grad_norm": 1.8812288045883179,
      "learning_rate": 3.0130567885750805e-05,
      "loss": 2.9658,
      "step": 4765
    },
    {
      "epoch": 0.43429925277929654,
      "grad_norm": 1.3461384773254395,
      "learning_rate": 3.012356310846579e-05,
      "loss": 2.7663,
      "step": 4766
    },
    {
      "epoch": 0.4343903772553308,
      "grad_norm": 2.7046759128570557,
      "learning_rate": 3.0116557911284606e-05,
      "loss": 2.8243,
      "step": 4767
    },
    {
      "epoch": 0.43448150173136507,
      "grad_norm": 2.3814337253570557,
      "learning_rate": 3.010955229478134e-05,
      "loss": 2.6969,
      "step": 4768
    },
    {
      "epoch": 0.4345726262073993,
      "grad_norm": 2.4580228328704834,
      "learning_rate": 3.0102546259530144e-05,
      "loss": 3.1001,
      "step": 4769
    },
    {
      "epoch": 0.43466375068343355,
      "grad_norm": 3.1775519847869873,
      "learning_rate": 3.009553980610519e-05,
      "loss": 4.1222,
      "step": 4770
    },
    {
      "epoch": 0.4347548751594678,
      "grad_norm": 1.9653373956680298,
      "learning_rate": 3.0088532935080666e-05,
      "loss": 2.8465,
      "step": 4771
    },
    {
      "epoch": 0.4348459996355021,
      "grad_norm": 2.4884750843048096,
      "learning_rate": 3.0081525647030828e-05,
      "loss": 2.9325,
      "step": 4772
    },
    {
      "epoch": 0.43493712411153634,
      "grad_norm": 1.4331941604614258,
      "learning_rate": 3.007451794252995e-05,
      "loss": 2.8157,
      "step": 4773
    },
    {
      "epoch": 0.4350282485875706,
      "grad_norm": 1.1838361024856567,
      "learning_rate": 3.006750982215234e-05,
      "loss": 2.6284,
      "step": 4774
    },
    {
      "epoch": 0.4351193730636049,
      "grad_norm": 2.152066469192505,
      "learning_rate": 3.0060501286472344e-05,
      "loss": 2.6963,
      "step": 4775
    },
    {
      "epoch": 0.43521049753963914,
      "grad_norm": 2.3367626667022705,
      "learning_rate": 3.0053492336064338e-05,
      "loss": 3.0349,
      "step": 4776
    },
    {
      "epoch": 0.4353016220156734,
      "grad_norm": 1.1775341033935547,
      "learning_rate": 3.004648297150272e-05,
      "loss": 2.7516,
      "step": 4777
    },
    {
      "epoch": 0.43539274649170767,
      "grad_norm": 1.207924485206604,
      "learning_rate": 3.003947319336195e-05,
      "loss": 2.8324,
      "step": 4778
    },
    {
      "epoch": 0.43548387096774194,
      "grad_norm": 1.9023174047470093,
      "learning_rate": 3.0032463002216505e-05,
      "loss": 2.726,
      "step": 4779
    },
    {
      "epoch": 0.4355749954437762,
      "grad_norm": 1.1747989654541016,
      "learning_rate": 3.0025452398640892e-05,
      "loss": 2.7064,
      "step": 4780
    },
    {
      "epoch": 0.43566611991981047,
      "grad_norm": 2.2251150608062744,
      "learning_rate": 3.001844138320966e-05,
      "loss": 3.0327,
      "step": 4781
    },
    {
      "epoch": 0.43575724439584473,
      "grad_norm": 1.8219935894012451,
      "learning_rate": 3.00114299564974e-05,
      "loss": 2.662,
      "step": 4782
    },
    {
      "epoch": 0.435848368871879,
      "grad_norm": 2.2384326457977295,
      "learning_rate": 3.000441811907871e-05,
      "loss": 2.9112,
      "step": 4783
    },
    {
      "epoch": 0.43593949334791326,
      "grad_norm": 1.060400128364563,
      "learning_rate": 2.9997405871528244e-05,
      "loss": 2.6618,
      "step": 4784
    },
    {
      "epoch": 0.43603061782394753,
      "grad_norm": 2.057602643966675,
      "learning_rate": 2.9990393214420682e-05,
      "loss": 2.5645,
      "step": 4785
    },
    {
      "epoch": 0.4361217422999818,
      "grad_norm": 1.6357135772705078,
      "learning_rate": 2.9983380148330743e-05,
      "loss": 2.777,
      "step": 4786
    },
    {
      "epoch": 0.43621286677601606,
      "grad_norm": 1.1299322843551636,
      "learning_rate": 2.997636667383317e-05,
      "loss": 2.6933,
      "step": 4787
    },
    {
      "epoch": 0.4363039912520503,
      "grad_norm": 1.598834753036499,
      "learning_rate": 2.996935279150275e-05,
      "loss": 2.644,
      "step": 4788
    },
    {
      "epoch": 0.43639511572808454,
      "grad_norm": 1.7617199420928955,
      "learning_rate": 2.99623385019143e-05,
      "loss": 2.5605,
      "step": 4789
    },
    {
      "epoch": 0.4364862402041188,
      "grad_norm": 1.164556622505188,
      "learning_rate": 2.995532380564266e-05,
      "loss": 2.7471,
      "step": 4790
    },
    {
      "epoch": 0.43657736468015307,
      "grad_norm": 1.3418508768081665,
      "learning_rate": 2.994830870326272e-05,
      "loss": 2.8806,
      "step": 4791
    },
    {
      "epoch": 0.43666848915618733,
      "grad_norm": 3.3263909816741943,
      "learning_rate": 2.9941293195349384e-05,
      "loss": 2.7042,
      "step": 4792
    },
    {
      "epoch": 0.4367596136322216,
      "grad_norm": 2.591545820236206,
      "learning_rate": 2.9934277282477607e-05,
      "loss": 2.4333,
      "step": 4793
    },
    {
      "epoch": 0.43685073810825586,
      "grad_norm": 1.1068850755691528,
      "learning_rate": 2.992726096522238e-05,
      "loss": 2.7916,
      "step": 4794
    },
    {
      "epoch": 0.43694186258429013,
      "grad_norm": 1.815421223640442,
      "learning_rate": 2.9920244244158706e-05,
      "loss": 2.7989,
      "step": 4795
    },
    {
      "epoch": 0.4370329870603244,
      "grad_norm": 1.2854191064834595,
      "learning_rate": 2.991322711986164e-05,
      "loss": 2.7024,
      "step": 4796
    },
    {
      "epoch": 0.43712411153635866,
      "grad_norm": 2.376046895980835,
      "learning_rate": 2.9906209592906254e-05,
      "loss": 3.0189,
      "step": 4797
    },
    {
      "epoch": 0.4372152360123929,
      "grad_norm": 1.6106380224227905,
      "learning_rate": 2.989919166386767e-05,
      "loss": 2.1665,
      "step": 4798
    },
    {
      "epoch": 0.4373063604884272,
      "grad_norm": 2.6142923831939697,
      "learning_rate": 2.9892173333321032e-05,
      "loss": 2.709,
      "step": 4799
    },
    {
      "epoch": 0.43739748496446146,
      "grad_norm": 2.8085036277770996,
      "learning_rate": 2.9885154601841524e-05,
      "loss": 2.8831,
      "step": 4800
    },
    {
      "epoch": 0.4374886094404957,
      "grad_norm": 1.0307453870773315,
      "learning_rate": 2.9878135470004352e-05,
      "loss": 2.7304,
      "step": 4801
    },
    {
      "epoch": 0.43757973391653,
      "grad_norm": 1.289904236793518,
      "learning_rate": 2.987111593838477e-05,
      "loss": 2.7397,
      "step": 4802
    },
    {
      "epoch": 0.43767085839256425,
      "grad_norm": 1.970460057258606,
      "learning_rate": 2.986409600755804e-05,
      "loss": 2.6582,
      "step": 4803
    },
    {
      "epoch": 0.4377619828685985,
      "grad_norm": 1.7031047344207764,
      "learning_rate": 2.9857075678099487e-05,
      "loss": 2.7675,
      "step": 4804
    },
    {
      "epoch": 0.4378531073446328,
      "grad_norm": 2.1312012672424316,
      "learning_rate": 2.985005495058446e-05,
      "loss": 2.5716,
      "step": 4805
    },
    {
      "epoch": 0.43794423182066705,
      "grad_norm": 2.0063412189483643,
      "learning_rate": 2.9843033825588325e-05,
      "loss": 3.0847,
      "step": 4806
    },
    {
      "epoch": 0.4380353562967013,
      "grad_norm": 2.499363660812378,
      "learning_rate": 2.983601230368649e-05,
      "loss": 2.9743,
      "step": 4807
    },
    {
      "epoch": 0.4381264807727356,
      "grad_norm": 1.4891536235809326,
      "learning_rate": 2.9828990385454392e-05,
      "loss": 2.7738,
      "step": 4808
    },
    {
      "epoch": 0.4382176052487698,
      "grad_norm": 2.900916814804077,
      "learning_rate": 2.9821968071467527e-05,
      "loss": 2.9671,
      "step": 4809
    },
    {
      "epoch": 0.43830872972480406,
      "grad_norm": 2.107529401779175,
      "learning_rate": 2.9814945362301373e-05,
      "loss": 2.9234,
      "step": 4810
    },
    {
      "epoch": 0.4383998542008383,
      "grad_norm": 2.57232403755188,
      "learning_rate": 2.9807922258531484e-05,
      "loss": 2.233,
      "step": 4811
    },
    {
      "epoch": 0.4384909786768726,
      "grad_norm": 1.8241093158721924,
      "learning_rate": 2.980089876073343e-05,
      "loss": 2.631,
      "step": 4812
    },
    {
      "epoch": 0.43858210315290685,
      "grad_norm": 1.7509831190109253,
      "learning_rate": 2.979387486948282e-05,
      "loss": 2.8993,
      "step": 4813
    },
    {
      "epoch": 0.4386732276289411,
      "grad_norm": 1.2654427289962769,
      "learning_rate": 2.9786850585355275e-05,
      "loss": 2.7381,
      "step": 4814
    },
    {
      "epoch": 0.4387643521049754,
      "grad_norm": 2.27424955368042,
      "learning_rate": 2.977982590892647e-05,
      "loss": 2.5533,
      "step": 4815
    },
    {
      "epoch": 0.43885547658100965,
      "grad_norm": 1.9555788040161133,
      "learning_rate": 2.9772800840772107e-05,
      "loss": 2.5906,
      "step": 4816
    },
    {
      "epoch": 0.4389466010570439,
      "grad_norm": 2.2514283657073975,
      "learning_rate": 2.9765775381467915e-05,
      "loss": 3.7773,
      "step": 4817
    },
    {
      "epoch": 0.4390377255330782,
      "grad_norm": 2.162853956222534,
      "learning_rate": 2.9758749531589662e-05,
      "loss": 2.8184,
      "step": 4818
    },
    {
      "epoch": 0.43912885000911245,
      "grad_norm": 2.21590518951416,
      "learning_rate": 2.975172329171314e-05,
      "loss": 3.0875,
      "step": 4819
    },
    {
      "epoch": 0.4392199744851467,
      "grad_norm": 2.026801347732544,
      "learning_rate": 2.9744696662414177e-05,
      "loss": 2.5718,
      "step": 4820
    },
    {
      "epoch": 0.439311098961181,
      "grad_norm": 1.2580608129501343,
      "learning_rate": 2.973766964426864e-05,
      "loss": 2.8005,
      "step": 4821
    },
    {
      "epoch": 0.43940222343721524,
      "grad_norm": 3.381014585494995,
      "learning_rate": 2.9730642237852406e-05,
      "loss": 2.763,
      "step": 4822
    },
    {
      "epoch": 0.4394933479132495,
      "grad_norm": 2.1895246505737305,
      "learning_rate": 2.9723614443741414e-05,
      "loss": 2.8854,
      "step": 4823
    },
    {
      "epoch": 0.4395844723892838,
      "grad_norm": 1.7425564527511597,
      "learning_rate": 2.971658626251162e-05,
      "loss": 2.8239,
      "step": 4824
    },
    {
      "epoch": 0.43967559686531804,
      "grad_norm": 2.2100436687469482,
      "learning_rate": 2.970955769473899e-05,
      "loss": 2.3366,
      "step": 4825
    },
    {
      "epoch": 0.4397667213413523,
      "grad_norm": 2.5919361114501953,
      "learning_rate": 2.970252874099957e-05,
      "loss": 2.5017,
      "step": 4826
    },
    {
      "epoch": 0.4398578458173866,
      "grad_norm": 2.537013053894043,
      "learning_rate": 2.9695499401869392e-05,
      "loss": 2.8258,
      "step": 4827
    },
    {
      "epoch": 0.43994897029342084,
      "grad_norm": 1.9661500453948975,
      "learning_rate": 2.968846967792454e-05,
      "loss": 2.7543,
      "step": 4828
    },
    {
      "epoch": 0.44004009476945505,
      "grad_norm": 1.1493171453475952,
      "learning_rate": 2.9681439569741144e-05,
      "loss": 2.654,
      "step": 4829
    },
    {
      "epoch": 0.4401312192454893,
      "grad_norm": 2.1770033836364746,
      "learning_rate": 2.967440907789532e-05,
      "loss": 2.699,
      "step": 4830
    },
    {
      "epoch": 0.4402223437215236,
      "grad_norm": 3.369387626647949,
      "learning_rate": 2.9667378202963277e-05,
      "loss": 2.2938,
      "step": 4831
    },
    {
      "epoch": 0.44031346819755784,
      "grad_norm": 1.849500298500061,
      "learning_rate": 2.9660346945521195e-05,
      "loss": 2.9158,
      "step": 4832
    },
    {
      "epoch": 0.4404045926735921,
      "grad_norm": 1.7585679292678833,
      "learning_rate": 2.965331530614533e-05,
      "loss": 2.8693,
      "step": 4833
    },
    {
      "epoch": 0.4404957171496264,
      "grad_norm": 3.2426416873931885,
      "learning_rate": 2.964628328541194e-05,
      "loss": 3.1066,
      "step": 4834
    },
    {
      "epoch": 0.44058684162566064,
      "grad_norm": 2.939274311065674,
      "learning_rate": 2.963925088389734e-05,
      "loss": 2.9058,
      "step": 4835
    },
    {
      "epoch": 0.4406779661016949,
      "grad_norm": 2.7943358421325684,
      "learning_rate": 2.9632218102177862e-05,
      "loss": 2.8257,
      "step": 4836
    },
    {
      "epoch": 0.4407690905777292,
      "grad_norm": 1.8000037670135498,
      "learning_rate": 2.9625184940829864e-05,
      "loss": 2.6536,
      "step": 4837
    },
    {
      "epoch": 0.44086021505376344,
      "grad_norm": 1.3739973306655884,
      "learning_rate": 2.961815140042974e-05,
      "loss": 2.7744,
      "step": 4838
    },
    {
      "epoch": 0.4409513395297977,
      "grad_norm": 1.6128652095794678,
      "learning_rate": 2.9611117481553912e-05,
      "loss": 2.6369,
      "step": 4839
    },
    {
      "epoch": 0.44104246400583197,
      "grad_norm": 1.4494088888168335,
      "learning_rate": 2.9604083184778852e-05,
      "loss": 2.7852,
      "step": 4840
    },
    {
      "epoch": 0.44113358848186623,
      "grad_norm": 2.0040643215179443,
      "learning_rate": 2.9597048510681042e-05,
      "loss": 2.669,
      "step": 4841
    },
    {
      "epoch": 0.4412247129579005,
      "grad_norm": 2.5493714809417725,
      "learning_rate": 2.9590013459837e-05,
      "loss": 3.1375,
      "step": 4842
    },
    {
      "epoch": 0.44131583743393477,
      "grad_norm": 2.1356396675109863,
      "learning_rate": 2.9582978032823273e-05,
      "loss": 2.6559,
      "step": 4843
    },
    {
      "epoch": 0.44140696190996903,
      "grad_norm": 1.6828135251998901,
      "learning_rate": 2.9575942230216448e-05,
      "loss": 2.8736,
      "step": 4844
    },
    {
      "epoch": 0.4414980863860033,
      "grad_norm": 2.1871190071105957,
      "learning_rate": 2.956890605259313e-05,
      "loss": 2.7771,
      "step": 4845
    },
    {
      "epoch": 0.44158921086203756,
      "grad_norm": 2.140308141708374,
      "learning_rate": 2.9561869500529964e-05,
      "loss": 3.0424,
      "step": 4846
    },
    {
      "epoch": 0.44168033533807183,
      "grad_norm": 1.975234031677246,
      "learning_rate": 2.955483257460363e-05,
      "loss": 2.7272,
      "step": 4847
    },
    {
      "epoch": 0.4417714598141061,
      "grad_norm": 2.157597541809082,
      "learning_rate": 2.9547795275390826e-05,
      "loss": 2.8908,
      "step": 4848
    },
    {
      "epoch": 0.44186258429014036,
      "grad_norm": 1.2776979207992554,
      "learning_rate": 2.9540757603468284e-05,
      "loss": 2.8039,
      "step": 4849
    },
    {
      "epoch": 0.44195370876617457,
      "grad_norm": 2.562398672103882,
      "learning_rate": 2.9533719559412776e-05,
      "loss": 4.2882,
      "step": 4850
    },
    {
      "epoch": 0.44204483324220883,
      "grad_norm": 1.0581672191619873,
      "learning_rate": 2.9526681143801093e-05,
      "loss": 2.7733,
      "step": 4851
    },
    {
      "epoch": 0.4421359577182431,
      "grad_norm": 2.2164032459259033,
      "learning_rate": 2.9519642357210053e-05,
      "loss": 3.045,
      "step": 4852
    },
    {
      "epoch": 0.44222708219427737,
      "grad_norm": 1.13514244556427,
      "learning_rate": 2.9512603200216526e-05,
      "loss": 2.7047,
      "step": 4853
    },
    {
      "epoch": 0.44231820667031163,
      "grad_norm": 1.7642154693603516,
      "learning_rate": 2.950556367339739e-05,
      "loss": 3.2119,
      "step": 4854
    },
    {
      "epoch": 0.4424093311463459,
      "grad_norm": 2.8292076587677,
      "learning_rate": 2.9498523777329574e-05,
      "loss": 2.8901,
      "step": 4855
    },
    {
      "epoch": 0.44250045562238016,
      "grad_norm": 1.8202112913131714,
      "learning_rate": 2.949148351259e-05,
      "loss": 3.1253,
      "step": 4856
    },
    {
      "epoch": 0.4425915800984144,
      "grad_norm": 1.958294153213501,
      "learning_rate": 2.9484442879755677e-05,
      "loss": 3.0397,
      "step": 4857
    },
    {
      "epoch": 0.4426827045744487,
      "grad_norm": 2.019104480743408,
      "learning_rate": 2.947740187940358e-05,
      "loss": 2.8601,
      "step": 4858
    },
    {
      "epoch": 0.44277382905048296,
      "grad_norm": 2.776658058166504,
      "learning_rate": 2.947036051211078e-05,
      "loss": 2.908,
      "step": 4859
    },
    {
      "epoch": 0.4428649535265172,
      "grad_norm": 1.6717549562454224,
      "learning_rate": 2.946331877845432e-05,
      "loss": 2.8192,
      "step": 4860
    },
    {
      "epoch": 0.4429560780025515,
      "grad_norm": 1.2616040706634521,
      "learning_rate": 2.9456276679011306e-05,
      "loss": 2.7366,
      "step": 4861
    },
    {
      "epoch": 0.44304720247858576,
      "grad_norm": 1.305716633796692,
      "learning_rate": 2.9449234214358874e-05,
      "loss": 2.7575,
      "step": 4862
    },
    {
      "epoch": 0.44313832695462,
      "grad_norm": 1.7685348987579346,
      "learning_rate": 2.9442191385074164e-05,
      "loss": 2.9061,
      "step": 4863
    },
    {
      "epoch": 0.4432294514306543,
      "grad_norm": 2.5789895057678223,
      "learning_rate": 2.9435148191734378e-05,
      "loss": 2.9816,
      "step": 4864
    },
    {
      "epoch": 0.44332057590668855,
      "grad_norm": 1.405606746673584,
      "learning_rate": 2.9428104634916725e-05,
      "loss": 2.8553,
      "step": 4865
    },
    {
      "epoch": 0.4434117003827228,
      "grad_norm": 1.787601351737976,
      "learning_rate": 2.942106071519847e-05,
      "loss": 2.8599,
      "step": 4866
    },
    {
      "epoch": 0.4435028248587571,
      "grad_norm": 1.8215950727462769,
      "learning_rate": 2.941401643315686e-05,
      "loss": 2.8311,
      "step": 4867
    },
    {
      "epoch": 0.44359394933479135,
      "grad_norm": 2.897700309753418,
      "learning_rate": 2.9406971789369227e-05,
      "loss": 2.9187,
      "step": 4868
    },
    {
      "epoch": 0.4436850738108256,
      "grad_norm": 1.2926105260849,
      "learning_rate": 2.939992678441289e-05,
      "loss": 2.8794,
      "step": 4869
    },
    {
      "epoch": 0.4437761982868598,
      "grad_norm": 1.9721453189849854,
      "learning_rate": 2.9392881418865232e-05,
      "loss": 2.9179,
      "step": 4870
    },
    {
      "epoch": 0.4438673227628941,
      "grad_norm": 1.3900399208068848,
      "learning_rate": 2.938583569330363e-05,
      "loss": 2.7561,
      "step": 4871
    },
    {
      "epoch": 0.44395844723892836,
      "grad_norm": 2.032315969467163,
      "learning_rate": 2.9378789608305524e-05,
      "loss": 2.8513,
      "step": 4872
    },
    {
      "epoch": 0.4440495717149626,
      "grad_norm": 1.6921727657318115,
      "learning_rate": 2.9371743164448367e-05,
      "loss": 2.8238,
      "step": 4873
    },
    {
      "epoch": 0.4441406961909969,
      "grad_norm": 1.1016764640808105,
      "learning_rate": 2.9364696362309635e-05,
      "loss": 2.7148,
      "step": 4874
    },
    {
      "epoch": 0.44423182066703115,
      "grad_norm": 1.9801093339920044,
      "learning_rate": 2.935764920246684e-05,
      "loss": 2.8379,
      "step": 4875
    },
    {
      "epoch": 0.4443229451430654,
      "grad_norm": 2.1257617473602295,
      "learning_rate": 2.935060168549753e-05,
      "loss": 2.8976,
      "step": 4876
    },
    {
      "epoch": 0.4444140696190997,
      "grad_norm": 2.10430908203125,
      "learning_rate": 2.934355381197928e-05,
      "loss": 2.881,
      "step": 4877
    },
    {
      "epoch": 0.44450519409513395,
      "grad_norm": 2.1780600547790527,
      "learning_rate": 2.933650558248968e-05,
      "loss": 2.818,
      "step": 4878
    },
    {
      "epoch": 0.4445963185711682,
      "grad_norm": 1.7511234283447266,
      "learning_rate": 2.932945699760638e-05,
      "loss": 2.3419,
      "step": 4879
    },
    {
      "epoch": 0.4446874430472025,
      "grad_norm": 1.4893224239349365,
      "learning_rate": 2.932240805790702e-05,
      "loss": 2.7394,
      "step": 4880
    },
    {
      "epoch": 0.44477856752323675,
      "grad_norm": 2.2618179321289062,
      "learning_rate": 2.931535876396929e-05,
      "loss": 3.0778,
      "step": 4881
    },
    {
      "epoch": 0.444869691999271,
      "grad_norm": 2.3927087783813477,
      "learning_rate": 2.9308309116370912e-05,
      "loss": 2.9468,
      "step": 4882
    },
    {
      "epoch": 0.4449608164753053,
      "grad_norm": 1.4946776628494263,
      "learning_rate": 2.930125911568964e-05,
      "loss": 2.9207,
      "step": 4883
    },
    {
      "epoch": 0.44505194095133954,
      "grad_norm": 3.3866899013519287,
      "learning_rate": 2.929420876250324e-05,
      "loss": 3.1021,
      "step": 4884
    },
    {
      "epoch": 0.4451430654273738,
      "grad_norm": 1.1989799737930298,
      "learning_rate": 2.928715805738952e-05,
      "loss": 2.798,
      "step": 4885
    },
    {
      "epoch": 0.4452341899034081,
      "grad_norm": 1.5234860181808472,
      "learning_rate": 2.9280107000926305e-05,
      "loss": 2.8644,
      "step": 4886
    },
    {
      "epoch": 0.44532531437944234,
      "grad_norm": 2.266920566558838,
      "learning_rate": 2.9273055593691467e-05,
      "loss": 3.0903,
      "step": 4887
    },
    {
      "epoch": 0.4454164388554766,
      "grad_norm": 1.1075750589370728,
      "learning_rate": 2.9266003836262888e-05,
      "loss": 2.7893,
      "step": 4888
    },
    {
      "epoch": 0.44550756333151087,
      "grad_norm": 1.961599588394165,
      "learning_rate": 2.92589517292185e-05,
      "loss": 2.6695,
      "step": 4889
    },
    {
      "epoch": 0.4455986878075451,
      "grad_norm": 2.473261594772339,
      "learning_rate": 2.925189927313624e-05,
      "loss": 2.8093,
      "step": 4890
    },
    {
      "epoch": 0.44568981228357935,
      "grad_norm": 1.9911112785339355,
      "learning_rate": 2.924484646859409e-05,
      "loss": 2.6238,
      "step": 4891
    },
    {
      "epoch": 0.4457809367596136,
      "grad_norm": 1.8888298273086548,
      "learning_rate": 2.9237793316170054e-05,
      "loss": 2.8606,
      "step": 4892
    },
    {
      "epoch": 0.4458720612356479,
      "grad_norm": 1.8483177423477173,
      "learning_rate": 2.9230739816442154e-05,
      "loss": 2.8059,
      "step": 4893
    },
    {
      "epoch": 0.44596318571168214,
      "grad_norm": 1.9512355327606201,
      "learning_rate": 2.922368596998847e-05,
      "loss": 2.6836,
      "step": 4894
    },
    {
      "epoch": 0.4460543101877164,
      "grad_norm": 2.1488192081451416,
      "learning_rate": 2.921663177738708e-05,
      "loss": 2.8273,
      "step": 4895
    },
    {
      "epoch": 0.4461454346637507,
      "grad_norm": 2.617561101913452,
      "learning_rate": 2.9209577239216114e-05,
      "loss": 2.9169,
      "step": 4896
    },
    {
      "epoch": 0.44623655913978494,
      "grad_norm": 1.6745774745941162,
      "learning_rate": 2.920252235605371e-05,
      "loss": 2.8578,
      "step": 4897
    },
    {
      "epoch": 0.4463276836158192,
      "grad_norm": 1.230726957321167,
      "learning_rate": 2.9195467128478044e-05,
      "loss": 2.7449,
      "step": 4898
    },
    {
      "epoch": 0.44641880809185347,
      "grad_norm": 1.1328648328781128,
      "learning_rate": 2.9188411557067325e-05,
      "loss": 2.6495,
      "step": 4899
    },
    {
      "epoch": 0.44650993256788774,
      "grad_norm": 2.061992883682251,
      "learning_rate": 2.9181355642399777e-05,
      "loss": 2.7627,
      "step": 4900
    },
    {
      "epoch": 0.446601057043922,
      "grad_norm": 2.2181179523468018,
      "learning_rate": 2.9174299385053666e-05,
      "loss": 2.7305,
      "step": 4901
    },
    {
      "epoch": 0.44669218151995627,
      "grad_norm": 2.1324572563171387,
      "learning_rate": 2.9167242785607286e-05,
      "loss": 2.9839,
      "step": 4902
    },
    {
      "epoch": 0.44678330599599053,
      "grad_norm": 2.005934000015259,
      "learning_rate": 2.9160185844638942e-05,
      "loss": 2.6825,
      "step": 4903
    },
    {
      "epoch": 0.4468744304720248,
      "grad_norm": 1.5520254373550415,
      "learning_rate": 2.9153128562726983e-05,
      "loss": 2.9208,
      "step": 4904
    },
    {
      "epoch": 0.44696555494805906,
      "grad_norm": 1.7277772426605225,
      "learning_rate": 2.9146070940449778e-05,
      "loss": 2.8588,
      "step": 4905
    },
    {
      "epoch": 0.44705667942409333,
      "grad_norm": 3.10925030708313,
      "learning_rate": 2.9139012978385727e-05,
      "loss": 2.8301,
      "step": 4906
    },
    {
      "epoch": 0.4471478039001276,
      "grad_norm": 1.1207301616668701,
      "learning_rate": 2.913195467711326e-05,
      "loss": 2.7814,
      "step": 4907
    },
    {
      "epoch": 0.44723892837616186,
      "grad_norm": 3.293431282043457,
      "learning_rate": 2.9124896037210837e-05,
      "loss": 2.7651,
      "step": 4908
    },
    {
      "epoch": 0.4473300528521961,
      "grad_norm": 2.4793081283569336,
      "learning_rate": 2.9117837059256943e-05,
      "loss": 4.3215,
      "step": 4909
    },
    {
      "epoch": 0.44742117732823034,
      "grad_norm": 1.7285553216934204,
      "learning_rate": 2.9110777743830076e-05,
      "loss": 2.7026,
      "step": 4910
    },
    {
      "epoch": 0.4475123018042646,
      "grad_norm": 1.8655694723129272,
      "learning_rate": 2.910371809150878e-05,
      "loss": 2.9795,
      "step": 4911
    },
    {
      "epoch": 0.44760342628029887,
      "grad_norm": 1.8424359560012817,
      "learning_rate": 2.9096658102871627e-05,
      "loss": 2.6669,
      "step": 4912
    },
    {
      "epoch": 0.44769455075633313,
      "grad_norm": 2.3700613975524902,
      "learning_rate": 2.9089597778497203e-05,
      "loss": 4.0241,
      "step": 4913
    },
    {
      "epoch": 0.4477856752323674,
      "grad_norm": 1.8584645986557007,
      "learning_rate": 2.908253711896413e-05,
      "loss": 2.8054,
      "step": 4914
    },
    {
      "epoch": 0.44787679970840166,
      "grad_norm": 1.0255930423736572,
      "learning_rate": 2.9075476124851065e-05,
      "loss": 2.8096,
      "step": 4915
    },
    {
      "epoch": 0.44796792418443593,
      "grad_norm": 2.029689311981201,
      "learning_rate": 2.9068414796736682e-05,
      "loss": 2.3919,
      "step": 4916
    },
    {
      "epoch": 0.4480590486604702,
      "grad_norm": 1.7061271667480469,
      "learning_rate": 2.9061353135199676e-05,
      "loss": 2.9303,
      "step": 4917
    },
    {
      "epoch": 0.44815017313650446,
      "grad_norm": 2.9316225051879883,
      "learning_rate": 2.905429114081879e-05,
      "loss": 2.6855,
      "step": 4918
    },
    {
      "epoch": 0.4482412976125387,
      "grad_norm": 2.7062902450561523,
      "learning_rate": 2.9047228814172774e-05,
      "loss": 2.237,
      "step": 4919
    },
    {
      "epoch": 0.448332422088573,
      "grad_norm": 3.0647268295288086,
      "learning_rate": 2.904016615584041e-05,
      "loss": 2.3255,
      "step": 4920
    },
    {
      "epoch": 0.44842354656460726,
      "grad_norm": 1.8258883953094482,
      "learning_rate": 2.9033103166400527e-05,
      "loss": 2.8534,
      "step": 4921
    },
    {
      "epoch": 0.4485146710406415,
      "grad_norm": 1.9094854593276978,
      "learning_rate": 2.9026039846431946e-05,
      "loss": 3.0456,
      "step": 4922
    },
    {
      "epoch": 0.4486057955166758,
      "grad_norm": 1.7399648427963257,
      "learning_rate": 2.9018976196513543e-05,
      "loss": 2.5027,
      "step": 4923
    },
    {
      "epoch": 0.44869691999271005,
      "grad_norm": 1.7721437215805054,
      "learning_rate": 2.9011912217224214e-05,
      "loss": 2.3266,
      "step": 4924
    },
    {
      "epoch": 0.4487880444687443,
      "grad_norm": 2.3019111156463623,
      "learning_rate": 2.9004847909142874e-05,
      "loss": 2.7733,
      "step": 4925
    },
    {
      "epoch": 0.4488791689447786,
      "grad_norm": 2.243587017059326,
      "learning_rate": 2.899778327284847e-05,
      "loss": 3.229,
      "step": 4926
    },
    {
      "epoch": 0.44897029342081285,
      "grad_norm": 1.277222752571106,
      "learning_rate": 2.8990718308919996e-05,
      "loss": 2.7857,
      "step": 4927
    },
    {
      "epoch": 0.4490614178968471,
      "grad_norm": 1.4556779861450195,
      "learning_rate": 2.8983653017936424e-05,
      "loss": 2.8766,
      "step": 4928
    },
    {
      "epoch": 0.4491525423728814,
      "grad_norm": 1.225383996963501,
      "learning_rate": 2.8976587400476802e-05,
      "loss": 2.6621,
      "step": 4929
    },
    {
      "epoch": 0.44924366684891565,
      "grad_norm": 1.9832390546798706,
      "learning_rate": 2.896952145712018e-05,
      "loss": 2.958,
      "step": 4930
    },
    {
      "epoch": 0.44933479132494986,
      "grad_norm": 2.9586856365203857,
      "learning_rate": 2.8962455188445643e-05,
      "loss": 2.6427,
      "step": 4931
    },
    {
      "epoch": 0.4494259158009841,
      "grad_norm": 1.8261196613311768,
      "learning_rate": 2.895538859503229e-05,
      "loss": 3.2508,
      "step": 4932
    },
    {
      "epoch": 0.4495170402770184,
      "grad_norm": 1.3958238363265991,
      "learning_rate": 2.894832167745926e-05,
      "loss": 2.7405,
      "step": 4933
    },
    {
      "epoch": 0.44960816475305265,
      "grad_norm": 2.560488224029541,
      "learning_rate": 2.894125443630573e-05,
      "loss": 2.2639,
      "step": 4934
    },
    {
      "epoch": 0.4496992892290869,
      "grad_norm": 2.0037035942077637,
      "learning_rate": 2.8934186872150866e-05,
      "loss": 2.9116,
      "step": 4935
    },
    {
      "epoch": 0.4497904137051212,
      "grad_norm": 1.7134760618209839,
      "learning_rate": 2.8927118985573892e-05,
      "loss": 2.751,
      "step": 4936
    },
    {
      "epoch": 0.44988153818115545,
      "grad_norm": 1.1018348932266235,
      "learning_rate": 2.892005077715405e-05,
      "loss": 2.6942,
      "step": 4937
    },
    {
      "epoch": 0.4499726626571897,
      "grad_norm": 1.7969982624053955,
      "learning_rate": 2.8912982247470606e-05,
      "loss": 2.9457,
      "step": 4938
    },
    {
      "epoch": 0.450063787133224,
      "grad_norm": 1.9811770915985107,
      "learning_rate": 2.890591339710286e-05,
      "loss": 2.4817,
      "step": 4939
    },
    {
      "epoch": 0.45015491160925825,
      "grad_norm": 1.1442551612854004,
      "learning_rate": 2.8898844226630118e-05,
      "loss": 2.7478,
      "step": 4940
    },
    {
      "epoch": 0.4502460360852925,
      "grad_norm": 2.1740434169769287,
      "learning_rate": 2.889177473663174e-05,
      "loss": 4.464,
      "step": 4941
    },
    {
      "epoch": 0.4503371605613268,
      "grad_norm": 1.9461133480072021,
      "learning_rate": 2.8884704927687095e-05,
      "loss": 2.7996,
      "step": 4942
    },
    {
      "epoch": 0.45042828503736104,
      "grad_norm": 1.9043599367141724,
      "learning_rate": 2.8877634800375576e-05,
      "loss": 3.1011,
      "step": 4943
    },
    {
      "epoch": 0.4505194095133953,
      "grad_norm": 2.058687448501587,
      "learning_rate": 2.8870564355276615e-05,
      "loss": 2.6058,
      "step": 4944
    },
    {
      "epoch": 0.4506105339894296,
      "grad_norm": 1.7810022830963135,
      "learning_rate": 2.8863493592969663e-05,
      "loss": 2.7495,
      "step": 4945
    },
    {
      "epoch": 0.45070165846546384,
      "grad_norm": 2.381775379180908,
      "learning_rate": 2.8856422514034193e-05,
      "loss": 2.9865,
      "step": 4946
    },
    {
      "epoch": 0.4507927829414981,
      "grad_norm": 1.6865187883377075,
      "learning_rate": 2.8849351119049706e-05,
      "loss": 2.638,
      "step": 4947
    },
    {
      "epoch": 0.45088390741753237,
      "grad_norm": 2.503490686416626,
      "learning_rate": 2.884227940859573e-05,
      "loss": 2.838,
      "step": 4948
    },
    {
      "epoch": 0.45097503189356664,
      "grad_norm": 2.183210611343384,
      "learning_rate": 2.8835207383251828e-05,
      "loss": 2.6497,
      "step": 4949
    },
    {
      "epoch": 0.4510661563696009,
      "grad_norm": 1.9541594982147217,
      "learning_rate": 2.882813504359757e-05,
      "loss": 2.5728,
      "step": 4950
    },
    {
      "epoch": 0.4511572808456351,
      "grad_norm": 1.8005566596984863,
      "learning_rate": 2.882106239021258e-05,
      "loss": 2.6452,
      "step": 4951
    },
    {
      "epoch": 0.4512484053216694,
      "grad_norm": 3.5055625438690186,
      "learning_rate": 2.8813989423676467e-05,
      "loss": 2.8715,
      "step": 4952
    },
    {
      "epoch": 0.45133952979770364,
      "grad_norm": 2.4540727138519287,
      "learning_rate": 2.88069161445689e-05,
      "loss": 2.9317,
      "step": 4953
    },
    {
      "epoch": 0.4514306542737379,
      "grad_norm": 1.8642497062683105,
      "learning_rate": 2.879984255346956e-05,
      "loss": 2.7154,
      "step": 4954
    },
    {
      "epoch": 0.4515217787497722,
      "grad_norm": 2.487065315246582,
      "learning_rate": 2.879276865095815e-05,
      "loss": 4.2092,
      "step": 4955
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 1.773271918296814,
      "learning_rate": 2.878569443761442e-05,
      "loss": 2.7559,
      "step": 4956
    },
    {
      "epoch": 0.4517040277018407,
      "grad_norm": 1.9055724143981934,
      "learning_rate": 2.8778619914018113e-05,
      "loss": 2.8322,
      "step": 4957
    },
    {
      "epoch": 0.45179515217787497,
      "grad_norm": 1.331254482269287,
      "learning_rate": 2.877154508074903e-05,
      "loss": 2.8539,
      "step": 4958
    },
    {
      "epoch": 0.45188627665390924,
      "grad_norm": 2.408444404602051,
      "learning_rate": 2.876446993838697e-05,
      "loss": 2.2951,
      "step": 4959
    },
    {
      "epoch": 0.4519774011299435,
      "grad_norm": 1.7390692234039307,
      "learning_rate": 2.875739448751176e-05,
      "loss": 3.0673,
      "step": 4960
    },
    {
      "epoch": 0.45206852560597777,
      "grad_norm": 1.8807743787765503,
      "learning_rate": 2.8750318728703286e-05,
      "loss": 2.6934,
      "step": 4961
    },
    {
      "epoch": 0.45215965008201203,
      "grad_norm": 1.856023907661438,
      "learning_rate": 2.8743242662541414e-05,
      "loss": 2.8739,
      "step": 4962
    },
    {
      "epoch": 0.4522507745580463,
      "grad_norm": 1.252497673034668,
      "learning_rate": 2.8736166289606065e-05,
      "loss": 2.7866,
      "step": 4963
    },
    {
      "epoch": 0.45234189903408056,
      "grad_norm": 1.745763897895813,
      "learning_rate": 2.8729089610477173e-05,
      "loss": 2.876,
      "step": 4964
    },
    {
      "epoch": 0.45243302351011483,
      "grad_norm": 1.8492294549942017,
      "learning_rate": 2.8722012625734697e-05,
      "loss": 2.8957,
      "step": 4965
    },
    {
      "epoch": 0.4525241479861491,
      "grad_norm": 1.408050775527954,
      "learning_rate": 2.8714935335958625e-05,
      "loss": 2.8529,
      "step": 4966
    },
    {
      "epoch": 0.45261527246218336,
      "grad_norm": 1.0268100500106812,
      "learning_rate": 2.8707857741728973e-05,
      "loss": 2.7518,
      "step": 4967
    },
    {
      "epoch": 0.4527063969382176,
      "grad_norm": 1.9507266283035278,
      "learning_rate": 2.8700779843625775e-05,
      "loss": 2.9183,
      "step": 4968
    },
    {
      "epoch": 0.4527975214142519,
      "grad_norm": 3.2243103981018066,
      "learning_rate": 2.8693701642229094e-05,
      "loss": 3.0047,
      "step": 4969
    },
    {
      "epoch": 0.45288864589028616,
      "grad_norm": 1.9034618139266968,
      "learning_rate": 2.8686623138119012e-05,
      "loss": 2.7545,
      "step": 4970
    },
    {
      "epoch": 0.45297977036632037,
      "grad_norm": 1.8987599611282349,
      "learning_rate": 2.8679544331875653e-05,
      "loss": 3.0043,
      "step": 4971
    },
    {
      "epoch": 0.45307089484235463,
      "grad_norm": 1.1180181503295898,
      "learning_rate": 2.8672465224079132e-05,
      "loss": 2.7077,
      "step": 4972
    },
    {
      "epoch": 0.4531620193183889,
      "grad_norm": 2.0920677185058594,
      "learning_rate": 2.866538581530962e-05,
      "loss": 2.9554,
      "step": 4973
    },
    {
      "epoch": 0.45325314379442316,
      "grad_norm": 2.020352602005005,
      "learning_rate": 2.8658306106147302e-05,
      "loss": 2.8353,
      "step": 4974
    },
    {
      "epoch": 0.45334426827045743,
      "grad_norm": 1.993905782699585,
      "learning_rate": 2.8651226097172394e-05,
      "loss": 2.8745,
      "step": 4975
    },
    {
      "epoch": 0.4534353927464917,
      "grad_norm": 1.303904414176941,
      "learning_rate": 2.864414578896513e-05,
      "loss": 2.8203,
      "step": 4976
    },
    {
      "epoch": 0.45352651722252596,
      "grad_norm": 2.005981683731079,
      "learning_rate": 2.8637065182105766e-05,
      "loss": 2.7996,
      "step": 4977
    },
    {
      "epoch": 0.4536176416985602,
      "grad_norm": 3.765611410140991,
      "learning_rate": 2.8629984277174576e-05,
      "loss": 2.8408,
      "step": 4978
    },
    {
      "epoch": 0.4537087661745945,
      "grad_norm": 2.050400495529175,
      "learning_rate": 2.8622903074751882e-05,
      "loss": 2.8722,
      "step": 4979
    },
    {
      "epoch": 0.45379989065062876,
      "grad_norm": 2.1093244552612305,
      "learning_rate": 2.861582157541801e-05,
      "loss": 2.7546,
      "step": 4980
    },
    {
      "epoch": 0.453891015126663,
      "grad_norm": 2.8119468688964844,
      "learning_rate": 2.860873977975332e-05,
      "loss": 2.9246,
      "step": 4981
    },
    {
      "epoch": 0.4539821396026973,
      "grad_norm": 1.9836241006851196,
      "learning_rate": 2.8601657688338197e-05,
      "loss": 2.4085,
      "step": 4982
    },
    {
      "epoch": 0.45407326407873155,
      "grad_norm": 2.376142740249634,
      "learning_rate": 2.8594575301753028e-05,
      "loss": 4.1262,
      "step": 4983
    },
    {
      "epoch": 0.4541643885547658,
      "grad_norm": 1.1277364492416382,
      "learning_rate": 2.8587492620578267e-05,
      "loss": 2.6933,
      "step": 4984
    },
    {
      "epoch": 0.4542555130308001,
      "grad_norm": 1.9412622451782227,
      "learning_rate": 2.8580409645394345e-05,
      "loss": 2.8825,
      "step": 4985
    },
    {
      "epoch": 0.45434663750683435,
      "grad_norm": 1.5833308696746826,
      "learning_rate": 2.8573326376781757e-05,
      "loss": 2.6154,
      "step": 4986
    },
    {
      "epoch": 0.4544377619828686,
      "grad_norm": 2.0494384765625,
      "learning_rate": 2.8566242815321e-05,
      "loss": 2.9203,
      "step": 4987
    },
    {
      "epoch": 0.4545288864589029,
      "grad_norm": 2.642484188079834,
      "learning_rate": 2.8559158961592592e-05,
      "loss": 2.4495,
      "step": 4988
    },
    {
      "epoch": 0.45462001093493715,
      "grad_norm": 2.812497615814209,
      "learning_rate": 2.8552074816177088e-05,
      "loss": 3.0033,
      "step": 4989
    },
    {
      "epoch": 0.4547111354109714,
      "grad_norm": 4.986684799194336,
      "learning_rate": 2.8544990379655062e-05,
      "loss": 2.6995,
      "step": 4990
    },
    {
      "epoch": 0.4548022598870056,
      "grad_norm": 1.1402956247329712,
      "learning_rate": 2.8537905652607122e-05,
      "loss": 2.7473,
      "step": 4991
    },
    {
      "epoch": 0.4548933843630399,
      "grad_norm": 2.5283329486846924,
      "learning_rate": 2.853082063561387e-05,
      "loss": 2.8798,
      "step": 4992
    },
    {
      "epoch": 0.45498450883907415,
      "grad_norm": 1.521990180015564,
      "learning_rate": 2.852373532925596e-05,
      "loss": 2.8654,
      "step": 4993
    },
    {
      "epoch": 0.4550756333151084,
      "grad_norm": 1.9248524904251099,
      "learning_rate": 2.8516649734114065e-05,
      "loss": 2.8336,
      "step": 4994
    },
    {
      "epoch": 0.4551667577911427,
      "grad_norm": 2.2869694232940674,
      "learning_rate": 2.8509563850768865e-05,
      "loss": 3.0046,
      "step": 4995
    },
    {
      "epoch": 0.45525788226717695,
      "grad_norm": 2.067301034927368,
      "learning_rate": 2.8502477679801092e-05,
      "loss": 2.654,
      "step": 4996
    },
    {
      "epoch": 0.4553490067432112,
      "grad_norm": 2.3251211643218994,
      "learning_rate": 2.8495391221791475e-05,
      "loss": 2.9695,
      "step": 4997
    },
    {
      "epoch": 0.4554401312192455,
      "grad_norm": 1.77293860912323,
      "learning_rate": 2.8488304477320776e-05,
      "loss": 2.7147,
      "step": 4998
    },
    {
      "epoch": 0.45553125569527975,
      "grad_norm": 1.1759133338928223,
      "learning_rate": 2.8481217446969794e-05,
      "loss": 2.7583,
      "step": 4999
    },
    {
      "epoch": 0.455622380171314,
      "grad_norm": 1.835942268371582,
      "learning_rate": 2.8474130131319327e-05,
      "loss": 3.1533,
      "step": 5000
    }
  ],
  "logging_steps": 1,
  "max_steps": 10974,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4227439732543693e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
